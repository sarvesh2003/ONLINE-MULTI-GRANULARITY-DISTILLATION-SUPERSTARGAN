{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1568691,"sourceType":"datasetVersion","datasetId":926989},{"sourceId":7743362,"sourceType":"datasetVersion","datasetId":4526191},{"sourceId":7939806,"sourceType":"datasetVersion","datasetId":4667923},{"sourceId":8109849,"sourceType":"datasetVersion","datasetId":4790503},{"sourceId":8113704,"sourceType":"datasetVersion","datasetId":4793302}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone \"https://github.com/bytedance/OMGD.git\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-06T14:08:11.764183Z","iopub.execute_input":"2024-05-06T14:08:11.764541Z","iopub.status.idle":"2024-05-06T14:08:13.355623Z","shell.execute_reply.started":"2024-05-06T14:08:11.764504Z","shell.execute_reply":"2024-05-06T14:08:13.354440Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'OMGD'...\nremote: Enumerating objects: 223, done.\u001b[K\nremote: Counting objects: 100% (100/100), done.\u001b[K\nremote: Compressing objects: 100% (51/51), done.\u001b[K\nremote: Total 223 (delta 53), reused 90 (delta 49), pack-reused 123\u001b[K\nReceiving objects: 100% (223/223), 1.05 MiB | 24.53 MiB/s, done.\nResolving deltas: 100% (90/90), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -r /kaggle/working/OMGD/requirements.txt\n!pip install thop","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:08:13.357942Z","iopub.execute_input":"2024-05-06T14:08:13.358260Z","iopub.status.idle":"2024-05-06T14:08:43.816832Z","shell.execute_reply.started":"2024-05-06T14:08:13.358233Z","shell.execute_reply":"2024-05-06T14:08:43.815704Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/OMGD/requirements.txt (line 1)) (1.4.0)\nCollecting blessings (from -r /kaggle/working/OMGD/requirements.txt (line 2))\n  Downloading blessings-1.7-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/OMGD/requirements.txt (line 3)) (2024.2.2)\nCollecting dominate (from -r /kaggle/working/OMGD/requirements.txt (line 4))\n  Downloading dominate-2.9.1-py2.py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: grpcio in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/OMGD/requirements.txt (line 5)) (1.51.1)\nRequirement already satisfied: Markdown in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/OMGD/requirements.txt (line 6)) (3.5.2)\nCollecting nvidia-ml-py3 (from -r /kaggle/working/OMGD/requirements.txt (line 7))\n  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: olefile in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/OMGD/requirements.txt (line 8)) (0.47)\nRequirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/OMGD/requirements.txt (line 9)) (4.9.0.80)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/OMGD/requirements.txt (line 10)) (9.5.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/OMGD/requirements.txt (line 11)) (3.20.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/OMGD/requirements.txt (line 12)) (5.9.3)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/OMGD/requirements.txt (line 13)) (1.11.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/OMGD/requirements.txt (line 14)) (1.16.0)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/OMGD/requirements.txt (line 15)) (2.15.1)\nRequirement already satisfied: tensorboardX in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/OMGD/requirements.txt (line 16)) (2.6.2.2)\nCollecting torchprofile (from -r /kaggle/working/OMGD/requirements.txt (line 17))\n  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/OMGD/requirements.txt (line 18)) (4.66.1)\nRequirement already satisfied: Werkzeug in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/OMGD/requirements.txt (line 19)) (3.0.2)\nCollecting wget (from -r /kaggle/working/OMGD/requirements.txt (line 20))\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/OMGD/requirements.txt (line 21)) (0.16.5)\nRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-python-headless->-r /kaggle/working/OMGD/requirements.txt (line 9)) (1.26.4)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r /kaggle/working/OMGD/requirements.txt (line 15)) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r /kaggle/working/OMGD/requirements.txt (line 15)) (1.2.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r /kaggle/working/OMGD/requirements.txt (line 15)) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r /kaggle/working/OMGD/requirements.txt (line 15)) (69.0.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r /kaggle/working/OMGD/requirements.txt (line 15)) (0.7.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorboardX->-r /kaggle/working/OMGD/requirements.txt (line 16)) (21.3)\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.10/site-packages (from torchprofile->-r /kaggle/working/OMGD/requirements.txt (line 17)) (2.1.2)\nRequirement already satisfied: torchvision>=0.4 in /opt/conda/lib/python3.10/site-packages (from torchprofile->-r /kaggle/working/OMGD/requirements.txt (line 17)) (0.16.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from Werkzeug->-r /kaggle/working/OMGD/requirements.txt (line 19)) (2.1.3)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->-r /kaggle/working/OMGD/requirements.txt (line 21)) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r /kaggle/working/OMGD/requirements.txt (line 21)) (3.1.41)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r /kaggle/working/OMGD/requirements.txt (line 21)) (1.44.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r /kaggle/working/OMGD/requirements.txt (line 21)) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb->-r /kaggle/working/OMGD/requirements.txt (line 21)) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb->-r /kaggle/working/OMGD/requirements.txt (line 21)) (1.3.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb->-r /kaggle/working/OMGD/requirements.txt (line 21)) (1.4.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->-r /kaggle/working/OMGD/requirements.txt (line 21)) (4.0.11)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r /kaggle/working/OMGD/requirements.txt (line 15)) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r /kaggle/working/OMGD/requirements.txt (line 15)) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r /kaggle/working/OMGD/requirements.txt (line 15)) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r /kaggle/working/OMGD/requirements.txt (line 15)) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->-r /kaggle/working/OMGD/requirements.txt (line 15)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->-r /kaggle/working/OMGD/requirements.txt (line 15)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->-r /kaggle/working/OMGD/requirements.txt (line 15)) (1.26.18)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile->-r /kaggle/working/OMGD/requirements.txt (line 17)) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile->-r /kaggle/working/OMGD/requirements.txt (line 17)) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile->-r /kaggle/working/OMGD/requirements.txt (line 17)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile->-r /kaggle/working/OMGD/requirements.txt (line 17)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile->-r /kaggle/working/OMGD/requirements.txt (line 17)) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->torchprofile->-r /kaggle/working/OMGD/requirements.txt (line 17)) (2024.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorboardX->-r /kaggle/working/OMGD/requirements.txt (line 16)) (3.1.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r /kaggle/working/OMGD/requirements.txt (line 21)) (5.0.1)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r /kaggle/working/OMGD/requirements.txt (line 15)) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r /kaggle/working/OMGD/requirements.txt (line 15)) (3.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4->torchprofile->-r /kaggle/working/OMGD/requirements.txt (line 17)) (1.3.0)\nDownloading blessings-1.7-py3-none-any.whl (18 kB)\nDownloading dominate-2.9.1-py2.py3-none-any.whl (29 kB)\nDownloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\nBuilding wheels for collected packages: nvidia-ml-py3, wget\n  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19171 sha256=b4e6184cc431ce8884d67b762091b6d3a3adffb66c4190fd4c47bfe8308db885\n  Stored in directory: /root/.cache/pip/wheels/5c/d8/c0/46899f8be7a75a2ffd197a23c8797700ea858b9b34819fbf9e\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=35d1064504e8a7d6f0a7f20b4ac710f898f53b624be5af3c398a0a41ac4dc8fa\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\nSuccessfully built nvidia-ml-py3 wget\nInstalling collected packages: wget, nvidia-ml-py3, dominate, blessings, torchprofile\nSuccessfully installed blessings-1.7 dominate-2.9.1 nvidia-ml-py3-7.352.0 torchprofile-0.0.4 wget-3.2\nCollecting thop\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from thop) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->thop) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->thop) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->thop) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->thop) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->thop) (1.3.0)\nDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nInstalling collected packages: thop\nSuccessfully installed thop-0.1.1.post2209072238\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Training Additions**","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/OMGD/scripts/unet_pix2pix/edges2shoes/distill.sh\n# %load /kaggle/working/OMGD/scripts/unet_pix2pix/edges2shoes/distill.sh\n#!/usr/bin/env bash\npython /kaggle/working/OMGD/distill.py --dataroot /kaggle/input/painters-sketch/processed_painters \\\n  --gpu_ids 0 --print_freq 100 --n_share 5 \\\n  --lambda_CD 1e1 \\\n  --distiller multiteacher \\\n  --log_dir logs/unet_pix2pix/edges2shoes-r/distill \\\n  --batch_size 4 --num_teacher 2 \\\n  --real_stat_path real_stat/edges2shoes-r_B.npz \\\n  --teacher_ngf_w 64 --teacher_ngf_d 16 --student_ngf 16  --norm batch \\\n  --teacher_netG_w unet_256 --teacher_netG_d unet_deepest_256 --netD multi_n_layers \\\n  --nepochs 19 --nepochs_decay 1 --n_dis 1 \\\n  --AGD_weights 1e1,1e4,1e1,1e-5","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:08:43.818268Z","iopub.execute_input":"2024-05-06T14:08:43.818559Z","iopub.status.idle":"2024-05-06T14:08:43.825975Z","shell.execute_reply.started":"2024-05-06T14:08:43.818532Z","shell.execute_reply":"2024-05-06T14:08:43.825012Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/OMGD/scripts/unet_pix2pix/edges2shoes/distill.sh\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:08:43.827222Z","iopub.execute_input":"2024-05-06T14:08:43.827541Z","iopub.status.idle":"2024-05-06T14:08:43.837406Z","shell.execute_reply.started":"2024-05-06T14:08:43.827510Z","shell.execute_reply":"2024-05-06T14:08:43.836514Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/OMGD/distillers/base_multiteacher_distiller.py\n# %load /kaggle/working/OMGD/distillers/base_multiteacher_distiller.py\nimport itertools\nimport os\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import DataParallel\n\nfrom torchprofile import profile_macs\nfrom collections import OrderedDict\nimport models.modules.loss\nfrom data import create_eval_dataloader\nfrom metric import create_metric_models\nfrom models import networks\nfrom models.base_model import BaseModel\nfrom utils import util\nfrom models.modules.discriminators import FLAGS\nimport math\n\nclass BaseMultiTeacherDistiller(BaseModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        assert is_train\n        parser = super(BaseMultiTeacherDistiller, BaseMultiTeacherDistiller).modify_commandline_options(parser, is_train)\n        parser.add_argument('--teacher_netG_w', type=str, default='unet_256',\n                            help='specify teacher generator architecture',)\n        parser.add_argument('--teacher_netG_d', type=str, default='unet_deepest_256',\n                            help='specify teacher generator architecture',)\n        parser.add_argument('--student_netG', type=str, default='unet_256',\n                            help='specify student generator architecture',)\n\n        parser.add_argument('--num_teacher', type=int, default=2,\n                            help='the number of teacher generators')\n        parser.add_argument('--teacher_ngf_w', type=int, default=64,\n                            help='the base number of filters of the teacher generator')\n        parser.add_argument('--teacher_ngf_d', type=int, default=16,\n                            help='the base number of filters of the teacher generator')\n        parser.add_argument('--student_ngf', type=int, default=16,\n                            help='the base number of filters of the student generator')\n\n        parser.add_argument('--restore_teacher_G_w_path', type=str, default=None,\n                            help='the path to restore the wider teacher generator')\n        parser.add_argument('--restore_teacher_G_d_path', type=str, default=None,\n                            help='the path to restore the deeper teacher generator')\n        parser.add_argument('--restore_student_G_path', type=str, default=None,\n                            help='the path to restore the student generator')\n        parser.add_argument('--restore_A_path', type=str, default=None,\n                            help='the path to restore the adaptors for distillation')\n        parser.add_argument('--restore_D_path', type=str, default=None,\n                            help='the path to restore the discriminator')\n        parser.add_argument('--restore_O_path', type=str, default=None,\n                            help='the path to restore the optimizer')\n\n        parser.add_argument('--recon_loss_type', type=str, default='l1',\n                            choices=['l1', 'l2', 'smooth_l1', 'vgg'],\n                            help='the type of the reconstruction loss')\n        parser.add_argument('--lambda_CD', type=float, default=0,\n                            help='weights for the intermediate activation distillation loss')\n        parser.add_argument('--lambda_recon', type=float, default=100,\n                            help='weights for the reconstruction loss.')\n        parser.add_argument('--lambda_gan', type=float, default=1,\n                            help='weight for gan loss')\n\n\n        parser.add_argument('--teacher_dropout_rate', type=float, default=0)\n        parser.add_argument('--student_dropout_rate', type=float, default=0)\n\n        parser.add_argument('--n_share', type=int, default=0, help='shared blocks in D')\n        parser.add_argument('--project', type=str, default=None, help='the project name of this trail')\n        parser.add_argument('--name', type=str, default=None, help='the name of this trail')\n        return parser\n\n    def __init__(self, opt):\n        assert opt.isTrain\n        super(BaseMultiTeacherDistiller, self).__init__(opt)\n        self.loss_names = ['G_gan_w',  'G_recon_w', 'G_gan_d', 'G_recon_d',\n                           'D_fake_w', 'D_real_w', 'D_fake_d', 'D_real_d',\n                           'G_SSIM', 'G_feature', 'G_style', 'G_tv', 'G_CD']\n        self.optimizers = []\n        self.image_paths = []\n        self.visual_names = ['real_A', 'Sfake_B', 'Tfake_B_w', 'Tfake_B_d', 'real_B']\n        self.model_names = ['netG_student', 'netG_teacher_w', 'netG_teacher_d', 'netD_teacher','netD_student',]\n        self.netG_teacher_w = networks.define_G(opt.input_nc, opt.output_nc, opt.teacher_ngf_w,\n                                              opt.teacher_netG_w, opt.norm, opt.teacher_dropout_rate,\n                                              opt.init_type, opt.init_gain, self.gpu_ids, opt=opt)\n        self.netG_teacher_d = networks.define_G(opt.input_nc, opt.output_nc, opt.teacher_ngf_d,\n                                                opt.teacher_netG_d, opt.norm, opt.teacher_dropout_rate,\n                                                opt.init_type, opt.init_gain, self.gpu_ids, opt=opt)\n        self.netG_student = networks.define_G(opt.input_nc, opt.output_nc, opt.student_ngf,\n                                              opt.student_netG, opt.norm, opt.student_dropout_rate,\n                                              opt.init_type, opt.init_gain, self.gpu_ids, opt=opt)\n\n        if opt.dataset_mode == 'aligned':\n            self.netD_teacher = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,\n                                          opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids, n_share=self.opt.n_share)\n        elif opt.dataset_mode == 'unaligned':\n            self.netD_teacher = networks.define_D(opt.output_nc, opt.ndf, opt.netD,\n                                          opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids, n_share=self.opt.n_share)\n        else:\n            raise NotImplementedError('Unknown dataset mode [%s]!!!' % opt.dataset_mode)\n\n        self.netG_teacher_w.train()\n        self.netG_teacher_d.train()\n        self.netG_student.train()\n        self.netD_teacher.train()\n\n        self.criterionGAN = models.modules.loss.GANLoss(opt.gan_mode).to(self.device)\n        if opt.recon_loss_type == 'l1':\n            self.criterionRecon = torch.nn.L1Loss()\n        elif opt.recon_loss_type == 'l2':\n            self.criterionRecon = torch.nn.MSELoss()\n        elif opt.recon_loss_type == 'smooth_l1':\n            self.criterionRecon = torch.nn.SmoothL1Loss()\n        elif opt.recon_loss_type == 'vgg':\n            self.criterionRecon = models.modules.loss.VGGLoss().to(self.device)\n        else:\n            raise NotImplementedError('Unknown reconstruction loss type [%s]!' % opt.loss_type)\n\n        self.mapping_layers = {'unet_256':['model.model.1.model.3.model.0',     # 2 * ngf\n                                            'model.model.1.model.3.model.3.model.3.model.0',      # 8 * ngf\n                                            'model.model.1.model.3.model.3.model.4',      # 16 * ngf\n                                            'model.model.1.model.4'],     # 4 * ngf\n                                'mobile_resnet_9blocks':['model.9',  # 4 * ngf\n                                                         'model.12',\n                                                         'model.15',\n                                                         'model.18'],}\n        self.netAs = []\n        self.Tacts, self.Sacts = {}, {}\n        G_params = [self.netG_student.parameters()]\n        if self.opt.lambda_CD:\n            for i, n in enumerate(self.mapping_layers[self.opt.teacher_netG_w]):\n                ft, fs = self.opt.teacher_ngf_w, self.opt.student_ngf\n                if 'resnet' in self.opt.teacher_netG_w:\n                    netA = self.build_feature_connector(4 * ft, 4 * fs)\n                elif i == 0:\n                    netA = self.build_feature_connector(2 * ft, 2 * fs)\n                elif i == 1:\n                    netA = self.build_feature_connector(8 * ft, 8 * fs)\n                elif i == 2:\n                    netA = self.build_feature_connector(16 * ft, 16 * fs)\n                else:\n                    netA = self.build_feature_connector(4 * ft, 4 * fs)\n\n                networks.init_net(netA)\n                G_params.append(netA.parameters())\n                self.netAs.append(netA)\n\n        self.optimizer_G_student = torch.optim.Adam(itertools.chain(*G_params), lr=opt.lr, betas=(opt.beta1, 0.999))\n        self.optimizer_G_teacher_w = torch.optim.Adam(self.netG_teacher_w.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n        self.optimizer_G_teacher_d = torch.optim.Adam(self.netG_teacher_d.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n        self.optimizer_D_teacher = torch.optim.Adam(self.netD_teacher.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n\n        self.optimizers.append(self.optimizer_G_student)\n        self.optimizers.append(self.optimizer_G_teacher_w)\n        self.optimizers.append(self.optimizer_G_teacher_d)\n        self.optimizers.append(self.optimizer_D_teacher)\n\n        self.eval_dataloader = create_eval_dataloader(self.opt, direction=opt.direction)\n        self.inception_model, self.drn_model = create_metric_models(opt, device=self.device)\n#         self.npz = np.load(opt.real_stat_path)\n        self.is_best = False\n        self.loss_D_fake, self.loss_D_real = 0, 0\n\n    def build_feature_connector(self, t_channel, s_channel):\n        C = [nn.Conv2d(s_channel, t_channel, kernel_size=1, stride=1, padding=0, bias=False),\n             nn.BatchNorm2d(t_channel),\n             nn.ReLU(inplace=True)]\n\n        for m in C:\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        return nn.Sequential(*C)\n\n    def setup(self, opt, verbose=True):\n        self.schedulers = [networks.get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n        self.load_networks(verbose)\n        if verbose:\n            self.print_networks()\n        if self.opt.lambda_CD > 0:\n            def get_activation(mem, name):\n                def get_output_hook(module, input, output):\n                    mem[name + str(output.device)] = output\n\n                return get_output_hook\n\n            def add_hook(net, mem, mapping_layers):\n                for n, m in net.named_modules():\n                    if n in mapping_layers:\n                        m.register_forward_hook(get_activation(mem, n))\n\n            add_hook(self.netG_teacher_w, self.Tacts, self.mapping_layers[self.opt.teacher_netG_w])\n            add_hook(self.netG_student, self.Sacts, self.mapping_layers[self.opt.teacher_netG_w])\n\n    def set_input(self, input):\n        AtoB = self.opt.direction == 'AtoB'\n        self.real_A = input['A' if AtoB else 'B'].to(self.device)\n        self.real_B = input['B' if AtoB else 'A'].to(self.device)\n        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n\n    def set_single_input(self, input):\n        self.real_A = input['A'].to(self.device)\n        self.image_paths = input['A_paths']\n\n    def forward(self):\n        raise NotImplementedError\n\n    def backward_D_teacher(self):\n        FLAGS.teacher_ids = 1\n        fake_AB_w = torch.cat((self.real_A, self.Tfake_B_w), 1).detach()\n        real_AB = torch.cat((self.real_A, self.real_B), 1).detach()\n        pred_fake_w = self.netD_teacher(fake_AB_w)\n        self.loss_D_fake_w = self.criterionGAN(pred_fake_w, False, for_discriminator=True)\n        pred_real_w = self.netD_teacher(real_AB)\n        self.loss_D_real_w = self.criterionGAN(pred_real_w, True, for_discriminator=True)\n        self.loss_D = (self.loss_D_fake_w + self.loss_D_real_w) * 0.5\n\n        FLAGS.teacher_ids = 2\n        fake_AB_d = torch.cat((self.real_A, self.Tfake_B_d), 1).detach()\n        pred_fake_d = self.netD_teacher(fake_AB_d)\n        pred_real_d = self.netD_teacher(real_AB)\n        self.loss_D_fake_d = self.criterionGAN(pred_fake_d, False, for_discriminator=True)\n        self.loss_D_real_d = self.criterionGAN(pred_real_d, True, for_discriminator=True)\n        self.loss_D += (self.loss_D_fake_d + self.loss_D_real_d) * 0.5\n\n        self.loss_D.backward()\n\n    def optimize_parameters(self, steps):\n        raise NotImplementedError\n\n    def print_networks(self):\n        print('---------- Networks initialized -------------')\n        for name in self.model_names:\n            if hasattr(self, name):\n                net = getattr(self, name)\n                num_params = 0\n                for param in net.parameters():\n                    num_params += param.numel()\n                print(net)\n                print('[Network %s] Total number of parameters : %.3f M' % (name, num_params / 1e6))\n                with open(os.path.join(self.opt.log_dir, name + '.txt'), 'w') as f:\n                    f.write(str(net) + '\\n')\n                    f.write('[Network %s] Total number of parameters : %.3f M\\n' % (name, num_params / 1e6))\n        print('-----------------------------------------------')\n\n    def load_networks(self, verbose=True):\n        if self.opt.restore_student_G_path is not None:\n            util.load_network(self.netG_student, self.opt.restore_student_G_path, verbose)\n        if self.opt.restore_teacher_G_w_path is not None:\n            util.load_network(self.netG_teacher_w, self.opt.restore_teacher_G_w_path, verbose)\n        if self.opt.restore_teacher_G_d_path is not None:\n            util.load_network(self.netG_teacher_d, self.opt.restore_teacher_G_d_path, verbose)\n        if self.opt.restore_D_path is not None:\n            util.load_network(self.netD_teacher, self.opt.restore_D_path, verbose)\n        if self.opt.restore_A_path is not None:\n            for i, netA in enumerate(self.netAs):\n                path = '%s-%d.pth' % (self.opt.restore_A_path, i)\n                util.load_network(netA, path, verbose)\n        if self.opt.restore_O_path is not None:\n            for i, optimizer in enumerate(self.optimizers):\n                path = '%s-%d.pth' % (self.opt.restore_O_path, i)\n                util.load_optimizer(optimizer, path, verbose)\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] = self.opt.lr\n\n    def save_net(self, net, save_path):\n        if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n            if isinstance(net, DataParallel):\n                torch.save(net.module.cpu().state_dict(), save_path)\n            else:\n                torch.save(net.cpu().state_dict(), save_path)\n            net.cuda(self.gpu_ids[0])\n        else:\n            torch.save(net.cpu().state_dict(), save_path)\n\n    def save_networks(self, epoch):\n\n        save_filename = '%s_net_%s_student.pth' % (epoch, 'G')\n        save_path = os.path.join(self.save_dir, save_filename)\n        net = getattr(self, 'net%s_student' % 'G')\n        self.save_net(net, save_path)\n\n        save_filename = '%s_net%s_teacher_w.pth' % (epoch, 'G')\n        save_path = os.path.join(self.save_dir, save_filename)\n        net = getattr(self, 'net%s_teacher_w' % 'G')\n        self.save_net(net, save_path)\n\n        save_filename = '%s_net_%s_teacher_d.pth' % (epoch, 'G')\n        save_path = os.path.join(self.save_dir, save_filename)\n        net = getattr(self, 'net%s_teacher_d' % 'G')\n        self.save_net(net, save_path)\n\n        save_filename = '%s_net_%s_teacher.pth' % (epoch, 'D')\n        save_path = os.path.join(self.save_dir, save_filename)\n        net = getattr(self, 'net%s_teacher' % 'D')\n        self.save_net(net, save_path)\n\n        for i, optimizer in enumerate(self.optimizers):\n            save_filename = '%s_optim-%d.pth' % (epoch, i)\n            save_path = os.path.join(self.save_dir, save_filename)\n            torch.save(optimizer.state_dict(), save_path)\n\n        if self.opt.lambda_CD:\n            for i, net in enumerate(self.netAs):\n                save_filename = '%s_net_%s-%d.pth' % (epoch, 'A', i)\n                save_path = os.path.join(self.save_dir, save_filename)\n                self.save_net(net, save_path)\n\n    def evaluate_model(self, step):\n        raise NotImplementedError\n\n    def test(self):\n        with torch.no_grad():\n            self.forward()\n\n    def get_current_visuals(self):\n        \"\"\"Return visualization images. \"\"\"\n        visual_ret = OrderedDict()\n        for name in self.visual_names:\n            if isinstance(name, str):\n                visual_ret[name] = getattr(self, name)\n        return visual_ret\n\n    def profile(self, config=None, verbose=True):\n        for name in self.model_names:\n            if hasattr(self,name) and 'D' not in name:\n                netG = getattr(self,name)\n                if isinstance(netG, nn.DataParallel):\n                    netG = netG.module\n                if config is not None:\n                    netG.configs = config\n                with torch.no_grad():\n                    macs = profile_macs(netG, (self.real_A[:1],))\n                    # flops, params = profile(netG, inputs=(self.real_A[:1],))\n                params = 0\n                for p in netG.parameters():\n                    params += p.numel()\n                if verbose:\n                    print('MACs: %.3fG\\tParams: %.3fM' % (macs / 1e9, params / 1e6), flush=True)\n\n        return None","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:08:43.840454Z","iopub.execute_input":"2024-05-06T14:08:43.840813Z","iopub.status.idle":"2024-05-06T14:08:43.859269Z","shell.execute_reply.started":"2024-05-06T14:08:43.840790Z","shell.execute_reply":"2024-05-06T14:08:43.858340Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/OMGD/distillers/base_multiteacher_distiller.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile /kaggle/working/OMGD/options/distill_options.py\n# %load /kaggle/working/OMGD/options/distill_options.py\nimport argparse\n\nimport data\nimport distillers\nfrom .base_options import BaseOptions\n\n\nclass DistillOptions(BaseOptions):\n    \"\"\"This class defines options used during both training and test time.\n\n    It also implements several helper functions such as parsing, printing, and saving the options.\n    It also gathers additional options defined in <modify_commandline_options> functions in both dataset class and model class.\n    \"\"\"\n\n    def __init__(self, isTrain=True):\n        \"\"\"Reset the class; indicates the class hasn't been initailized\"\"\"\n        super(DistillOptions, self).__init__()\n        self.isTrain = isTrain\n\n    def initialize(self, parser):\n        parser = BaseOptions.initialize(self, parser)\n        # log parameters\n        parser.add_argument('--log_dir', type=str, default='logs/distill',\n                            help='specify an experiment directory')\n        parser.add_argument('--tensorboard_dir', type=str, default=None,\n                            help='tensorboard is saved here')\n        parser.add_argument('--print_freq', type=int, default=100,\n                            help='frequency of showing training results on console')\n        parser.add_argument('--save_latest_freq', type=int, default=20000,\n                            help='frequency of evaluating and save the latest model')\n        parser.add_argument('--save_epoch_freq', type=int, default=500,\n                            help='frequency of saving checkpoints at the end of epoch')\n        parser.add_argument('--epoch_base', type=int, default=1,\n                            help='the epoch base of the training (used for resuming)')\n        parser.add_argument('--iter_base', type=int, default=0,\n                            help='the iteration base of the training (used for resuming)')\n\n        # model parameters\n        parser.add_argument('--distiller', type=str, default='resnet',\n                            help='specify which distiller you want to use [resnet | spade]')\n        parser.add_argument('--netD', type=str, default='n_layers',\n                            help='specify discriminator architecture [n_layers | pixel]. '\n                                 'The basic model is a 70x70 PatchGAN. '\n                                 'n_layers allows you to specify the layers in the discriminator')\n        parser.add_argument('--ndf', type=int, default=128, help='the base number of discriminator filters')\n        parser.add_argument('--n_layers_D', type=int, default=3, help='only used if netD==n_layers')\n        parser.add_argument('--gan_mode', type=str, default='hinge', choices=['lsgan', 'vanilla', 'hinge'],\n                            help='the type of GAN objective. [vanilla| lsgan | hinge]. '\n                                 'vanilla GAN loss is the cross-entropy objective used in the original GAN paper.')\n\n        # training parameters\n        parser.add_argument('--nepochs', type=int, default=5,\n                            help='number of epochs with the initial learning rate')\n        parser.add_argument('--nepochs_decay', type=int, default=15,\n                            help='number of epochs to linearly decay learning rate to zero')\n        parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n        parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n        parser.add_argument('--lr_policy', type=str, default='linear',\n                            help='learning rate policy. [linear | step | plateau | cosine]')\n        parser.add_argument('--lr_decay_iters', type=int, default=50,\n                            help='multiply by a gamma every lr_decay_iters iterations')\n\n        parser.add_argument('--eval_batch_size', type=int, default=1, help='the evaluation batch size')\n        parser.add_argument('--real_stat_path', type=str,\n                            help='the path to load the ground-truth images information to compute FID.')\n        return parser\n\n    def gather_options(self):\n        parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n        parser = self.initialize(parser)\n\n        opt, _ = parser.parse_known_args()\n\n        distiller_name = opt.distiller\n        distiller_option_setter = distillers.get_option_setter(distiller_name)\n        parser = distiller_option_setter(parser, self.isTrain)\n        opt, _ = parser.parse_known_args()\n\n        # modify dataset-related parser options\n        dataset_name = opt.dataset_mode\n        dataset_option_setter = data.get_option_setter(dataset_name)\n        parser = dataset_option_setter(parser, self.isTrain)\n\n        # save and return the parser\n        self.parser = parser\n        return parser.parse_args()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:08:43.860421Z","iopub.execute_input":"2024-05-06T14:08:43.860757Z","iopub.status.idle":"2024-05-06T14:08:43.878979Z","shell.execute_reply.started":"2024-05-06T14:08:43.860731Z","shell.execute_reply":"2024-05-06T14:08:43.877996Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/OMGD/options/distill_options.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile /kaggle/working/OMGD/distillers/multiteacher_distiller.py\n# %load /kaggle/working/OMGD/distillers/multiteacher_distiller.py\nimport ntpath\nimport os\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.nn.parallel import gather, parallel_apply, replicate\nfrom tqdm import tqdm\n\nfrom metric import get_fid, get_cityscapes_mIoU\nfrom utils import util\nfrom utils.vgg_feature import VGGFeature\nfrom .base_multiteacher_distiller import BaseMultiTeacherDistiller\nfrom models.modules import pytorch_ssim\nfrom models.modules.discriminators import FLAGS\n\n\nclass MultiTeacherDistiller(BaseMultiTeacherDistiller):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        assert is_train\n        parser = super(MultiTeacherDistiller, MultiTeacherDistiller).modify_commandline_options(parser, is_train)\n        parser.add_argument('--AGD_weights', type=str, default='1e1, 1e4, 1e1, 1e-5', help='weights for losses in AGD mode')\n        parser.add_argument('--n_dis', type=int, default=1, help='iter time for student before update teacher')\n        parser.set_defaults(norm='instance', dataset_mode='aligned')\n\n        return parser\n\n    def __init__(self, opt):\n        assert opt.isTrain\n        super(MultiTeacherDistiller, self).__init__(opt)\n        self.best_fid_teachers, self.best_fid_student = [1e9 for _ in range(self.opt.num_teacher)],  1e9\n        self.best_mIoU_teachers, self.best_mIoU_student = [-1e9 for _ in range(self.opt.num_teacher)], -1e9\n        self.fids_teacher, self.fids_student, self.mIoUs_teacher, self.mIoUs_student = [], [], [], []\n#         self.npz = np.load(opt.real_stat_path)\n        # weights for AGD mood\n        loss_weight = [float(char) for char in opt.AGD_weights.split(',')]\n        self.lambda_SSIM = loss_weight[0]\n        self.lambda_style = loss_weight[1]\n        self.lambda_feature = loss_weight[2]\n        self.lambda_tv = loss_weight[3]\n        self.vgg = VGGFeature().to(self.device)\n\n    def forward(self):\n        self.Tfake_B_w = self.netG_teacher_w(self.real_A)\n        self.Tfake_B_d = self.netG_teacher_d(self.real_A)\n        self.Tfake_Bs = [self.Tfake_B_w.detach(), self.Tfake_B_d.detach()]\n        self.Sfake_B = self.netG_student(self.real_A)\n\n    def calc_CD_loss(self):\n        losses = []\n        mapping_layers = self.mapping_layers[self.opt.teacher_netG_w]\n        for i, netA in enumerate(self.netAs):\n            n = mapping_layers[i]\n            netA_replicas = replicate(netA.cuda(), self.gpu_ids)\n            Sacts = parallel_apply(netA_replicas,\n                                       tuple([self.Sacts[key] for key in sorted(self.Sacts.keys()) if n in key]))\n            Tacts = [self.Tacts[key] for key in sorted(self.Tacts.keys()) if n in key]\n            for Sact, Tact in zip(Sacts, Tacts):\n                source, target = Sact, Tact.detach()\n                source = source.mean(dim=(2, 3), keepdim=False)\n                target = target.mean(dim=(2, 3), keepdim=False)\n                loss = torch.mean(torch.pow(source - target, 2))\n                losses.append(loss)\n        return sum(losses)\n\n    def backward_G_teacher(self):\n\n        fake_AB_w = torch.cat((self.real_A, self.Tfake_B_w), 1)\n        FLAGS.teacher_ids = 1\n        pred_fake_w = self.netD_teacher(fake_AB_w)\n        self.loss_G_gan_w = self.criterionGAN(pred_fake_w, True, for_discriminator=False) * self.opt.lambda_gan\n        # Second, G(A) = B\n        self.loss_G_recon_w = self.criterionRecon(self.Tfake_B_w, self.real_B) * self.opt.lambda_recon\n        # combine loss and calculate gradients\n        self.loss_G_w = self.loss_G_gan_w + self.loss_G_recon_w\n\n        fake_AB_d = torch.cat((self.real_A, self.Tfake_B_d), 1)\n        FLAGS.teacher_ids = 2\n        pred_fake_d = self.netD_teacher(fake_AB_d)\n        self.loss_G_gan_d = self.criterionGAN(pred_fake_d, True, for_discriminator=False) * self.opt.lambda_gan\n        self.loss_G_recon_d = self.criterionRecon(self.Tfake_B_d, self.real_B) * self.opt.lambda_recon\n        self.loss_G_d = self.loss_G_gan_d + self.loss_G_recon_d\n\n        self.loss_G_d.backward()\n        self.loss_G_w.backward()\n\n\n    def backward_G_student(self):\n        self.loss_G_student = 0\n        for i, teacher_image in enumerate(self.Tfake_Bs):\n            ssim_loss = pytorch_ssim.SSIM()\n            self.loss_G_SSIM = (1 - ssim_loss(self.Sfake_B, teacher_image)) * self.lambda_SSIM\n            Tfeatures = self.vgg(teacher_image)\n            Sfeatures = self.vgg(self.Sfake_B)\n            Tgram = [self.gram(fmap) for fmap in Tfeatures]\n            Sgram = [self.gram(fmap) for fmap in Sfeatures]\n            self.loss_G_style = 0\n            for i in range(len(Tgram)):\n                self.loss_G_style += self.lambda_style * F.l1_loss(Sgram[i], Tgram[i])\n            Srecon, Trecon = Sfeatures[1], Tfeatures[1]\n            self.loss_G_feature = self.lambda_feature * F.l1_loss(Srecon, Trecon)\n            diff_i = torch.sum(torch.abs(self.Sfake_B[:, :, :, 1:] - self.Sfake_B[:, :, :, :-1]))\n            diff_j = torch.sum(torch.abs(self.Sfake_B[:, :, 1:, :] - self.Sfake_B[:, :, :-1, :]))\n            self.loss_G_tv = self.lambda_tv * (diff_i + diff_j)\n            self.loss_G_student += self.loss_G_SSIM + self.loss_G_style + self.loss_G_feature + self.loss_G_tv\n        if self.opt.lambda_CD:\n            self.loss_G_CD = self.calc_CD_loss() * self.opt.lambda_CD\n            self.loss_G_student += self.loss_G_CD\n        self.loss_G_student.backward()\n\n    def gram(self, x):\n        (bs, ch, h, w) = x.size()\n        f = x.view(bs, ch, w*h)\n        f_T = f.transpose(1, 2)\n        G = f.bmm(f_T) / (ch * h * w)\n        return G\n\n    def optimize_parameters(self, steps):\n        self.optimizer_D_teacher.zero_grad()\n        self.optimizer_G_teacher_w.zero_grad()\n        self.optimizer_G_teacher_d.zero_grad()\n        self.optimizer_G_student.zero_grad()\n        self.forward()\n        if steps % self.opt.n_dis == 0:\n            util.set_requires_grad(self.netD_teacher, True)\n            self.backward_D_teacher()\n            util.set_requires_grad(self.netD_teacher, False)\n            self.backward_G_teacher()\n            self.optimizer_D_teacher.step()\n            self.optimizer_G_teacher_w.step()\n            self.optimizer_G_teacher_d.step()\n        self.backward_G_student()\n        self.optimizer_G_student.step()\n\n    def load_networks(self, verbose=True):\n        super(MultiTeacherDistiller, self).load_networks()\n\n    def evaluate_model(self, step):\n        self.is_best = False\n        save_dir = os.path.join(self.opt.log_dir, 'eval', str(step))\n        os.makedirs(save_dir, exist_ok=True)\n        self.netG_student.eval()\n        self.netG_teacher_w.eval()\n        self.netG_teacher_d.eval()\n        S_fakes, T_fakes, names = [], [[] for _ in range(self.opt.num_teacher)],  []\n        cnt = 0\n        id_model_dict = {0: 'w', 1: 'd'}\n        for i, data_i in enumerate(tqdm(self.eval_dataloader, desc='Eval       ', position=2, leave=False)):\n            self.set_input(data_i)\n            self.test()\n            S_fakes.append(self.Sfake_B.cpu())\n            for k in range(len(self.Tfake_Bs)):\n                T_fakes[k].append(self.Tfake_Bs[k].cpu())\n                for j in range(len(self.image_paths)):\n                    short_path = ntpath.basename(self.image_paths[j])\n                    name = os.path.splitext(short_path)[0]\n                    if k == 0:\n                        names.append(name)\n                    if cnt < 10 * len(self.Tfake_Bs):\n                        Tfake_im = util.tensor2im(self.Tfake_Bs[k][j])\n                        if k == 0:\n                            input_im = util.tensor2im(self.real_A[j])\n                            Sfake_im = util.tensor2im(self.Sfake_B[j])\n                            util.save_image(input_im, os.path.join(save_dir, 'input', '%s.png') % name, create_dir=True)\n                            util.save_image(Sfake_im, os.path.join(save_dir, 'Sfake', '%s.png' % name), create_dir=True)\n                        util.save_image(Tfake_im, os.path.join(save_dir, f'Tfake_{id_model_dict[k]}', '%s.png' %name), create_dir=True)\n                        if self.opt.dataset_mode == 'aligned' and k == 0:\n                            real_im = util.tensor2im(self.real_B[j])\n                            util.save_image(real_im, os.path.join(save_dir, 'real', '%s.png' % name), create_dir=True)\n                    cnt += 1\n#         fid_teachers = [get_fid(T_fakes[m], self.inception_model, self.npz, device=self.device,\n#                       batch_size=self.opt.eval_batch_size, tqdm_position=2) for m in range(self.opt.num_teacher)]\n#         fid_student = get_fid(S_fakes, self.inception_model, self.npz, device=self.device,\n#                       batch_size=self.opt.eval_batch_size, tqdm_position=2)\n        fid_teachers = [0.05, 0.1, 0.9]\n        fid_student = 0.4\n        if fid_student < self.best_fid_student:\n            self.is_best = True\n            self.best_fid_student = fid_student\n\n        ret = {}\n        for i in range(self.opt.num_teacher):\n            ret[f'metric/fid_teacher_{id_model_dict[i]}'] = fid_teachers[i]\n            if fid_teachers[i] < self.best_fid_teachers[i]:\n                self.best_fid_teachers[i] = fid_teachers[i]\n            ret[f'metric/fid-best_teacher_{id_model_dict[i]}'] = self.best_fid_teachers[i]\n        ret['metric/fid_student'] = fid_student\n        ret['metric/fid-best_student'] = self.best_fid_student\n        if 'cityscapes' in self.opt.dataroot and self.opt.direction == 'BtoA':\n            mIoU_teachers = [get_cityscapes_mIoU(T_fakes[m], names, self.drn_model, self.device,\n                                       table_path=self.opt.table_path,\n                                       data_dir=self.opt.cityscapes_path,\n                                       batch_size=self.opt.eval_batch_size,\n                                       num_workers=self.opt.num_threads, tqdm_position=2) for m in range(self.opt.num_teacher)]\n            mIoU_student = get_cityscapes_mIoU(S_fakes, names, self.drn_model, self.device,\n                                       table_path=self.opt.table_path,\n                                       data_dir=self.opt.cityscapes_path,\n                                       batch_size=self.opt.eval_batch_size,\n                                       num_workers=self.opt.num_threads, tqdm_position=2)\n            if mIoU_student > self.best_mIoU_student:\n                self.is_best = True\n                self.best_mIoU_student = mIoU_student\n            for i in range(self.opt.num_teacher):\n                ret[f'metric/mIoU_teacher_{id_model_dict[i]}'] = mIoU_teachers[i]\n                if mIoU_teachers[i] > self.best_mIoU_teachers[i]:\n                    self.best_mIoU_teachers[i] = mIoU_teachers[i]\n                ret[f'metric/mIoU-best_teacher_{id_model_dict[i]}'] = self.best_mIoU_teachers[i]\n            ret['metric/mIoU_student'] = mIoU_student\n            ret['metric/mIoU-best_student'] = self.best_mIoU_student\n        self.netG_teacher_w.train()\n        self.netG_teacher_d.train()\n        self.netG_student.train()\n        return ret\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:08:43.880891Z","iopub.execute_input":"2024-05-06T14:08:43.881273Z","iopub.status.idle":"2024-05-06T14:08:43.899736Z","shell.execute_reply.started":"2024-05-06T14:08:43.881240Z","shell.execute_reply":"2024-05-06T14:08:43.898825Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/OMGD/distillers/multiteacher_distiller.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Testing Side config**","metadata":{}},{"cell_type":"code","source":"# %%writefile /kaggle/working/OMGD/scripts/unet_pix2pix/edges2shoes/test.sh\n# # %load /kaggle/working/OMGD/scripts/unet_pix2pix/edges2shoes/test.sh\n# #!/usr/bin/env bash\n# python /kaggle/working/OMGD/test.py --dataroot  /kaggle/input/data-edge/kaggle/working/edges2shoes_changed \\\n#   --results_dir  results/unet_pix2pix/edges2shoes-r/S16 \\\n#   --ngf 16 --netG unet_256 --norm batch \\\n#   --restore_G_path checkpoints/unet_pix2pix/edges2shoes/best_net_G16.pth \\\n#   --real_stat_path  real_stat/edges2shoes-r_B.npz \\\n#   --need_profile --num_test 30 --phase val\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:08:43.901045Z","iopub.execute_input":"2024-05-06T14:08:43.901326Z","iopub.status.idle":"2024-05-06T14:08:43.912450Z","shell.execute_reply.started":"2024-05-06T14:08:43.901302Z","shell.execute_reply":"2024-05-06T14:08:43.911375Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Running Commands**","metadata":{}},{"cell_type":"code","source":"# !bash /kaggle/working/OMGD/scripts/unet_pix2pix/edges2shoes/test.sh","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:08:43.913945Z","iopub.execute_input":"2024-05-06T14:08:43.914366Z","iopub.status.idle":"2024-05-06T14:08:43.925735Z","shell.execute_reply.started":"2024-05-06T14:08:43.914329Z","shell.execute_reply":"2024-05-06T14:08:43.924923Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!bash /kaggle/working/OMGD/scripts/unet_pix2pix/edges2shoes/distill.sh","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:08:43.926963Z","iopub.execute_input":"2024-05-06T14:08:43.927678Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"-------------modifying commandline options-----------\n----------------- Options ---------------\n              AGD_weights: 1e1,1e4,1e1,1e-5              \t[default: 1e1, 1e4, 1e1, 1e-5]\n             aspect_ratio: 1.0                           \n               batch_size: 4                             \t[default: 1]\n                    beta1: 0.5                           \n          cityscapes_path: database/cityscapes-origin    \n               config_set: None                          \n               config_str: None                          \n          cosine_distance: False                         \n                crop_size: 256                           \n                 dataroot: /kaggle/input/painters-sketch/processed_painters\t[default: None]\n             dataset_mode: aligned                       \n           deeplabv2_path: deeplabv2_resnet101_msc-cocostuff164k-100000.pth\n                direction: AtoB                          \n          display_winsize: 256                           \n                distiller: multiteacher                  \t[default: resnet]\n                 drn_path: drn-d-105_ms_cityscapes.pth   \n               epoch_base: 1                             \n          eval_batch_size: 1                             \n                 gan_mode: hinge                         \n                  gpu_ids: 0                             \n                init_gain: 0.02                          \n                init_type: normal                        \n                 input_nc: 3                             \n                  isTrain: True                          \t[default: None]\n                iter_base: 0                             \n                lambda_CD: 10.0                          \t[default: 0]\n               lambda_gan: 1                             \n             lambda_recon: 100                           \n            large_teacher: False                         \n           load_in_memory: False                         \n                load_size: 286                           \n                  log_dir: logs/unet_pix2pix/edges2shoes-r/distill\t[default: logs/distill]\n                       lr: 0.0002                        \n           lr_decay_iters: 50                            \n                lr_policy: linear                        \n         max_dataset_size: -1                            \n                    n_dis: 1                             \n               n_layers_D: 3                             \n                  n_share: 5                             \t[default: 0]\n                     name: None                          \n                      ndf: 128                           \n                  nepochs: 19                            \t[default: 5]\n            nepochs_decay: 1                             \t[default: 15]\n                     netD: multi_n_layers                \t[default: n_layers]\n                  no_flip: False                         \n                     norm: batch                         \t[default: instance]\n              num_teacher: 2                             \n              num_threads: 4                             \n                output_nc: 3                             \n                    phase: train                         \n               preprocess: resize_and_crop               \n               print_freq: 100                           \n                  project: None                          \n           real_stat_path: real_stat/edges2shoes-r_B.npz \t[default: None]\n          recon_loss_type: l1                            \n           restore_A_path: None                          \n           restore_D_path: None                          \n           restore_O_path: None                          \n   restore_student_G_path: None                          \n restore_teacher_G_d_path: None                          \n restore_teacher_G_w_path: None                          \n          save_epoch_freq: 500                           \n         save_latest_freq: 20000                         \n                     seed: 233                           \n           serial_batches: False                         \n     student_dropout_rate: 0                             \n             student_netG: unet_256                      \n              student_ngf: 16                            \n               table_path: datasets/table.txt            \n     teacher_dropout_rate: 0                             \n           teacher_netG_d: unet_deepest_256              \n           teacher_netG_w: unet_256                      \n            teacher_ngf_d: 16                            \n            teacher_ngf_w: 64                            \n          tensorboard_dir: None                          \n----------------- End -------------------\n/kaggle/working/OMGD/distill.py --dataroot /kaggle/input/painters-sketch/processed_painters --gpu_ids 0 --print_freq 100 --n_share 5 --lambda_CD 1e1 --distiller multiteacher --log_dir logs/unet_pix2pix/edges2shoes-r/distill --batch_size 4 --num_teacher 2 --real_stat_path real_stat/edges2shoes-r_B.npz --teacher_ngf_w 64 --teacher_ngf_d 16 --student_ngf 16 --norm batch --teacher_netG_w unet_256 --teacher_netG_d unet_deepest_256 --netD multi_n_layers --nepochs 19 --nepochs_decay 1 --n_dis 1 --AGD_weights 1e1,1e4,1e1,1e-5\ndataset [AlignedDataset] was created\nThe number of training images = 6418\ninitialize network with normal\ninitialize network with normal\ninitialize network with normal\ninitialize network with normal\ninitialize network with normal\ninitialize network with normal\ninitialize network with normal\ninitialize network with normal\ndataset [AlignedDataset] was created\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n/opt/conda/lib/python3.10/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n  warnings.warn(\nDownloading: \"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/pt_inception-2015-12-05-6726825d.pth\n100%|| 91.2M/91.2M [00:00<00:00, 268MB/s]\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|| 528M/528M [00:03<00:00, 180MB/s]\ndistiller [MultiTeacherDistiller] was created\n---------- Networks initialized -------------\nUnetGenerator(\n  (model): UnetSkipConnectionBlock(\n    (model): Sequential(\n      (0): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): UnetSkipConnectionBlock(\n        (model): Sequential(\n          (0): LeakyReLU(negative_slope=0.2, inplace=True)\n          (1): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n          (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (3): UnetSkipConnectionBlock(\n            (model): Sequential(\n              (0): LeakyReLU(negative_slope=0.2, inplace=True)\n              (1): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n              (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (3): UnetSkipConnectionBlock(\n                (model): Sequential(\n                  (0): LeakyReLU(negative_slope=0.2, inplace=True)\n                  (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                  (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (3): UnetSkipConnectionBlock(\n                    (model): Sequential(\n                      (0): LeakyReLU(negative_slope=0.2, inplace=True)\n                      (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (3): UnetSkipConnectionBlock(\n                        (model): Sequential(\n                          (0): LeakyReLU(negative_slope=0.2, inplace=True)\n                          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (3): UnetSkipConnectionBlock(\n                            (model): Sequential(\n                              (0): LeakyReLU(negative_slope=0.2, inplace=True)\n                              (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                              (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (3): UnetSkipConnectionBlock(\n                                (model): Sequential(\n                                  (0): LeakyReLU(negative_slope=0.2, inplace=True)\n                                  (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                                  (2): ReLU(inplace=True)\n                                  (3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                                  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                )\n                              )\n                              (4): ReLU(inplace=True)\n                              (5): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                              (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (7): Dropout(p=0, inplace=False)\n                            )\n                          )\n                          (4): ReLU(inplace=True)\n                          (5): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                          (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (7): Dropout(p=0, inplace=False)\n                        )\n                      )\n                      (4): ReLU(inplace=True)\n                      (5): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                      (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (7): Dropout(p=0, inplace=False)\n                    )\n                  )\n                  (4): ReLU(inplace=True)\n                  (5): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                  (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (7): Dropout(p=0, inplace=False)\n                )\n              )\n              (4): ReLU(inplace=True)\n              (5): ConvTranspose2d(128, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n              (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (7): Dropout(p=0, inplace=False)\n            )\n          )\n          (4): ReLU(inplace=True)\n          (5): ConvTranspose2d(64, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n          (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (7): Dropout(p=0, inplace=False)\n        )\n      )\n      (2): ReLU(inplace=True)\n      (3): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n      (4): Tanh()\n    )\n  )\n)\n[Network netG_student] Total number of parameters : 3.404 M\nUnetGenerator(\n  (model): UnetSkipConnectionBlock(\n    (model): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): UnetSkipConnectionBlock(\n        (model): Sequential(\n          (0): LeakyReLU(negative_slope=0.2, inplace=True)\n          (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (3): UnetSkipConnectionBlock(\n            (model): Sequential(\n              (0): LeakyReLU(negative_slope=0.2, inplace=True)\n              (1): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n              (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (3): UnetSkipConnectionBlock(\n                (model): Sequential(\n                  (0): LeakyReLU(negative_slope=0.2, inplace=True)\n                  (1): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                  (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (3): UnetSkipConnectionBlock(\n                    (model): Sequential(\n                      (0): LeakyReLU(negative_slope=0.2, inplace=True)\n                      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (3): UnetSkipConnectionBlock(\n                        (model): Sequential(\n                          (0): LeakyReLU(negative_slope=0.2, inplace=True)\n                          (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (3): UnetSkipConnectionBlock(\n                            (model): Sequential(\n                              (0): LeakyReLU(negative_slope=0.2, inplace=True)\n                              (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                              (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (3): UnetSkipConnectionBlock(\n                                (model): Sequential(\n                                  (0): LeakyReLU(negative_slope=0.2, inplace=True)\n                                  (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                                  (2): ReLU(inplace=True)\n                                  (3): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                                  (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                )\n                              )\n                              (4): ReLU(inplace=True)\n                              (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                              (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (7): Dropout(p=0, inplace=False)\n                            )\n                          )\n                          (4): ReLU(inplace=True)\n                          (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                          (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (7): Dropout(p=0, inplace=False)\n                        )\n                      )\n                      (4): ReLU(inplace=True)\n                      (5): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (7): Dropout(p=0, inplace=False)\n                    )\n                  )\n                  (4): ReLU(inplace=True)\n                  (5): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                  (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (7): Dropout(p=0, inplace=False)\n                )\n              )\n              (4): ReLU(inplace=True)\n              (5): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n              (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (7): Dropout(p=0, inplace=False)\n            )\n          )\n          (4): ReLU(inplace=True)\n          (5): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n          (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (7): Dropout(p=0, inplace=False)\n        )\n      )\n      (2): ReLU(inplace=True)\n      (3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n      (4): Tanh()\n    )\n  )\n)\n[Network netG_teacher_w] Total number of parameters : 54.414 M\nUnetDeepestGenerator(\n  (model): UnetSkipConnectionBlock(\n    (model): Sequential(\n      (0): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): ResnetBlock(\n        (conv_block): Sequential(\n          (0): ReflectionPad2d((1, 1, 1, 1))\n          (1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (3): ReLU(inplace=True)\n          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n          (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (6): ReLU(inplace=True)\n          (7): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (9): ReLU(inplace=True)\n          (10): Dropout(p=0, inplace=False)\n          (11): ReflectionPad2d((1, 1, 1, 1))\n          (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n          (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): ResnetBlock(\n        (conv_block): Sequential(\n          (0): ReflectionPad2d((1, 1, 1, 1))\n          (1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (3): ReLU(inplace=True)\n          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n          (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (6): ReLU(inplace=True)\n          (7): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (9): ReLU(inplace=True)\n          (10): Dropout(p=0, inplace=False)\n          (11): ReflectionPad2d((1, 1, 1, 1))\n          (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n          (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (3): UnetSkipConnectionBlock(\n        (model): Sequential(\n          (0): LeakyReLU(negative_slope=0.2, inplace=True)\n          (1): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n          (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (3): ResnetBlock(\n            (conv_block): Sequential(\n              (0): ReflectionPad2d((1, 1, 1, 1))\n              (1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (3): ReLU(inplace=True)\n              (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n              (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (6): ReLU(inplace=True)\n              (7): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (9): ReLU(inplace=True)\n              (10): Dropout(p=0, inplace=False)\n              (11): ReflectionPad2d((1, 1, 1, 1))\n              (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n              (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (4): ResnetBlock(\n            (conv_block): Sequential(\n              (0): ReflectionPad2d((1, 1, 1, 1))\n              (1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (3): ReLU(inplace=True)\n              (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n              (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (6): ReLU(inplace=True)\n              (7): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (9): ReLU(inplace=True)\n              (10): Dropout(p=0, inplace=False)\n              (11): ReflectionPad2d((1, 1, 1, 1))\n              (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n              (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (5): UnetSkipConnectionBlock(\n            (model): Sequential(\n              (0): LeakyReLU(negative_slope=0.2, inplace=True)\n              (1): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n              (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (3): ResnetBlock(\n                (conv_block): Sequential(\n                  (0): ReflectionPad2d((1, 1, 1, 1))\n                  (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (3): ReLU(inplace=True)\n                  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (6): ReLU(inplace=True)\n                  (7): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (9): ReLU(inplace=True)\n                  (10): Dropout(p=0, inplace=False)\n                  (11): ReflectionPad2d((1, 1, 1, 1))\n                  (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                )\n              )\n              (4): ResnetBlock(\n                (conv_block): Sequential(\n                  (0): ReflectionPad2d((1, 1, 1, 1))\n                  (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (3): ReLU(inplace=True)\n                  (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                  (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (6): ReLU(inplace=True)\n                  (7): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (9): ReLU(inplace=True)\n                  (10): Dropout(p=0, inplace=False)\n                  (11): ReflectionPad2d((1, 1, 1, 1))\n                  (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                )\n              )\n              (5): UnetSkipConnectionBlock(\n                (model): Sequential(\n                  (0): LeakyReLU(negative_slope=0.2, inplace=True)\n                  (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                  (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (3): ResnetBlock(\n                    (conv_block): Sequential(\n                      (0): ReflectionPad2d((1, 1, 1, 1))\n                      (1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (3): ReLU(inplace=True)\n                      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                      (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (6): ReLU(inplace=True)\n                      (7): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (9): ReLU(inplace=True)\n                      (10): Dropout(p=0, inplace=False)\n                      (11): ReflectionPad2d((1, 1, 1, 1))\n                      (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                      (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                    )\n                  )\n                  (4): ResnetBlock(\n                    (conv_block): Sequential(\n                      (0): ReflectionPad2d((1, 1, 1, 1))\n                      (1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (3): ReLU(inplace=True)\n                      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                      (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (6): ReLU(inplace=True)\n                      (7): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (9): ReLU(inplace=True)\n                      (10): Dropout(p=0, inplace=False)\n                      (11): ReflectionPad2d((1, 1, 1, 1))\n                      (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                      (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                    )\n                  )\n                  (5): UnetSkipConnectionBlock(\n                    (model): Sequential(\n                      (0): LeakyReLU(negative_slope=0.2, inplace=True)\n                      (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (3): ResnetBlock(\n                        (conv_block): Sequential(\n                          (0): ReflectionPad2d((1, 1, 1, 1))\n                          (1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (3): ReLU(inplace=True)\n                          (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                          (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (6): ReLU(inplace=True)\n                          (7): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                          (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (9): ReLU(inplace=True)\n                          (10): Dropout(p=0, inplace=False)\n                          (11): ReflectionPad2d((1, 1, 1, 1))\n                          (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                          (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                        )\n                      )\n                      (4): ResnetBlock(\n                        (conv_block): Sequential(\n                          (0): ReflectionPad2d((1, 1, 1, 1))\n                          (1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (3): ReLU(inplace=True)\n                          (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                          (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (6): ReLU(inplace=True)\n                          (7): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                          (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (9): ReLU(inplace=True)\n                          (10): Dropout(p=0, inplace=False)\n                          (11): ReflectionPad2d((1, 1, 1, 1))\n                          (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                          (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                        )\n                      )\n                      (5): UnetSkipConnectionBlock(\n                        (model): Sequential(\n                          (0): LeakyReLU(negative_slope=0.2, inplace=True)\n                          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (3): ResnetBlock(\n                            (conv_block): Sequential(\n                              (0): ReflectionPad2d((1, 1, 1, 1))\n                              (1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                              (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (3): ReLU(inplace=True)\n                              (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                              (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (6): ReLU(inplace=True)\n                              (7): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                              (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (9): ReLU(inplace=True)\n                              (10): Dropout(p=0, inplace=False)\n                              (11): ReflectionPad2d((1, 1, 1, 1))\n                              (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                              (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                            )\n                          )\n                          (4): ResnetBlock(\n                            (conv_block): Sequential(\n                              (0): ReflectionPad2d((1, 1, 1, 1))\n                              (1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                              (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (3): ReLU(inplace=True)\n                              (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                              (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (6): ReLU(inplace=True)\n                              (7): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                              (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (9): ReLU(inplace=True)\n                              (10): Dropout(p=0, inplace=False)\n                              (11): ReflectionPad2d((1, 1, 1, 1))\n                              (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                              (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                            )\n                          )\n                          (5): UnetSkipConnectionBlock(\n                            (model): Sequential(\n                              (0): LeakyReLU(negative_slope=0.2, inplace=True)\n                              (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                              (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (3): ResnetBlock(\n                                (conv_block): Sequential(\n                                  (0): ReflectionPad2d((1, 1, 1, 1))\n                                  (1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                                  (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                  (3): ReLU(inplace=True)\n                                  (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                                  (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                  (6): ReLU(inplace=True)\n                                  (7): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                                  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                  (9): ReLU(inplace=True)\n                                  (10): Dropout(p=0, inplace=False)\n                                  (11): ReflectionPad2d((1, 1, 1, 1))\n                                  (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                                  (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                )\n                              )\n                              (4): ResnetBlock(\n                                (conv_block): Sequential(\n                                  (0): ReflectionPad2d((1, 1, 1, 1))\n                                  (1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                                  (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                  (3): ReLU(inplace=True)\n                                  (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                                  (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                  (6): ReLU(inplace=True)\n                                  (7): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                                  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                  (9): ReLU(inplace=True)\n                                  (10): Dropout(p=0, inplace=False)\n                                  (11): ReflectionPad2d((1, 1, 1, 1))\n                                  (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                                  (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                )\n                              )\n                              (5): UnetSkipConnectionBlock(\n                                (model): Sequential(\n                                  (0): LeakyReLU(negative_slope=0.2, inplace=True)\n                                  (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                                  (2): ReLU(inplace=True)\n                                  (3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                                  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                )\n                              )\n                              (6): ReLU(inplace=True)\n                              (7): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                              (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (9): ResnetBlock(\n                                (conv_block): Sequential(\n                                  (0): ReflectionPad2d((1, 1, 1, 1))\n                                  (1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                                  (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                  (3): ReLU(inplace=True)\n                                  (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                                  (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                  (6): ReLU(inplace=True)\n                                  (7): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                                  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                  (9): ReLU(inplace=True)\n                                  (10): Dropout(p=0, inplace=False)\n                                  (11): ReflectionPad2d((1, 1, 1, 1))\n                                  (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                                  (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                )\n                              )\n                              (10): ResnetBlock(\n                                (conv_block): Sequential(\n                                  (0): ReflectionPad2d((1, 1, 1, 1))\n                                  (1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                                  (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                  (3): ReLU(inplace=True)\n                                  (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                                  (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                  (6): ReLU(inplace=True)\n                                  (7): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                                  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                  (9): ReLU(inplace=True)\n                                  (10): Dropout(p=0, inplace=False)\n                                  (11): ReflectionPad2d((1, 1, 1, 1))\n                                  (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                                  (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                                )\n                              )\n                              (11): Dropout(p=0, inplace=False)\n                            )\n                          )\n                          (6): ReLU(inplace=True)\n                          (7): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                          (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (9): ResnetBlock(\n                            (conv_block): Sequential(\n                              (0): ReflectionPad2d((1, 1, 1, 1))\n                              (1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                              (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (3): ReLU(inplace=True)\n                              (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                              (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (6): ReLU(inplace=True)\n                              (7): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                              (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (9): ReLU(inplace=True)\n                              (10): Dropout(p=0, inplace=False)\n                              (11): ReflectionPad2d((1, 1, 1, 1))\n                              (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                              (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                            )\n                          )\n                          (10): ResnetBlock(\n                            (conv_block): Sequential(\n                              (0): ReflectionPad2d((1, 1, 1, 1))\n                              (1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                              (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (3): ReLU(inplace=True)\n                              (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                              (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (6): ReLU(inplace=True)\n                              (7): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                              (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                              (9): ReLU(inplace=True)\n                              (10): Dropout(p=0, inplace=False)\n                              (11): ReflectionPad2d((1, 1, 1, 1))\n                              (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                              (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                            )\n                          )\n                          (11): Dropout(p=0, inplace=False)\n                        )\n                      )\n                      (6): ReLU(inplace=True)\n                      (7): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (9): ResnetBlock(\n                        (conv_block): Sequential(\n                          (0): ReflectionPad2d((1, 1, 1, 1))\n                          (1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (3): ReLU(inplace=True)\n                          (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                          (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (6): ReLU(inplace=True)\n                          (7): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                          (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (9): ReLU(inplace=True)\n                          (10): Dropout(p=0, inplace=False)\n                          (11): ReflectionPad2d((1, 1, 1, 1))\n                          (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                          (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                        )\n                      )\n                      (10): ResnetBlock(\n                        (conv_block): Sequential(\n                          (0): ReflectionPad2d((1, 1, 1, 1))\n                          (1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                          (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (3): ReLU(inplace=True)\n                          (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                          (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (6): ReLU(inplace=True)\n                          (7): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                          (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                          (9): ReLU(inplace=True)\n                          (10): Dropout(p=0, inplace=False)\n                          (11): ReflectionPad2d((1, 1, 1, 1))\n                          (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                          (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                        )\n                      )\n                      (11): Dropout(p=0, inplace=False)\n                    )\n                  )\n                  (6): ReLU(inplace=True)\n                  (7): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n                  (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (9): ResnetBlock(\n                    (conv_block): Sequential(\n                      (0): ReflectionPad2d((1, 1, 1, 1))\n                      (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (3): ReLU(inplace=True)\n                      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                      (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (6): ReLU(inplace=True)\n                      (7): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (9): ReLU(inplace=True)\n                      (10): Dropout(p=0, inplace=False)\n                      (11): ReflectionPad2d((1, 1, 1, 1))\n                      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                    )\n                  )\n                  (10): ResnetBlock(\n                    (conv_block): Sequential(\n                      (0): ReflectionPad2d((1, 1, 1, 1))\n                      (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (3): ReLU(inplace=True)\n                      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                      (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (6): ReLU(inplace=True)\n                      (7): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                      (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                      (9): ReLU(inplace=True)\n                      (10): Dropout(p=0, inplace=False)\n                      (11): ReflectionPad2d((1, 1, 1, 1))\n                      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                    )\n                  )\n                  (11): Dropout(p=0, inplace=False)\n                )\n              )\n              (6): ReLU(inplace=True)\n              (7): ConvTranspose2d(128, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n              (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (9): ResnetBlock(\n                (conv_block): Sequential(\n                  (0): ReflectionPad2d((1, 1, 1, 1))\n                  (1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (3): ReLU(inplace=True)\n                  (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (6): ReLU(inplace=True)\n                  (7): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (9): ReLU(inplace=True)\n                  (10): Dropout(p=0, inplace=False)\n                  (11): ReflectionPad2d((1, 1, 1, 1))\n                  (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                  (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                )\n              )\n              (10): ResnetBlock(\n                (conv_block): Sequential(\n                  (0): ReflectionPad2d((1, 1, 1, 1))\n                  (1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (3): ReLU(inplace=True)\n                  (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (6): ReLU(inplace=True)\n                  (7): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                  (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                  (9): ReLU(inplace=True)\n                  (10): Dropout(p=0, inplace=False)\n                  (11): ReflectionPad2d((1, 1, 1, 1))\n                  (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n                  (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n                )\n              )\n              (11): Dropout(p=0, inplace=False)\n            )\n          )\n          (6): ReLU(inplace=True)\n          (7): ConvTranspose2d(64, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n          (8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (9): ResnetBlock(\n            (conv_block): Sequential(\n              (0): ReflectionPad2d((1, 1, 1, 1))\n              (1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (3): ReLU(inplace=True)\n              (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n              (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (6): ReLU(inplace=True)\n              (7): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (9): ReLU(inplace=True)\n              (10): Dropout(p=0, inplace=False)\n              (11): ReflectionPad2d((1, 1, 1, 1))\n              (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n              (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (10): ResnetBlock(\n            (conv_block): Sequential(\n              (0): ReflectionPad2d((1, 1, 1, 1))\n              (1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (3): ReLU(inplace=True)\n              (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n              (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (6): ReLU(inplace=True)\n              (7): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              (9): ReLU(inplace=True)\n              (10): Dropout(p=0, inplace=False)\n              (11): ReflectionPad2d((1, 1, 1, 1))\n              (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n              (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (11): Dropout(p=0, inplace=False)\n        )\n      )\n      (4): ResnetBlock(\n        (conv_block): Sequential(\n          (0): ReflectionPad2d((1, 1, 1, 1))\n          (1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (3): ReLU(inplace=True)\n          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n          (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (6): ReLU(inplace=True)\n          (7): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (9): ReLU(inplace=True)\n          (10): Dropout(p=0, inplace=False)\n          (11): ReflectionPad2d((1, 1, 1, 1))\n          (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n          (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (5): ResnetBlock(\n        (conv_block): Sequential(\n          (0): ReflectionPad2d((1, 1, 1, 1))\n          (1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (3): ReLU(inplace=True)\n          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n          (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (6): ReLU(inplace=True)\n          (7): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (9): ReLU(inplace=True)\n          (10): Dropout(p=0, inplace=False)\n          (11): ReflectionPad2d((1, 1, 1, 1))\n          (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n          (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (6): ReLU(inplace=True)\n      (7): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n      (8): Tanh()\n    )\n  )\n)\n[Network netG_teacher_d] Total number of parameters : 44.172 M\nMultiNLayerDiscriminator(\n  (block1s): ModuleList(\n    (0-1): 2 x ConvReLU(\n      (block): Sequential(\n        (0): Conv2d(6, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n        (1): LeakyReLU(negative_slope=0.2, inplace=True)\n      )\n    )\n  )\n  (block2s): ModuleList(\n    (0-1): 2 x ConvBNReLU(\n      (block): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n      )\n    )\n  )\n  (block3s): ModuleList(\n    (0-1): 2 x ConvBNReLU(\n      (block): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n      )\n    )\n  )\n  (block4s): ModuleList(\n    (0-1): 2 x ConvBNReLU(\n      (block): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n      )\n    )\n  )\n  (block5s): ModuleList(\n    (0-1): 2 x Conv(\n      (block): Sequential(\n        (0): Conv2d(1024, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n      )\n    )\n  )\n)\n[Network netD_teacher] Total number of parameters : 22.085 M\n-----------------------------------------------\nEpoch      :   0%|                                       | 0/20 [00:00<?, ?it/s]\nBatch      :   0%|                                     | 0/1605 [00:00<?, ?it/s]\u001b[A/opt/conda/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::leaky_relu_\". Skipped.\n  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\nMACs: 1.219G\tParams: 3.404M\n/opt/conda/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::leaky_relu_\". Skipped.\n  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\nMACs: 18.143G\tParams: 54.414M\n/opt/conda/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::pad\". Skipped.\n  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n/opt/conda/lib/python3.10/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::leaky_relu_\". Skipped.\n  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\nMACs: 18.376G\tParams: 44.172M\n\nBatch      :   0%|                          | 1/1605 [00:30<13:25:37, 30.14s/it]\u001b[A\nBatch      :   0%|                           | 2/1605 [00:30<5:43:15, 12.85s/it]\u001b[A\nBatch      :   0%|                           | 3/1605 [00:31<3:15:41,  7.33s/it]\u001b[A\nBatch      :   0%|                           | 4/1605 [00:32<2:06:20,  4.73s/it]\u001b[A\nBatch      :   0%|                           | 5/1605 [00:33<1:27:51,  3.29s/it]\u001b[A\nBatch      :   0%|                           | 6/1605 [00:33<1:04:38,  2.43s/it]\u001b[A\nBatch      :   0%|                            | 7/1605 [00:34<49:56,  1.87s/it]\u001b[A\nBatch      :   0%|                            | 8/1605 [00:35<40:20,  1.52s/it]\u001b[A\nBatch      :   1%|                            | 9/1605 [00:36<33:53,  1.27s/it]\u001b[A\nBatch      :   1%|                           | 10/1605 [00:36<29:32,  1.11s/it]\u001b[A\nBatch      :   1%|                           | 11/1605 [00:37<26:34,  1.00s/it]\u001b[A\nBatch      :   1%|                           | 12/1605 [00:38<24:28,  1.08it/s]\u001b[A\nBatch      :   1%|                           | 13/1605 [00:39<22:59,  1.15it/s]\u001b[A\nBatch      :   1%|                           | 14/1605 [00:39<21:58,  1.21it/s]\u001b[A\nBatch      :   1%|                           | 15/1605 [00:40<21:14,  1.25it/s]\u001b[A\nBatch      :   1%|                           | 16/1605 [00:41<20:52,  1.27it/s]\u001b[A\nBatch      :   1%|                           | 17/1605 [00:42<20:35,  1.29it/s]\u001b[A\nBatch      :   1%|                           | 18/1605 [00:42<20:17,  1.30it/s]\u001b[A\nBatch      :   1%|                           | 19/1605 [00:43<20:05,  1.32it/s]\u001b[A\nBatch      :   1%|                           | 20/1605 [00:44<19:52,  1.33it/s]\u001b[A\nBatch      :   1%|                           | 21/1605 [00:45<19:47,  1.33it/s]\u001b[A\nBatch      :   1%|                           | 22/1605 [00:45<19:43,  1.34it/s]\u001b[A\nBatch      :   1%|                           | 23/1605 [00:46<19:39,  1.34it/s]\u001b[A\nBatch      :   1%|                           | 24/1605 [00:47<19:36,  1.34it/s]\u001b[A\nBatch      :   2%|                           | 25/1605 [00:48<19:33,  1.35it/s]\u001b[A\nBatch      :   2%|                           | 26/1605 [00:48<19:31,  1.35it/s]\u001b[A\nBatch      :   2%|                           | 27/1605 [00:49<19:29,  1.35it/s]\u001b[A\nBatch      :   2%|                           | 28/1605 [00:50<19:27,  1.35it/s]\u001b[A\nBatch      :   2%|                           | 29/1605 [00:50<19:26,  1.35it/s]\u001b[A\nBatch      :   2%|                           | 30/1605 [00:51<19:30,  1.35it/s]\u001b[A\nBatch      :   2%|                           | 31/1605 [00:52<19:26,  1.35it/s]\u001b[A\nBatch      :   2%|                           | 32/1605 [00:53<19:27,  1.35it/s]\u001b[A\nBatch      :   2%|                           | 33/1605 [00:53<19:25,  1.35it/s]\u001b[A\nBatch      :   2%|                           | 34/1605 [00:54<19:24,  1.35it/s]\u001b[A\nBatch      :   2%|                           | 35/1605 [00:55<19:28,  1.34it/s]\u001b[A\nBatch      :   2%|                           | 36/1605 [00:56<19:23,  1.35it/s]\u001b[A\nBatch      :   2%|                           | 37/1605 [00:56<19:24,  1.35it/s]\u001b[A\nBatch      :   2%|                           | 38/1605 [00:57<19:22,  1.35it/s]\u001b[A\nBatch      :   2%|                           | 39/1605 [00:58<19:19,  1.35it/s]\u001b[A\nBatch      :   2%|                           | 40/1605 [00:59<19:20,  1.35it/s]\u001b[A\nBatch      :   3%|                           | 41/1605 [00:59<19:18,  1.35it/s]\u001b[A\nBatch      :   3%|                           | 42/1605 [01:00<19:18,  1.35it/s]\u001b[A\nBatch      :   3%|                           | 43/1605 [01:01<19:16,  1.35it/s]\u001b[A\nBatch      :   3%|                           | 44/1605 [01:02<19:15,  1.35it/s]\u001b[A\nBatch      :   3%|                           | 45/1605 [01:02<19:14,  1.35it/s]\u001b[A\nBatch      :   3%|                           | 46/1605 [01:03<19:12,  1.35it/s]\u001b[A\nBatch      :   3%|                           | 47/1605 [01:04<19:11,  1.35it/s]\u001b[A\nBatch      :   3%|                           | 48/1605 [01:05<19:08,  1.36it/s]\u001b[A\nBatch      :   3%|                           | 49/1605 [01:05<19:09,  1.35it/s]\u001b[A\nBatch      :   3%|                           | 50/1605 [01:06<19:05,  1.36it/s]\u001b[A\nBatch      :   3%|                           | 51/1605 [01:07<19:04,  1.36it/s]\u001b[A\nBatch      :   3%|                           | 52/1605 [01:07<19:06,  1.35it/s]\u001b[A\nBatch      :   3%|                           | 53/1605 [01:08<19:08,  1.35it/s]\u001b[A\nBatch      :   3%|                           | 54/1605 [01:09<19:05,  1.35it/s]\u001b[A\nBatch      :   3%|                           | 55/1605 [01:10<19:07,  1.35it/s]\u001b[A\nBatch      :   3%|                           | 56/1605 [01:10<19:08,  1.35it/s]\u001b[A\nBatch      :   4%|                           | 57/1605 [01:11<19:14,  1.34it/s]\u001b[A\nBatch      :   4%|                           | 58/1605 [01:12<21:26,  1.20it/s]\u001b[A\nBatch      :   4%|                           | 59/1605 [01:13<20:47,  1.24it/s]\u001b[A\nBatch      :   4%|                           | 60/1605 [01:14<20:13,  1.27it/s]\u001b[A\nBatch      :   4%|                           | 61/1605 [01:14<19:50,  1.30it/s]\u001b[A\nBatch      :   4%|                           | 62/1605 [01:15<19:37,  1.31it/s]\u001b[A\nBatch      :   4%|                           | 63/1605 [01:16<19:24,  1.32it/s]\u001b[A\nBatch      :   4%|                           | 64/1605 [01:17<19:15,  1.33it/s]\u001b[A\nBatch      :   4%|                          | 65/1605 [01:17<19:11,  1.34it/s]\u001b[A\nBatch      :   4%|                          | 66/1605 [01:18<19:11,  1.34it/s]\u001b[A\nBatch      :   4%|                          | 67/1605 [01:19<19:08,  1.34it/s]\u001b[A\nBatch      :   4%|                          | 68/1605 [01:20<19:03,  1.34it/s]\u001b[A\nBatch      :   4%|                          | 69/1605 [01:20<18:59,  1.35it/s]\u001b[A\nBatch      :   4%|                          | 70/1605 [01:21<18:56,  1.35it/s]\u001b[A\nBatch      :   4%|                          | 71/1605 [01:22<18:58,  1.35it/s]\u001b[A\nBatch      :   4%|                          | 72/1605 [01:23<18:58,  1.35it/s]\u001b[A\nBatch      :   5%|                          | 73/1605 [01:23<18:59,  1.34it/s]\u001b[A\nBatch      :   5%|                          | 74/1605 [01:24<18:59,  1.34it/s]\u001b[A\nBatch      :   5%|                          | 75/1605 [01:25<19:03,  1.34it/s]\u001b[A\nBatch      :   5%|                          | 76/1605 [01:26<19:00,  1.34it/s]\u001b[A\nBatch      :   5%|                          | 77/1605 [01:26<18:58,  1.34it/s]\u001b[A\nBatch      :   5%|                          | 78/1605 [01:27<18:53,  1.35it/s]\u001b[A\nBatch      :   5%|                          | 79/1605 [01:28<18:50,  1.35it/s]\u001b[A\nBatch      :   5%|                          | 80/1605 [01:29<18:48,  1.35it/s]\u001b[A\nBatch      :   5%|                          | 81/1605 [01:29<18:48,  1.35it/s]\u001b[A\nBatch      :   5%|                          | 82/1605 [01:30<18:51,  1.35it/s]\u001b[A\nBatch      :   5%|                          | 83/1605 [01:31<18:51,  1.35it/s]\u001b[A\nBatch      :   5%|                          | 84/1605 [01:32<18:50,  1.34it/s]\u001b[A\nBatch      :   5%|                          | 85/1605 [01:32<18:48,  1.35it/s]\u001b[A\nBatch      :   5%|                          | 86/1605 [01:33<18:46,  1.35it/s]\u001b[A\nBatch      :   5%|                          | 87/1605 [01:34<18:44,  1.35it/s]\u001b[A\nBatch      :   5%|                          | 88/1605 [01:35<18:41,  1.35it/s]\u001b[A\nBatch      :   6%|                          | 89/1605 [01:35<18:42,  1.35it/s]\u001b[A\nBatch      :   6%|                          | 90/1605 [01:36<18:41,  1.35it/s]\u001b[A\nBatch      :   6%|                          | 91/1605 [01:37<18:40,  1.35it/s]\u001b[A\nBatch      :   6%|                          | 92/1605 [01:37<18:38,  1.35it/s]\u001b[A\nBatch      :   6%|                          | 93/1605 [01:38<18:38,  1.35it/s]\u001b[A\nBatch      :   6%|                          | 94/1605 [01:39<18:40,  1.35it/s]\u001b[A\nBatch      :   6%|                          | 95/1605 [01:40<18:41,  1.35it/s]\u001b[A\nBatch      :   6%|                          | 96/1605 [01:40<18:39,  1.35it/s]\u001b[A\nBatch      :   6%|                          | 97/1605 [01:41<18:38,  1.35it/s]\u001b[A\nBatch      :   6%|                          | 98/1605 [01:42<18:35,  1.35it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A(epoch: 1, iters: 100, time: 0.751) G_gan_w: 2.071 G_recon_w: 30.783 G_gan_d: 1.770 G_recon_d: 31.079 D_fake_w: 0.049 D_real_w: 0.656 D_fake_d: 0.146 D_real_d: 0.656 G_SSIM: 8.126 G_feature: 3.265 G_style: 9.471 G_tv: 1.125 G_CD: 0.394 \nEpoch      :   0%|                                       | 0/20 [01:43<?, ?it/s]\nBatch      :   6%|                          | 99/1605 [01:43<18:36,  1.35it/s]\u001b[A\nBatch      :   6%|                         | 100/1605 [01:43<18:43,  1.34it/s]\u001b[A\nBatch      :   6%|                         | 101/1605 [01:44<18:40,  1.34it/s]\u001b[A\nBatch      :   6%|                         | 102/1605 [01:45<18:42,  1.34it/s]\u001b[A\nBatch      :   6%|                         | 103/1605 [01:46<18:40,  1.34it/s]\u001b[A\nBatch      :   6%|                         | 104/1605 [01:46<18:39,  1.34it/s]\u001b[A\nBatch      :   7%|                         | 105/1605 [01:47<18:37,  1.34it/s]\u001b[A\nBatch      :   7%|                         | 106/1605 [01:48<18:35,  1.34it/s]\u001b[A\nBatch      :   7%|                         | 107/1605 [01:49<18:31,  1.35it/s]\u001b[A\nBatch      :   7%|                         | 108/1605 [01:49<18:28,  1.35it/s]\u001b[A\nBatch      :   7%|                         | 109/1605 [01:50<18:25,  1.35it/s]\u001b[A\nBatch      :   7%|                         | 110/1605 [01:51<18:22,  1.36it/s]\u001b[A\nBatch      :   7%|                         | 111/1605 [01:52<18:21,  1.36it/s]\u001b[A\nBatch      :   7%|                         | 112/1605 [01:52<18:20,  1.36it/s]\u001b[A\nBatch      :   7%|                         | 113/1605 [01:53<18:19,  1.36it/s]\u001b[A\nBatch      :   7%|                         | 114/1605 [01:54<18:22,  1.35it/s]\u001b[A\nBatch      :   7%|                         | 115/1605 [01:55<18:19,  1.36it/s]\u001b[A\nBatch      :   7%|                         | 116/1605 [01:55<18:20,  1.35it/s]\u001b[A\nBatch      :   7%|                         | 117/1605 [01:56<18:17,  1.36it/s]\u001b[A\nBatch      :   7%|                         | 118/1605 [01:57<18:15,  1.36it/s]\u001b[A\nBatch      :   7%|                         | 119/1605 [01:57<18:13,  1.36it/s]\u001b[A\nBatch      :   7%|                         | 120/1605 [01:58<18:12,  1.36it/s]\u001b[A\nBatch      :   8%|                         | 121/1605 [01:59<18:13,  1.36it/s]\u001b[A\nBatch      :   8%|                         | 122/1605 [02:00<18:13,  1.36it/s]\u001b[A\nBatch      :   8%|                         | 123/1605 [02:00<18:16,  1.35it/s]\u001b[A\nBatch      :   8%|                         | 124/1605 [02:01<18:17,  1.35it/s]\u001b[A\nBatch      :   8%|                         | 125/1605 [02:02<18:14,  1.35it/s]\u001b[A\nBatch      :   8%|                         | 126/1605 [02:03<18:13,  1.35it/s]\u001b[A\nBatch      :   8%|                        | 127/1605 [02:03<18:12,  1.35it/s]\u001b[A\nBatch      :   8%|                        | 128/1605 [02:04<18:12,  1.35it/s]\u001b[A\nBatch      :   8%|                        | 129/1605 [02:05<18:14,  1.35it/s]\u001b[A\nBatch      :   8%|                        | 130/1605 [02:06<18:13,  1.35it/s]\u001b[A\nBatch      :   8%|                        | 131/1605 [02:06<18:14,  1.35it/s]\u001b[A\nBatch      :   8%|                        | 132/1605 [02:07<18:15,  1.34it/s]\u001b[A\nBatch      :   8%|                        | 133/1605 [02:08<18:13,  1.35it/s]\u001b[A\nBatch      :   8%|                        | 134/1605 [02:09<18:12,  1.35it/s]\u001b[A\nBatch      :   8%|                        | 135/1605 [02:09<18:09,  1.35it/s]\u001b[A\nBatch      :   8%|                        | 136/1605 [02:10<18:06,  1.35it/s]\u001b[A\nBatch      :   9%|                        | 137/1605 [02:11<18:04,  1.35it/s]\u001b[A\nBatch      :   9%|                        | 138/1605 [02:12<18:03,  1.35it/s]\u001b[A\nBatch      :   9%|                        | 139/1605 [02:12<18:01,  1.36it/s]\u001b[A\nBatch      :   9%|                        | 140/1605 [02:13<18:01,  1.35it/s]\u001b[A\nBatch      :   9%|                        | 141/1605 [02:14<18:01,  1.35it/s]\u001b[A\nBatch      :   9%|                        | 142/1605 [02:14<18:08,  1.34it/s]\u001b[A\nBatch      :   9%|                        | 143/1605 [02:15<18:10,  1.34it/s]\u001b[A\nBatch      :   9%|                        | 144/1605 [02:16<18:09,  1.34it/s]\u001b[A\nBatch      :   9%|                        | 145/1605 [02:17<18:02,  1.35it/s]\u001b[A\nBatch      :   9%|                        | 146/1605 [02:17<17:57,  1.35it/s]\u001b[A\nBatch      :   9%|                        | 147/1605 [02:18<17:55,  1.36it/s]\u001b[A\nBatch      :   9%|                        | 148/1605 [02:19<17:54,  1.36it/s]\u001b[A\nBatch      :   9%|                        | 149/1605 [02:20<17:54,  1.36it/s]\u001b[A\nBatch      :   9%|                        | 150/1605 [02:20<17:54,  1.35it/s]\u001b[A\nBatch      :   9%|                        | 151/1605 [02:21<17:58,  1.35it/s]\u001b[A\nBatch      :   9%|                        | 152/1605 [02:22<17:56,  1.35it/s]\u001b[A\nBatch      :  10%|                        | 153/1605 [02:23<17:52,  1.35it/s]\u001b[A\nBatch      :  10%|                        | 154/1605 [02:23<17:49,  1.36it/s]\u001b[A\nBatch      :  10%|                        | 155/1605 [02:24<17:46,  1.36it/s]\u001b[A\nBatch      :  10%|                        | 156/1605 [02:25<17:52,  1.35it/s]\u001b[A\nBatch      :  10%|                        | 157/1605 [02:26<17:51,  1.35it/s]\u001b[A\nBatch      :  10%|                        | 158/1605 [02:26<17:50,  1.35it/s]\u001b[A\nBatch      :  10%|                        | 159/1605 [02:27<17:51,  1.35it/s]\u001b[A\nBatch      :  10%|                        | 160/1605 [02:28<17:49,  1.35it/s]\u001b[A\nBatch      :  10%|                        | 161/1605 [02:29<17:47,  1.35it/s]\u001b[A\nBatch      :  10%|                        | 162/1605 [02:29<17:48,  1.35it/s]\u001b[A\nBatch      :  10%|                        | 163/1605 [02:30<17:49,  1.35it/s]\u001b[A\nBatch      :  10%|                        | 164/1605 [02:31<17:48,  1.35it/s]\u001b[A\nBatch      :  10%|                        | 165/1605 [02:32<17:48,  1.35it/s]\u001b[A\nBatch      :  10%|                        | 166/1605 [02:32<17:46,  1.35it/s]\u001b[A\nBatch      :  10%|                        | 167/1605 [02:33<17:50,  1.34it/s]\u001b[A\nBatch      :  10%|                        | 168/1605 [02:34<17:51,  1.34it/s]\u001b[A\nBatch      :  11%|                        | 169/1605 [02:34<17:50,  1.34it/s]\u001b[A\nBatch      :  11%|                        | 170/1605 [02:35<17:50,  1.34it/s]\u001b[A\nBatch      :  11%|                        | 171/1605 [02:36<17:50,  1.34it/s]\u001b[A\nBatch      :  11%|                        | 172/1605 [02:37<17:46,  1.34it/s]\u001b[A\nBatch      :  11%|                        | 173/1605 [02:37<17:46,  1.34it/s]\u001b[A\nBatch      :  11%|                        | 174/1605 [02:38<17:43,  1.35it/s]\u001b[A\nBatch      :  11%|                        | 175/1605 [02:39<17:40,  1.35it/s]\u001b[A\nBatch      :  11%|                        | 176/1605 [02:40<17:40,  1.35it/s]\u001b[A\nBatch      :  11%|                        | 177/1605 [02:40<17:37,  1.35it/s]\u001b[A\nBatch      :  11%|                        | 178/1605 [02:41<17:35,  1.35it/s]\u001b[A\nBatch      :  11%|                        | 179/1605 [02:42<17:34,  1.35it/s]\u001b[A\nBatch      :  11%|                        | 180/1605 [02:43<17:32,  1.35it/s]\u001b[A\nBatch      :  11%|                        | 181/1605 [02:43<17:29,  1.36it/s]\u001b[A\nBatch      :  11%|                        | 182/1605 [02:44<17:28,  1.36it/s]\u001b[A\nBatch      :  11%|                        | 183/1605 [02:45<17:30,  1.35it/s]\u001b[A\nBatch      :  11%|                        | 184/1605 [02:46<17:33,  1.35it/s]\u001b[A\nBatch      :  12%|                        | 185/1605 [02:46<17:37,  1.34it/s]\u001b[A\nBatch      :  12%|                       | 186/1605 [02:47<17:33,  1.35it/s]\u001b[A\nBatch      :  12%|                       | 187/1605 [02:48<17:30,  1.35it/s]\u001b[A\nBatch      :  12%|                       | 188/1605 [02:49<17:29,  1.35it/s]\u001b[A\nBatch      :  12%|                       | 189/1605 [02:49<17:29,  1.35it/s]\u001b[A\nBatch      :  12%|                       | 190/1605 [02:50<17:29,  1.35it/s]\u001b[A\nBatch      :  12%|                       | 191/1605 [02:51<17:29,  1.35it/s]\u001b[A\nBatch      :  12%|                       | 192/1605 [02:52<17:30,  1.35it/s]\u001b[A\nBatch      :  12%|                       | 193/1605 [02:52<17:30,  1.34it/s]\u001b[A\nBatch      :  12%|                       | 194/1605 [02:53<17:25,  1.35it/s]\u001b[A\nBatch      :  12%|                       | 195/1605 [02:54<17:21,  1.35it/s]\u001b[A\nBatch      :  12%|                       | 196/1605 [02:54<17:19,  1.36it/s]\u001b[A\nBatch      :  12%|                       | 197/1605 [02:55<17:21,  1.35it/s]\u001b[A\nBatch      :  12%|                       | 198/1605 [02:56<17:20,  1.35it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A(epoch: 1, iters: 200, time: 0.733) G_gan_w: 1.421 G_recon_w: 31.826 G_gan_d: 2.502 G_recon_d: 31.860 D_fake_w: 0.215 D_real_w: 1.717 D_fake_d: 0.034 D_real_d: 1.717 G_SSIM: 7.353 G_feature: 3.064 G_style: 8.711 G_tv: 0.916 G_CD: 0.235 \nEpoch      :   0%|                                       | 0/20 [02:57<?, ?it/s]\nBatch      :  12%|                       | 199/1605 [02:57<17:18,  1.35it/s]\u001b[A\nBatch      :  12%|                       | 200/1605 [02:57<17:17,  1.35it/s]\u001b[A\nBatch      :  13%|                       | 201/1605 [02:58<17:19,  1.35it/s]\u001b[A\nBatch      :  13%|                       | 202/1605 [02:59<17:17,  1.35it/s]\u001b[A\nBatch      :  13%|                       | 203/1605 [03:00<17:15,  1.35it/s]\u001b[A\nBatch      :  13%|                       | 204/1605 [03:00<17:14,  1.35it/s]\u001b[A\nBatch      :  13%|                       | 205/1605 [03:01<17:16,  1.35it/s]\u001b[A\nBatch      :  13%|                       | 206/1605 [03:02<17:17,  1.35it/s]\u001b[A\nBatch      :  13%|                       | 207/1605 [03:03<17:20,  1.34it/s]\u001b[A\nBatch      :  13%|                       | 208/1605 [03:03<17:17,  1.35it/s]\u001b[A\nBatch      :  13%|                       | 209/1605 [03:04<17:13,  1.35it/s]\u001b[A\nBatch      :  13%|                       | 210/1605 [03:05<17:14,  1.35it/s]\u001b[A\nBatch      :  13%|                       | 211/1605 [03:06<17:11,  1.35it/s]\u001b[A\nBatch      :  13%|                       | 212/1605 [03:06<17:10,  1.35it/s]\u001b[A\nBatch      :  13%|                       | 213/1605 [03:07<17:08,  1.35it/s]\u001b[A\nBatch      :  13%|                       | 214/1605 [03:08<17:10,  1.35it/s]\u001b[A\nBatch      :  13%|                       | 215/1605 [03:09<17:08,  1.35it/s]\u001b[A\nBatch      :  13%|                       | 216/1605 [03:09<17:07,  1.35it/s]\u001b[A\nBatch      :  14%|                       | 217/1605 [03:10<17:05,  1.35it/s]\u001b[A\nBatch      :  14%|                       | 218/1605 [03:11<17:04,  1.35it/s]\u001b[A\nBatch      :  14%|                       | 219/1605 [03:12<17:04,  1.35it/s]\u001b[A\nBatch      :  14%|                       | 220/1605 [03:12<17:05,  1.35it/s]\u001b[A\nBatch      :  14%|                       | 221/1605 [03:13<17:07,  1.35it/s]\u001b[A\nBatch      :  14%|                       | 222/1605 [03:14<17:06,  1.35it/s]\u001b[A\nBatch      :  14%|                       | 223/1605 [03:14<17:03,  1.35it/s]\u001b[A\nBatch      :  14%|                       | 224/1605 [03:15<17:05,  1.35it/s]\u001b[A\nBatch      :  14%|                       | 225/1605 [03:16<17:04,  1.35it/s]\u001b[A\nBatch      :  14%|                       | 226/1605 [03:17<17:10,  1.34it/s]\u001b[A\nBatch      :  14%|                       | 227/1605 [03:17<17:11,  1.34it/s]\u001b[A\nBatch      :  14%|                       | 228/1605 [03:18<17:09,  1.34it/s]\u001b[A\nBatch      :  14%|                       | 229/1605 [03:19<17:06,  1.34it/s]\u001b[A\nBatch      :  14%|                       | 230/1605 [03:20<17:04,  1.34it/s]\u001b[A\nBatch      :  14%|                       | 231/1605 [03:20<17:00,  1.35it/s]\u001b[A\nBatch      :  14%|                       | 232/1605 [03:21<16:57,  1.35it/s]\u001b[A\nBatch      :  15%|                       | 233/1605 [03:22<16:57,  1.35it/s]\u001b[A\nBatch      :  15%|                       | 234/1605 [03:23<16:57,  1.35it/s]\u001b[A\nBatch      :  15%|                       | 235/1605 [03:23<16:55,  1.35it/s]\u001b[A\nBatch      :  15%|                       | 236/1605 [03:24<16:53,  1.35it/s]\u001b[A\nBatch      :  15%|                       | 237/1605 [03:25<16:55,  1.35it/s]\u001b[A\nBatch      :  15%|                       | 238/1605 [03:26<16:51,  1.35it/s]\u001b[A\nBatch      :  15%|                       | 239/1605 [03:26<16:49,  1.35it/s]\u001b[A\nBatch      :  15%|                       | 240/1605 [03:27<16:47,  1.36it/s]\u001b[A\nBatch      :  15%|                       | 241/1605 [03:28<16:46,  1.36it/s]\u001b[A\nBatch      :  15%|                       | 242/1605 [03:29<16:47,  1.35it/s]\u001b[A\nBatch      :  15%|                       | 243/1605 [03:29<16:45,  1.35it/s]\u001b[A\nBatch      :  15%|                       | 244/1605 [03:30<16:44,  1.35it/s]\u001b[A\nBatch      :  15%|                       | 245/1605 [03:31<16:42,  1.36it/s]\u001b[A\nBatch      :  15%|                      | 246/1605 [03:32<16:41,  1.36it/s]\u001b[A\nBatch      :  15%|                      | 247/1605 [03:32<16:39,  1.36it/s]\u001b[A\nBatch      :  15%|                      | 248/1605 [03:33<16:39,  1.36it/s]\u001b[A\nBatch      :  16%|                      | 249/1605 [03:34<16:42,  1.35it/s]\u001b[A\nBatch      :  16%|                      | 250/1605 [03:34<16:39,  1.36it/s]\u001b[A\nBatch      :  16%|                      | 251/1605 [03:35<16:40,  1.35it/s]\u001b[A\nBatch      :  16%|                      | 252/1605 [03:36<16:38,  1.36it/s]\u001b[A\nBatch      :  16%|                      | 253/1605 [03:37<16:35,  1.36it/s]\u001b[A\nBatch      :  16%|                      | 254/1605 [03:37<16:32,  1.36it/s]\u001b[A\nBatch      :  16%|                      | 255/1605 [03:38<16:30,  1.36it/s]\u001b[A\nBatch      :  16%|                      | 256/1605 [03:39<16:29,  1.36it/s]\u001b[A\nBatch      :  16%|                      | 257/1605 [03:40<16:30,  1.36it/s]\u001b[A\nBatch      :  16%|                      | 258/1605 [03:40<16:28,  1.36it/s]\u001b[A\nBatch      :  16%|                      | 259/1605 [03:41<16:27,  1.36it/s]\u001b[A\nBatch      :  16%|                      | 260/1605 [03:42<16:27,  1.36it/s]\u001b[A\nBatch      :  16%|                      | 261/1605 [03:43<16:27,  1.36it/s]\u001b[A\nBatch      :  16%|                      | 262/1605 [03:43<16:26,  1.36it/s]\u001b[A\nBatch      :  16%|                      | 263/1605 [03:44<16:28,  1.36it/s]\u001b[A\nBatch      :  16%|                      | 264/1605 [03:45<16:29,  1.36it/s]\u001b[A\nBatch      :  17%|                      | 265/1605 [03:46<16:33,  1.35it/s]\u001b[A\nBatch      :  17%|                      | 266/1605 [03:46<16:31,  1.35it/s]\u001b[A\nBatch      :  17%|                      | 267/1605 [03:47<16:29,  1.35it/s]\u001b[A\nBatch      :  17%|                      | 268/1605 [03:48<16:33,  1.35it/s]\u001b[A\nBatch      :  17%|                      | 269/1605 [03:49<16:32,  1.35it/s]\u001b[A\nBatch      :  17%|                      | 270/1605 [03:49<16:29,  1.35it/s]\u001b[A\nBatch      :  17%|                      | 271/1605 [03:50<16:28,  1.35it/s]\u001b[A\nBatch      :  17%|                      | 272/1605 [03:51<16:26,  1.35it/s]\u001b[A\nBatch      :  17%|                      | 273/1605 [03:51<16:24,  1.35it/s]\u001b[A\nBatch      :  17%|                      | 274/1605 [03:52<16:22,  1.35it/s]\u001b[A\nBatch      :  17%|                      | 275/1605 [03:53<16:20,  1.36it/s]\u001b[A\nBatch      :  17%|                      | 276/1605 [03:54<16:19,  1.36it/s]\u001b[A\nBatch      :  17%|                      | 277/1605 [03:54<16:19,  1.36it/s]\u001b[A\nBatch      :  17%|                      | 278/1605 [03:55<16:23,  1.35it/s]\u001b[A\nBatch      :  17%|                      | 279/1605 [03:56<16:21,  1.35it/s]\u001b[A\nBatch      :  17%|                      | 280/1605 [03:57<16:19,  1.35it/s]\u001b[A\nBatch      :  18%|                      | 281/1605 [03:57<16:17,  1.35it/s]\u001b[A\nBatch      :  18%|                      | 282/1605 [03:58<16:16,  1.35it/s]\u001b[A\nBatch      :  18%|                      | 283/1605 [03:59<16:16,  1.35it/s]\u001b[A\nBatch      :  18%|                      | 284/1605 [04:00<16:13,  1.36it/s]\u001b[A\nBatch      :  18%|                      | 285/1605 [04:00<16:13,  1.36it/s]\u001b[A\nBatch      :  18%|                      | 286/1605 [04:01<16:12,  1.36it/s]\u001b[A\nBatch      :  18%|                      | 287/1605 [04:02<16:11,  1.36it/s]\u001b[A\nBatch      :  18%|                      | 288/1605 [04:03<16:10,  1.36it/s]\u001b[A\nBatch      :  18%|                      | 289/1605 [04:03<16:09,  1.36it/s]\u001b[A\nBatch      :  18%|                      | 290/1605 [04:04<16:10,  1.36it/s]\u001b[A\nBatch      :  18%|                      | 291/1605 [04:05<16:11,  1.35it/s]\u001b[A\nBatch      :  18%|                      | 292/1605 [04:06<16:18,  1.34it/s]\u001b[A\nBatch      :  18%|                      | 293/1605 [04:06<16:14,  1.35it/s]\u001b[A\nBatch      :  18%|                      | 294/1605 [04:07<16:10,  1.35it/s]\u001b[A\nBatch      :  18%|                      | 295/1605 [04:08<16:08,  1.35it/s]\u001b[A\nBatch      :  18%|                      | 296/1605 [04:08<16:06,  1.35it/s]\u001b[A\nBatch      :  19%|                      | 297/1605 [04:09<16:05,  1.35it/s]\u001b[A\nBatch      :  19%|                      | 298/1605 [04:10<16:04,  1.35it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A(epoch: 1, iters: 300, time: 0.737) G_gan_w: 0.182 G_recon_w: 36.009 G_gan_d: 0.420 G_recon_d: 33.751 D_fake_w: 0.917 D_real_w: 0.131 D_fake_d: 0.780 D_real_d: 0.131 G_SSIM: 5.896 G_feature: 3.119 G_style: 7.987 G_tv: 1.013 G_CD: 0.272 \nEpoch      :   0%|                                       | 0/20 [04:11<?, ?it/s]\nBatch      :  19%|                      | 299/1605 [04:11<16:05,  1.35it/s]\u001b[A\nBatch      :  19%|                      | 300/1605 [04:11<16:05,  1.35it/s]\u001b[A\nBatch      :  19%|                      | 301/1605 [04:12<16:02,  1.36it/s]\u001b[A\nBatch      :  19%|                      | 302/1605 [04:13<16:00,  1.36it/s]\u001b[A\nBatch      :  19%|                      | 303/1605 [04:14<15:58,  1.36it/s]\u001b[A\nBatch      :  19%|                      | 304/1605 [04:14<15:58,  1.36it/s]\u001b[A\nBatch      :  19%|                     | 305/1605 [04:15<16:00,  1.35it/s]\u001b[A\nBatch      :  19%|                     | 306/1605 [04:16<16:00,  1.35it/s]\u001b[A\nBatch      :  19%|                     | 307/1605 [04:17<16:00,  1.35it/s]\u001b[A\nBatch      :  19%|                     | 308/1605 [04:17<15:58,  1.35it/s]\u001b[A\nBatch      :  19%|                     | 309/1605 [04:18<15:56,  1.35it/s]\u001b[A\nBatch      :  19%|                     | 310/1605 [04:19<16:02,  1.35it/s]\u001b[A\nBatch      :  19%|                     | 311/1605 [04:20<16:02,  1.34it/s]\u001b[A\nBatch      :  19%|                     | 312/1605 [04:20<15:59,  1.35it/s]\u001b[A\nBatch      :  20%|                     | 313/1605 [04:21<15:55,  1.35it/s]\u001b[A\nBatch      :  20%|                     | 314/1605 [04:22<15:53,  1.35it/s]\u001b[A\nBatch      :  20%|                     | 315/1605 [04:22<15:51,  1.36it/s]\u001b[A\nBatch      :  20%|                     | 316/1605 [04:23<15:50,  1.36it/s]\u001b[A\nBatch      :  20%|                     | 317/1605 [04:24<15:48,  1.36it/s]\u001b[A\nBatch      :  20%|                     | 318/1605 [04:25<15:48,  1.36it/s]\u001b[A\nBatch      :  20%|                     | 319/1605 [04:25<15:53,  1.35it/s]\u001b[A\nBatch      :  20%|                     | 320/1605 [04:26<15:51,  1.35it/s]\u001b[A\nBatch      :  20%|                     | 321/1605 [04:27<15:49,  1.35it/s]\u001b[A\nBatch      :  20%|                     | 322/1605 [04:28<15:46,  1.36it/s]\u001b[A\nBatch      :  20%|                     | 323/1605 [04:28<15:43,  1.36it/s]\u001b[A\nBatch      :  20%|                     | 324/1605 [04:29<15:43,  1.36it/s]\u001b[A\nBatch      :  20%|                     | 325/1605 [04:30<15:42,  1.36it/s]\u001b[A\nBatch      :  20%|                     | 326/1605 [04:31<15:40,  1.36it/s]\u001b[A\nBatch      :  20%|                     | 327/1605 [04:31<15:41,  1.36it/s]\u001b[A\nBatch      :  20%|                     | 328/1605 [04:32<15:40,  1.36it/s]\u001b[A\nBatch      :  20%|                     | 329/1605 [04:33<15:39,  1.36it/s]\u001b[A\nBatch      :  21%|                     | 330/1605 [04:34<15:39,  1.36it/s]\u001b[A\nBatch      :  21%|                     | 331/1605 [04:34<15:38,  1.36it/s]\u001b[A\nBatch      :  21%|                     | 332/1605 [04:35<15:44,  1.35it/s]\u001b[A\nBatch      :  21%|                     | 333/1605 [04:36<15:44,  1.35it/s]\u001b[A\nBatch      :  21%|                     | 334/1605 [04:37<15:44,  1.35it/s]\u001b[A\nBatch      :  21%|                     | 335/1605 [04:37<15:41,  1.35it/s]\u001b[A\nBatch      :  21%|                     | 336/1605 [04:38<15:39,  1.35it/s]\u001b[A\nBatch      :  21%|                     | 337/1605 [04:39<15:37,  1.35it/s]\u001b[A\nBatch      :  21%|                     | 338/1605 [04:39<15:35,  1.35it/s]\u001b[A\nBatch      :  21%|                     | 339/1605 [04:40<15:34,  1.35it/s]\u001b[A\nBatch      :  21%|                     | 340/1605 [04:41<15:31,  1.36it/s]\u001b[A\nBatch      :  21%|                     | 341/1605 [04:42<15:31,  1.36it/s]\u001b[A\nBatch      :  21%|                     | 342/1605 [04:42<15:31,  1.36it/s]\u001b[A\nBatch      :  21%|                     | 343/1605 [04:43<15:30,  1.36it/s]\u001b[A\nBatch      :  21%|                     | 344/1605 [04:44<15:31,  1.35it/s]\u001b[A\nBatch      :  21%|                     | 345/1605 [04:45<15:33,  1.35it/s]\u001b[A\nBatch      :  22%|                     | 346/1605 [04:45<15:33,  1.35it/s]\u001b[A\nBatch      :  22%|                     | 347/1605 [04:46<15:32,  1.35it/s]\u001b[A\nBatch      :  22%|                     | 348/1605 [04:47<15:28,  1.35it/s]\u001b[A\nBatch      :  22%|                     | 349/1605 [04:48<15:26,  1.36it/s]\u001b[A\nBatch      :  22%|                     | 350/1605 [04:48<15:26,  1.35it/s]\u001b[A\nBatch      :  22%|                     | 351/1605 [04:49<15:25,  1.36it/s]\u001b[A\nBatch      :  22%|                     | 352/1605 [04:50<15:28,  1.35it/s]\u001b[A\nBatch      :  22%|                     | 353/1605 [04:51<15:30,  1.35it/s]\u001b[A\nBatch      :  22%|                     | 354/1605 [04:51<15:30,  1.34it/s]\u001b[A\nBatch      :  22%|                     | 355/1605 [04:52<15:30,  1.34it/s]\u001b[A\nBatch      :  22%|                     | 356/1605 [04:53<15:28,  1.34it/s]\u001b[A\nBatch      :  22%|                     | 357/1605 [04:54<15:26,  1.35it/s]\u001b[A\nBatch      :  22%|                     | 358/1605 [04:54<15:24,  1.35it/s]\u001b[A\nBatch      :  22%|                     | 359/1605 [04:55<15:29,  1.34it/s]\u001b[A\nBatch      :  22%|                     | 360/1605 [04:56<15:24,  1.35it/s]\u001b[A\nBatch      :  22%|                     | 361/1605 [04:57<15:22,  1.35it/s]\u001b[A\nBatch      :  23%|                     | 362/1605 [04:57<15:21,  1.35it/s]\u001b[A\nBatch      :  23%|                     | 363/1605 [04:58<15:19,  1.35it/s]\u001b[A\nBatch      :  23%|                     | 364/1605 [04:59<15:20,  1.35it/s]\u001b[A\nBatch      :  23%|                    | 365/1605 [04:59<15:21,  1.35it/s]\u001b[A\nBatch      :  23%|                    | 366/1605 [05:00<15:20,  1.35it/s]\u001b[A\nBatch      :  23%|                    | 367/1605 [05:01<15:18,  1.35it/s]\u001b[A\nBatch      :  23%|                    | 368/1605 [05:02<15:17,  1.35it/s]\u001b[A\nBatch      :  23%|                    | 369/1605 [05:02<15:18,  1.35it/s]\u001b[A\nBatch      :  23%|                    | 370/1605 [05:03<15:17,  1.35it/s]\u001b[A\nBatch      :  23%|                    | 371/1605 [05:04<15:14,  1.35it/s]\u001b[A\nBatch      :  23%|                    | 372/1605 [05:05<15:12,  1.35it/s]\u001b[A\nBatch      :  23%|                    | 373/1605 [05:05<15:12,  1.35it/s]\u001b[A\nBatch      :  23%|                    | 374/1605 [05:06<15:13,  1.35it/s]\u001b[A\nBatch      :  23%|                    | 375/1605 [05:07<15:13,  1.35it/s]\u001b[A\nBatch      :  23%|                    | 376/1605 [05:08<15:14,  1.34it/s]\u001b[A\nBatch      :  23%|                    | 377/1605 [05:08<15:18,  1.34it/s]\u001b[A\nBatch      :  24%|                    | 378/1605 [05:09<15:13,  1.34it/s]\u001b[A\nBatch      :  24%|                    | 379/1605 [05:10<15:09,  1.35it/s]\u001b[A\nBatch      :  24%|                    | 380/1605 [05:11<15:06,  1.35it/s]\u001b[A\nBatch      :  24%|                    | 381/1605 [05:11<15:03,  1.36it/s]\u001b[A\nBatch      :  24%|                    | 382/1605 [05:12<15:01,  1.36it/s]\u001b[A\nBatch      :  24%|                    | 383/1605 [05:13<15:00,  1.36it/s]\u001b[A\nBatch      :  24%|                    | 384/1605 [05:14<14:58,  1.36it/s]\u001b[A\nBatch      :  24%|                    | 385/1605 [05:14<14:58,  1.36it/s]\u001b[A\nBatch      :  24%|                    | 386/1605 [05:15<15:04,  1.35it/s]\u001b[A\nBatch      :  24%|                    | 387/1605 [05:16<15:02,  1.35it/s]\u001b[A\nBatch      :  24%|                    | 388/1605 [05:17<14:59,  1.35it/s]\u001b[A\nBatch      :  24%|                    | 389/1605 [05:17<14:56,  1.36it/s]\u001b[A\nBatch      :  24%|                    | 390/1605 [05:18<14:55,  1.36it/s]\u001b[A\nBatch      :  24%|                    | 391/1605 [05:19<14:55,  1.36it/s]\u001b[A\nBatch      :  24%|                    | 392/1605 [05:19<14:55,  1.36it/s]\u001b[A\nBatch      :  24%|                    | 393/1605 [05:20<14:53,  1.36it/s]\u001b[A\nBatch      :  25%|                    | 394/1605 [05:21<14:57,  1.35it/s]\u001b[A\nBatch      :  25%|                    | 395/1605 [05:22<15:01,  1.34it/s]\u001b[A\nBatch      :  25%|                    | 396/1605 [05:22<14:58,  1.35it/s]\u001b[A\nBatch      :  25%|                    | 397/1605 [05:23<14:54,  1.35it/s]\u001b[A\nBatch      :  25%|                    | 398/1605 [05:24<14:50,  1.36it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A(epoch: 1, iters: 400, time: 0.738) G_gan_w: 0.627 G_recon_w: 23.363 G_gan_d: 0.554 G_recon_d: 26.395 D_fake_w: 0.434 D_real_w: 0.153 D_fake_d: 0.536 D_real_d: 0.153 G_SSIM: 6.254 G_feature: 3.098 G_style: 10.315 G_tv: 0.960 G_CD: 0.357 \nEpoch      :   0%|                                       | 0/20 [05:25<?, ?it/s]\nBatch      :  25%|                    | 399/1605 [05:25<14:49,  1.36it/s]\u001b[A\nBatch      :  25%|                    | 400/1605 [05:25<14:50,  1.35it/s]\u001b[A\nBatch      :  25%|                    | 401/1605 [05:26<14:50,  1.35it/s]\u001b[A\nBatch      :  25%|                    | 402/1605 [05:27<14:48,  1.35it/s]\u001b[A\nBatch      :  25%|                    | 403/1605 [05:28<14:48,  1.35it/s]\u001b[A\nBatch      :  25%|                    | 404/1605 [05:28<14:48,  1.35it/s]\u001b[A\nBatch      :  25%|                    | 405/1605 [05:29<14:47,  1.35it/s]\u001b[A\nBatch      :  25%|                    | 406/1605 [05:30<14:45,  1.35it/s]\u001b[A\nBatch      :  25%|                    | 407/1605 [05:31<14:43,  1.36it/s]\u001b[A\nBatch      :  25%|                    | 408/1605 [05:31<14:42,  1.36it/s]\u001b[A\nBatch      :  25%|                    | 409/1605 [05:32<14:40,  1.36it/s]\u001b[A\nBatch      :  26%|                    | 410/1605 [05:33<14:42,  1.35it/s]\u001b[A\nBatch      :  26%|                    | 411/1605 [05:34<14:42,  1.35it/s]\u001b[A\nBatch      :  26%|                    | 412/1605 [05:34<14:42,  1.35it/s]\u001b[A\nBatch      :  26%|                    | 413/1605 [05:35<14:44,  1.35it/s]\u001b[A\nBatch      :  26%|                    | 414/1605 [05:36<14:42,  1.35it/s]\u001b[A\nBatch      :  26%|                    | 415/1605 [05:36<14:41,  1.35it/s]\u001b[A\nBatch      :  26%|                    | 416/1605 [05:37<14:42,  1.35it/s]\u001b[A\nBatch      :  26%|                    | 417/1605 [05:38<14:40,  1.35it/s]\u001b[A\nBatch      :  26%|                    | 418/1605 [05:39<14:38,  1.35it/s]\u001b[A\nBatch      :  26%|                    | 419/1605 [05:39<14:37,  1.35it/s]\u001b[A\nBatch      :  26%|                    | 420/1605 [05:40<14:36,  1.35it/s]\u001b[A\nBatch      :  26%|                    | 421/1605 [05:41<14:34,  1.35it/s]\u001b[A\nBatch      :  26%|                    | 422/1605 [05:42<14:33,  1.35it/s]\u001b[A\nBatch      :  26%|                    | 423/1605 [05:42<14:33,  1.35it/s]\u001b[A\nBatch      :  26%|                   | 424/1605 [05:43<14:32,  1.35it/s]\u001b[A\nBatch      :  26%|                   | 425/1605 [05:44<14:30,  1.36it/s]\u001b[A\nBatch      :  27%|                   | 426/1605 [05:45<14:31,  1.35it/s]\u001b[A\nBatch      :  27%|                   | 427/1605 [05:45<14:33,  1.35it/s]\u001b[A\nBatch      :  27%|                   | 428/1605 [05:46<14:33,  1.35it/s]\u001b[A\nBatch      :  27%|                   | 429/1605 [05:47<14:33,  1.35it/s]\u001b[A\nBatch      :  27%|                   | 430/1605 [05:48<14:31,  1.35it/s]\u001b[A\nBatch      :  27%|                   | 431/1605 [05:48<14:30,  1.35it/s]\u001b[A\nBatch      :  27%|                   | 432/1605 [05:49<14:29,  1.35it/s]\u001b[A\nBatch      :  27%|                   | 433/1605 [05:50<14:29,  1.35it/s]\u001b[A\nBatch      :  27%|                   | 434/1605 [05:51<14:28,  1.35it/s]\u001b[A\nBatch      :  27%|                   | 435/1605 [05:51<14:30,  1.34it/s]\u001b[A\nBatch      :  27%|                   | 436/1605 [05:52<14:38,  1.33it/s]\u001b[A\nBatch      :  27%|                   | 437/1605 [05:53<14:36,  1.33it/s]\u001b[A\nBatch      :  27%|                   | 438/1605 [05:54<14:30,  1.34it/s]\u001b[A\nBatch      :  27%|                   | 439/1605 [05:54<14:27,  1.34it/s]\u001b[A\nBatch      :  27%|                   | 440/1605 [05:55<14:28,  1.34it/s]\u001b[A\nBatch      :  27%|                   | 441/1605 [05:56<14:26,  1.34it/s]\u001b[A\nBatch      :  28%|                   | 442/1605 [05:57<14:24,  1.35it/s]\u001b[A\nBatch      :  28%|                   | 443/1605 [05:57<14:22,  1.35it/s]\u001b[A\nBatch      :  28%|                   | 444/1605 [05:58<14:20,  1.35it/s]\u001b[A\nBatch      :  28%|                   | 445/1605 [05:59<14:19,  1.35it/s]\u001b[A\nBatch      :  28%|                   | 446/1605 [05:59<14:18,  1.35it/s]\u001b[A\nBatch      :  28%|                   | 447/1605 [06:00<14:17,  1.35it/s]\u001b[A\nBatch      :  28%|                   | 448/1605 [06:01<14:16,  1.35it/s]\u001b[A\nBatch      :  28%|                   | 449/1605 [06:02<14:18,  1.35it/s]\u001b[A\nBatch      :  28%|                   | 450/1605 [06:02<14:18,  1.35it/s]\u001b[A\nBatch      :  28%|                   | 451/1605 [06:03<14:16,  1.35it/s]\u001b[A\nBatch      :  28%|                   | 452/1605 [06:04<14:15,  1.35it/s]\u001b[A\nBatch      :  28%|                   | 453/1605 [06:05<14:13,  1.35it/s]\u001b[A\nBatch      :  28%|                   | 454/1605 [06:05<14:13,  1.35it/s]\u001b[A\nBatch      :  28%|                   | 455/1605 [06:06<14:12,  1.35it/s]\u001b[A\nBatch      :  28%|                   | 456/1605 [06:07<14:12,  1.35it/s]\u001b[A\nBatch      :  28%|                   | 457/1605 [06:08<14:09,  1.35it/s]\u001b[A\nBatch      :  29%|                   | 458/1605 [06:08<14:10,  1.35it/s]\u001b[A\nBatch      :  29%|                   | 459/1605 [06:09<14:07,  1.35it/s]\u001b[A\nBatch      :  29%|                   | 460/1605 [06:10<14:07,  1.35it/s]\u001b[A\nBatch      :  29%|                   | 461/1605 [06:11<14:06,  1.35it/s]\u001b[A\nBatch      :  29%|                   | 462/1605 [06:11<14:06,  1.35it/s]\u001b[A\nBatch      :  29%|                   | 463/1605 [06:12<14:06,  1.35it/s]\u001b[A\nBatch      :  29%|                   | 464/1605 [06:13<14:04,  1.35it/s]\u001b[A\nBatch      :  29%|                   | 465/1605 [06:14<14:02,  1.35it/s]\u001b[A\nBatch      :  29%|                   | 466/1605 [06:14<14:01,  1.35it/s]\u001b[A\nBatch      :  29%|                   | 467/1605 [06:15<14:03,  1.35it/s]\u001b[A\nBatch      :  29%|                   | 468/1605 [06:16<14:01,  1.35it/s]\u001b[A\nBatch      :  29%|                   | 469/1605 [06:17<14:01,  1.35it/s]\u001b[A\nBatch      :  29%|                   | 470/1605 [06:17<14:00,  1.35it/s]\u001b[A\nBatch      :  29%|                   | 471/1605 [06:18<13:58,  1.35it/s]\u001b[A\nBatch      :  29%|                   | 472/1605 [06:19<13:57,  1.35it/s]\u001b[A\nBatch      :  29%|                   | 473/1605 [06:19<13:56,  1.35it/s]\u001b[A\nBatch      :  30%|                   | 474/1605 [06:20<13:55,  1.35it/s]\u001b[A\nBatch      :  30%|                   | 475/1605 [06:21<13:53,  1.36it/s]\u001b[A\nBatch      :  30%|                   | 476/1605 [06:22<13:52,  1.36it/s]\u001b[A\nBatch      :  30%|                   | 477/1605 [06:22<13:51,  1.36it/s]\u001b[A\nBatch      :  30%|                   | 478/1605 [06:23<13:57,  1.35it/s]\u001b[A\nBatch      :  30%|                   | 479/1605 [06:24<14:01,  1.34it/s]\u001b[A\nBatch      :  30%|                   | 480/1605 [06:25<13:59,  1.34it/s]\u001b[A\nBatch      :  30%|                   | 481/1605 [06:25<14:01,  1.34it/s]\u001b[A\nBatch      :  30%|                   | 482/1605 [06:26<13:59,  1.34it/s]\u001b[A\nBatch      :  30%|                  | 483/1605 [06:27<13:59,  1.34it/s]\u001b[A\nBatch      :  30%|                  | 484/1605 [06:28<13:57,  1.34it/s]\u001b[A\nBatch      :  30%|                  | 485/1605 [06:28<13:52,  1.35it/s]\u001b[A\nBatch      :  30%|                  | 486/1605 [06:29<13:47,  1.35it/s]\u001b[A\nBatch      :  30%|                  | 487/1605 [06:30<13:46,  1.35it/s]\u001b[A\nBatch      :  30%|                  | 488/1605 [06:31<13:45,  1.35it/s]\u001b[A\nBatch      :  30%|                  | 489/1605 [06:31<13:44,  1.35it/s]\u001b[A\nBatch      :  31%|                  | 490/1605 [06:32<13:42,  1.36it/s]\u001b[A\nBatch      :  31%|                  | 491/1605 [06:33<13:42,  1.35it/s]\u001b[A\nBatch      :  31%|                  | 492/1605 [06:34<13:43,  1.35it/s]\u001b[A\nBatch      :  31%|                  | 493/1605 [06:34<13:43,  1.35it/s]\u001b[A\nBatch      :  31%|                  | 494/1605 [06:35<13:43,  1.35it/s]\u001b[A\nBatch      :  31%|                  | 495/1605 [06:36<13:40,  1.35it/s]\u001b[A\nBatch      :  31%|                  | 496/1605 [06:37<13:38,  1.36it/s]\u001b[A\nBatch      :  31%|                  | 497/1605 [06:37<13:35,  1.36it/s]\u001b[A\nBatch      :  31%|                  | 498/1605 [06:38<13:34,  1.36it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A(epoch: 1, iters: 500, time: 0.744) G_gan_w: 0.197 G_recon_w: 27.269 G_gan_d: 0.412 G_recon_d: 29.464 D_fake_w: 0.907 D_real_w: 0.344 D_fake_d: 0.745 D_real_d: 0.344 G_SSIM: 4.998 G_feature: 3.129 G_style: 6.794 G_tv: 1.144 G_CD: 0.152 \nEpoch      :   0%|                                       | 0/20 [06:39<?, ?it/s]\nBatch      :  31%|                  | 499/1605 [06:39<13:33,  1.36it/s]\u001b[A\nBatch      :  31%|                  | 500/1605 [06:39<13:37,  1.35it/s]\u001b[A\nBatch      :  31%|                  | 501/1605 [06:40<13:36,  1.35it/s]\u001b[A\nBatch      :  31%|                  | 502/1605 [06:41<13:36,  1.35it/s]\u001b[A\nBatch      :  31%|                  | 503/1605 [06:42<13:35,  1.35it/s]\u001b[A\nBatch      :  31%|                  | 504/1605 [06:42<13:33,  1.35it/s]\u001b[A\nBatch      :  31%|                  | 505/1605 [06:43<13:31,  1.36it/s]\u001b[A\nBatch      :  32%|                  | 506/1605 [06:44<13:30,  1.36it/s]\u001b[A\nBatch      :  32%|                  | 507/1605 [06:45<13:30,  1.35it/s]\u001b[A\nBatch      :  32%|                  | 508/1605 [06:45<13:30,  1.35it/s]\u001b[A\nBatch      :  32%|                  | 509/1605 [06:46<13:29,  1.35it/s]\u001b[A\nBatch      :  32%|                  | 510/1605 [06:47<13:30,  1.35it/s]\u001b[A\nBatch      :  32%|                  | 511/1605 [06:48<13:29,  1.35it/s]\u001b[A\nBatch      :  32%|                  | 512/1605 [06:48<13:27,  1.35it/s]\u001b[A\nBatch      :  32%|                  | 513/1605 [06:49<13:25,  1.36it/s]\u001b[A\nBatch      :  32%|                  | 514/1605 [06:50<13:24,  1.36it/s]\u001b[A\nBatch      :  32%|                  | 515/1605 [06:51<13:23,  1.36it/s]\u001b[A\nBatch      :  32%|                  | 516/1605 [06:51<13:22,  1.36it/s]\u001b[A\nBatch      :  32%|                  | 517/1605 [06:52<13:22,  1.36it/s]\u001b[A\nBatch      :  32%|                  | 518/1605 [06:53<13:21,  1.36it/s]\u001b[A\nBatch      :  32%|                  | 519/1605 [06:54<13:20,  1.36it/s]\u001b[A\nBatch      :  32%|                  | 520/1605 [06:54<13:25,  1.35it/s]\u001b[A\nBatch      :  32%|                  | 521/1605 [06:55<13:32,  1.33it/s]\u001b[A\nBatch      :  33%|                  | 522/1605 [06:56<13:29,  1.34it/s]\u001b[A\nBatch      :  33%|                  | 523/1605 [06:57<13:27,  1.34it/s]\u001b[A\nBatch      :  33%|                  | 524/1605 [06:57<13:25,  1.34it/s]\u001b[A\nBatch      :  33%|                  | 525/1605 [06:58<13:22,  1.35it/s]\u001b[A\nBatch      :  33%|                  | 526/1605 [06:59<13:19,  1.35it/s]\u001b[A\nBatch      :  33%|                  | 527/1605 [06:59<13:18,  1.35it/s]\u001b[A\nBatch      :  33%|                  | 528/1605 [07:00<13:17,  1.35it/s]\u001b[A\nBatch      :  33%|                  | 529/1605 [07:01<13:16,  1.35it/s]\u001b[A\nBatch      :  33%|                  | 530/1605 [07:02<13:18,  1.35it/s]\u001b[A\nBatch      :  33%|                  | 531/1605 [07:02<13:17,  1.35it/s]\u001b[A\nBatch      :  33%|                  | 532/1605 [07:03<13:16,  1.35it/s]\u001b[A\nBatch      :  33%|                  | 533/1605 [07:04<13:15,  1.35it/s]\u001b[A\nBatch      :  33%|                  | 534/1605 [07:05<13:16,  1.35it/s]\u001b[A\nBatch      :  33%|                  | 535/1605 [07:05<13:16,  1.34it/s]\u001b[A\nBatch      :  33%|                  | 536/1605 [07:06<13:15,  1.34it/s]\u001b[A\nBatch      :  33%|                  | 537/1605 [07:07<13:17,  1.34it/s]\u001b[A\nBatch      :  34%|                  | 538/1605 [07:08<13:16,  1.34it/s]\u001b[A\nBatch      :  34%|                  | 539/1605 [07:08<13:16,  1.34it/s]\u001b[A\nBatch      :  34%|                  | 540/1605 [07:09<13:15,  1.34it/s]\u001b[A\nBatch      :  34%|                  | 541/1605 [07:10<13:12,  1.34it/s]\u001b[A\nBatch      :  34%|                  | 542/1605 [07:11<13:12,  1.34it/s]\u001b[A\nBatch      :  34%|                 | 543/1605 [07:11<13:12,  1.34it/s]\u001b[A\nBatch      :  34%|                 | 544/1605 [07:12<13:13,  1.34it/s]\u001b[A\nBatch      :  34%|                 | 545/1605 [07:13<13:11,  1.34it/s]\u001b[A\nBatch      :  34%|                 | 546/1605 [07:14<13:10,  1.34it/s]\u001b[A\nBatch      :  34%|                 | 547/1605 [07:14<13:09,  1.34it/s]\u001b[A\nBatch      :  34%|                 | 548/1605 [07:15<13:11,  1.34it/s]\u001b[A\nBatch      :  34%|                 | 549/1605 [07:16<13:06,  1.34it/s]\u001b[A\nBatch      :  34%|                 | 550/1605 [07:17<13:04,  1.35it/s]\u001b[A\nBatch      :  34%|                 | 551/1605 [07:17<13:00,  1.35it/s]\u001b[A\nBatch      :  34%|                 | 552/1605 [07:18<13:00,  1.35it/s]\u001b[A\nBatch      :  34%|                 | 553/1605 [07:19<12:58,  1.35it/s]\u001b[A\nBatch      :  35%|                 | 554/1605 [07:20<12:56,  1.35it/s]\u001b[A\nBatch      :  35%|                 | 555/1605 [07:20<12:56,  1.35it/s]\u001b[A\nBatch      :  35%|                 | 556/1605 [07:21<12:55,  1.35it/s]\u001b[A\nBatch      :  35%|                 | 557/1605 [07:22<12:54,  1.35it/s]\u001b[A\nBatch      :  35%|                 | 558/1605 [07:23<12:52,  1.35it/s]\u001b[A\nBatch      :  35%|                 | 559/1605 [07:23<12:51,  1.36it/s]\u001b[A\nBatch      :  35%|                 | 560/1605 [07:24<12:49,  1.36it/s]\u001b[A\nBatch      :  35%|                 | 561/1605 [07:25<12:48,  1.36it/s]\u001b[A\nBatch      :  35%|                 | 562/1605 [07:25<12:55,  1.34it/s]\u001b[A\nBatch      :  35%|                 | 563/1605 [07:26<12:55,  1.34it/s]\u001b[A\nBatch      :  35%|                 | 564/1605 [07:27<12:52,  1.35it/s]\u001b[A\nBatch      :  35%|                 | 565/1605 [07:28<12:51,  1.35it/s]\u001b[A\nBatch      :  35%|                 | 566/1605 [07:28<12:51,  1.35it/s]\u001b[A\nBatch      :  35%|                 | 567/1605 [07:29<12:50,  1.35it/s]\u001b[A\nBatch      :  35%|                 | 568/1605 [07:30<12:48,  1.35it/s]\u001b[A\nBatch      :  35%|                 | 569/1605 [07:31<12:46,  1.35it/s]\u001b[A\nBatch      :  36%|                 | 570/1605 [07:31<12:44,  1.35it/s]\u001b[A\nBatch      :  36%|                 | 571/1605 [07:32<12:42,  1.36it/s]\u001b[A\nBatch      :  36%|                 | 572/1605 [07:33<12:42,  1.35it/s]\u001b[A\nBatch      :  36%|                 | 573/1605 [07:34<12:43,  1.35it/s]\u001b[A\nBatch      :  36%|                 | 574/1605 [07:34<12:42,  1.35it/s]\u001b[A\nBatch      :  36%|                 | 575/1605 [07:35<12:47,  1.34it/s]\u001b[A\nBatch      :  36%|                 | 576/1605 [07:36<12:44,  1.35it/s]\u001b[A\nBatch      :  36%|                 | 577/1605 [07:37<12:42,  1.35it/s]\u001b[A\nBatch      :  36%|                 | 578/1605 [07:37<12:41,  1.35it/s]\u001b[A\nBatch      :  36%|                 | 579/1605 [07:38<12:38,  1.35it/s]\u001b[A\nBatch      :  36%|                 | 580/1605 [07:39<12:37,  1.35it/s]\u001b[A\nBatch      :  36%|                 | 581/1605 [07:40<12:36,  1.35it/s]\u001b[A\nBatch      :  36%|                 | 582/1605 [07:40<12:34,  1.36it/s]\u001b[A\nBatch      :  36%|                 | 583/1605 [07:41<12:35,  1.35it/s]\u001b[A\nBatch      :  36%|                 | 584/1605 [07:42<12:33,  1.36it/s]\u001b[A\nBatch      :  36%|                 | 585/1605 [07:42<12:31,  1.36it/s]\u001b[A\nBatch      :  37%|                 | 586/1605 [07:43<12:32,  1.35it/s]\u001b[A\nBatch      :  37%|                 | 587/1605 [07:44<12:32,  1.35it/s]\u001b[A\nBatch      :  37%|                 | 588/1605 [07:45<12:34,  1.35it/s]\u001b[A\nBatch      :  37%|                 | 589/1605 [07:45<12:37,  1.34it/s]\u001b[A\nBatch      :  37%|                 | 590/1605 [07:46<12:34,  1.35it/s]\u001b[A\nBatch      :  37%|                 | 591/1605 [07:47<12:31,  1.35it/s]\u001b[A\nBatch      :  37%|                 | 592/1605 [07:48<12:29,  1.35it/s]\u001b[A\nBatch      :  37%|                 | 593/1605 [07:48<12:26,  1.36it/s]\u001b[A\nBatch      :  37%|                 | 594/1605 [07:49<12:25,  1.36it/s]\u001b[A\nBatch      :  37%|                 | 595/1605 [07:50<12:24,  1.36it/s]\u001b[A\nBatch      :  37%|                 | 596/1605 [07:51<12:23,  1.36it/s]\u001b[A\nBatch      :  37%|                 | 597/1605 [07:51<12:22,  1.36it/s]\u001b[A\nBatch      :  37%|                 | 598/1605 [07:52<12:23,  1.35it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A(epoch: 1, iters: 600, time: 0.745) G_gan_w: -0.233 G_recon_w: 38.248 G_gan_d: 0.442 G_recon_d: 37.843 D_fake_w: 1.245 D_real_w: 0.046 D_fake_d: 0.647 D_real_d: 0.046 G_SSIM: 5.062 G_feature: 2.769 G_style: 5.962 G_tv: 0.984 G_CD: 0.154 \nEpoch      :   0%|                                       | 0/20 [07:54<?, ?it/s]\nBatch      :  37%|                 | 599/1605 [07:54<12:25,  1.35it/s]\u001b[A\nBatch      :  37%|                 | 600/1605 [07:54<12:28,  1.34it/s]\u001b[A\nBatch      :  37%|                 | 601/1605 [07:54<12:28,  1.34it/s]\u001b[A\nBatch      :  38%|                | 602/1605 [07:55<12:31,  1.33it/s]\u001b[A\nBatch      :  38%|                | 603/1605 [07:56<12:28,  1.34it/s]\u001b[A\nBatch      :  38%|                | 604/1605 [07:57<12:30,  1.33it/s]\u001b[A\nBatch      :  38%|                | 605/1605 [07:57<12:29,  1.33it/s]\u001b[A\nBatch      :  38%|                | 606/1605 [07:58<12:24,  1.34it/s]\u001b[A\nBatch      :  38%|                | 607/1605 [07:59<12:20,  1.35it/s]\u001b[A\nBatch      :  38%|                | 608/1605 [08:00<12:18,  1.35it/s]\u001b[A\nBatch      :  38%|                | 609/1605 [08:00<12:17,  1.35it/s]\u001b[A\nBatch      :  38%|                | 610/1605 [08:01<12:16,  1.35it/s]\u001b[A\nBatch      :  38%|                | 611/1605 [08:02<12:15,  1.35it/s]\u001b[A\nBatch      :  38%|                | 612/1605 [08:03<12:14,  1.35it/s]\u001b[A\nBatch      :  38%|                | 613/1605 [08:03<12:12,  1.35it/s]\u001b[A\nBatch      :  38%|                | 614/1605 [08:04<12:11,  1.35it/s]\u001b[A\nBatch      :  38%|                | 615/1605 [08:05<12:12,  1.35it/s]\u001b[A\nBatch      :  38%|                | 616/1605 [08:06<12:15,  1.34it/s]\u001b[A\nBatch      :  38%|                | 617/1605 [08:06<12:13,  1.35it/s]\u001b[A\nBatch      :  39%|                | 618/1605 [08:07<12:11,  1.35it/s]\u001b[A\nBatch      :  39%|                | 619/1605 [08:08<12:10,  1.35it/s]\u001b[A\nBatch      :  39%|                | 620/1605 [08:08<12:11,  1.35it/s]\u001b[A\nBatch      :  39%|                | 621/1605 [08:09<12:08,  1.35it/s]\u001b[A\nBatch      :  39%|                | 622/1605 [08:10<12:06,  1.35it/s]\u001b[A\nBatch      :  39%|                | 623/1605 [08:11<12:06,  1.35it/s]\u001b[A\nBatch      :  39%|                | 624/1605 [08:11<12:05,  1.35it/s]\u001b[A\nBatch      :  39%|                | 625/1605 [08:12<12:03,  1.36it/s]\u001b[A\nBatch      :  39%|                | 626/1605 [08:13<12:02,  1.35it/s]\u001b[A\nBatch      :  39%|                | 627/1605 [08:14<12:01,  1.36it/s]\u001b[A\nBatch      :  39%|                | 628/1605 [08:14<12:02,  1.35it/s]\u001b[A\nBatch      :  39%|                | 629/1605 [08:15<12:04,  1.35it/s]\u001b[A\nBatch      :  39%|                | 630/1605 [08:16<12:04,  1.35it/s]\u001b[A\nBatch      :  39%|                | 631/1605 [08:17<12:03,  1.35it/s]\u001b[A\nBatch      :  39%|                | 632/1605 [08:17<12:01,  1.35it/s]\u001b[A\nBatch      :  39%|                | 633/1605 [08:18<12:01,  1.35it/s]\u001b[A\nBatch      :  40%|                | 634/1605 [08:19<12:01,  1.35it/s]\u001b[A\nBatch      :  40%|                | 635/1605 [08:20<11:59,  1.35it/s]\u001b[A\nBatch      :  40%|                | 636/1605 [08:20<11:57,  1.35it/s]\u001b[A\nBatch      :  40%|                | 637/1605 [08:21<11:58,  1.35it/s]\u001b[A\nBatch      :  40%|                | 638/1605 [08:22<11:56,  1.35it/s]\u001b[A\nBatch      :  40%|                | 639/1605 [08:23<11:55,  1.35it/s]\u001b[A\nBatch      :  40%|                | 640/1605 [08:23<11:55,  1.35it/s]\u001b[A\nBatch      :  40%|                | 641/1605 [08:24<11:55,  1.35it/s]\u001b[A\nBatch      :  40%|                | 642/1605 [08:25<11:52,  1.35it/s]\u001b[A\nBatch      :  40%|                | 643/1605 [08:26<11:54,  1.35it/s]\u001b[A\nBatch      :  40%|                | 644/1605 [08:26<11:51,  1.35it/s]\u001b[A\nBatch      :  40%|                | 645/1605 [08:27<11:52,  1.35it/s]\u001b[A\nBatch      :  40%|                | 646/1605 [08:28<11:55,  1.34it/s]\u001b[A\nBatch      :  40%|                | 647/1605 [08:28<11:53,  1.34it/s]\u001b[A\nBatch      :  40%|                | 648/1605 [08:29<11:51,  1.34it/s]\u001b[A\nBatch      :  40%|                | 649/1605 [08:30<11:50,  1.35it/s]\u001b[A\nBatch      :  40%|                | 650/1605 [08:31<11:48,  1.35it/s]\u001b[A\nBatch      :  41%|                | 651/1605 [08:31<11:46,  1.35it/s]\u001b[A\nBatch      :  41%|                | 652/1605 [08:32<11:47,  1.35it/s]\u001b[A\nBatch      :  41%|                | 653/1605 [08:33<11:47,  1.35it/s]\u001b[A\nBatch      :  41%|                | 654/1605 [08:34<11:47,  1.34it/s]\u001b[A\nBatch      :  41%|                | 655/1605 [08:34<11:45,  1.35it/s]\u001b[A\nBatch      :  41%|                | 656/1605 [08:35<11:48,  1.34it/s]\u001b[A\nBatch      :  41%|                | 657/1605 [08:36<11:48,  1.34it/s]\u001b[A\nBatch      :  41%|                | 658/1605 [08:37<11:47,  1.34it/s]\u001b[A\nBatch      :  41%|                | 659/1605 [08:37<11:43,  1.35it/s]\u001b[A\nBatch      :  41%|                | 660/1605 [08:38<11:39,  1.35it/s]\u001b[A\nBatch      :  41%|                | 661/1605 [08:39<11:37,  1.35it/s]\u001b[A\nBatch      :  41%|               | 662/1605 [08:40<11:35,  1.35it/s]\u001b[A\nBatch      :  41%|               | 663/1605 [08:40<11:34,  1.36it/s]\u001b[A\nBatch      :  41%|               | 664/1605 [08:41<11:33,  1.36it/s]\u001b[A\nBatch      :  41%|               | 665/1605 [08:42<11:31,  1.36it/s]\u001b[A\nBatch      :  41%|               | 666/1605 [08:43<11:30,  1.36it/s]\u001b[A\nBatch      :  42%|               | 667/1605 [08:43<11:29,  1.36it/s]\u001b[A\nBatch      :  42%|               | 668/1605 [08:44<11:30,  1.36it/s]\u001b[A\nBatch      :  42%|               | 669/1605 [08:45<11:29,  1.36it/s]\u001b[A\nBatch      :  42%|               | 670/1605 [08:46<11:33,  1.35it/s]\u001b[A\nBatch      :  42%|               | 671/1605 [08:46<11:32,  1.35it/s]\u001b[A\nBatch      :  42%|               | 672/1605 [08:47<11:32,  1.35it/s]\u001b[A\nBatch      :  42%|               | 673/1605 [08:48<11:33,  1.34it/s]\u001b[A\nBatch      :  42%|               | 674/1605 [08:48<11:32,  1.35it/s]\u001b[A\nBatch      :  42%|               | 675/1605 [08:49<11:29,  1.35it/s]\u001b[A\nBatch      :  42%|               | 676/1605 [08:50<11:28,  1.35it/s]\u001b[A\nBatch      :  42%|               | 677/1605 [08:51<11:26,  1.35it/s]\u001b[A\nBatch      :  42%|               | 678/1605 [08:51<11:27,  1.35it/s]\u001b[A\nBatch      :  42%|               | 679/1605 [08:52<11:26,  1.35it/s]\u001b[A\nBatch      :  42%|               | 680/1605 [08:53<11:24,  1.35it/s]\u001b[A\nBatch      :  42%|               | 681/1605 [08:54<11:23,  1.35it/s]\u001b[A\nBatch      :  42%|               | 682/1605 [08:54<11:22,  1.35it/s]\u001b[A\nBatch      :  43%|               | 683/1605 [08:55<11:24,  1.35it/s]\u001b[A\nBatch      :  43%|               | 684/1605 [08:56<11:22,  1.35it/s]\u001b[A\nBatch      :  43%|               | 685/1605 [08:57<11:20,  1.35it/s]\u001b[A\nBatch      :  43%|               | 686/1605 [08:57<11:19,  1.35it/s]\u001b[A\nBatch      :  43%|               | 687/1605 [08:58<11:17,  1.35it/s]\u001b[A\nBatch      :  43%|               | 688/1605 [08:59<11:24,  1.34it/s]\u001b[A\nBatch      :  43%|               | 689/1605 [09:00<11:22,  1.34it/s]\u001b[A\nBatch      :  43%|               | 690/1605 [09:00<11:20,  1.34it/s]\u001b[A\nBatch      :  43%|               | 691/1605 [09:01<11:18,  1.35it/s]\u001b[A\nBatch      :  43%|               | 692/1605 [09:02<11:16,  1.35it/s]\u001b[A\nBatch      :  43%|               | 693/1605 [09:03<11:15,  1.35it/s]\u001b[A\nBatch      :  43%|               | 694/1605 [09:03<11:13,  1.35it/s]\u001b[A\nBatch      :  43%|               | 695/1605 [09:04<11:12,  1.35it/s]\u001b[A\nBatch      :  43%|               | 696/1605 [09:05<11:11,  1.35it/s]\u001b[A\nBatch      :  43%|               | 697/1605 [09:06<11:14,  1.35it/s]\u001b[A\nBatch      :  43%|               | 698/1605 [09:06<11:12,  1.35it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A(epoch: 1, iters: 700, time: 0.732) G_gan_w: -0.238 G_recon_w: 33.651 G_gan_d: 0.535 G_recon_d: 32.877 D_fake_w: 1.238 D_real_w: 0.361 D_fake_d: 0.480 D_real_d: 0.361 G_SSIM: 6.446 G_feature: 3.135 G_style: 7.851 G_tv: 1.098 G_CD: 0.243 \nEpoch      :   0%|                                       | 0/20 [09:08<?, ?it/s]\nBatch      :  44%|               | 699/1605 [09:08<11:11,  1.35it/s]\u001b[A\nBatch      :  44%|               | 700/1605 [09:08<11:09,  1.35it/s]\u001b[A\nBatch      :  44%|               | 701/1605 [09:08<11:10,  1.35it/s]\u001b[A\nBatch      :  44%|               | 702/1605 [09:09<11:10,  1.35it/s]\u001b[A\nBatch      :  44%|               | 703/1605 [09:10<11:08,  1.35it/s]\u001b[A\nBatch      :  44%|               | 704/1605 [09:11<11:06,  1.35it/s]\u001b[A\nBatch      :  44%|               | 705/1605 [09:11<11:04,  1.35it/s]\u001b[A\nBatch      :  44%|               | 706/1605 [09:12<11:03,  1.36it/s]\u001b[A\nBatch      :  44%|               | 707/1605 [09:13<11:01,  1.36it/s]\u001b[A\nBatch      :  44%|               | 708/1605 [09:14<11:00,  1.36it/s]\u001b[A\nBatch      :  44%|               | 709/1605 [09:14<11:00,  1.36it/s]\u001b[A\nBatch      :  44%|               | 710/1605 [09:15<11:04,  1.35it/s]\u001b[A\nBatch      :  44%|               | 711/1605 [09:16<11:03,  1.35it/s]\u001b[A\nBatch      :  44%|               | 712/1605 [09:17<11:00,  1.35it/s]\u001b[A\nBatch      :  44%|               | 713/1605 [09:17<10:59,  1.35it/s]\u001b[A\nBatch      :  44%|               | 714/1605 [09:18<11:00,  1.35it/s]\u001b[A\nBatch      :  45%|               | 715/1605 [09:19<11:03,  1.34it/s]\u001b[A\nBatch      :  45%|               | 716/1605 [09:20<11:00,  1.35it/s]\u001b[A\nBatch      :  45%|               | 717/1605 [09:20<10:58,  1.35it/s]\u001b[A\nBatch      :  45%|               | 718/1605 [09:21<10:56,  1.35it/s]\u001b[A\nBatch      :  45%|               | 719/1605 [09:22<10:55,  1.35it/s]\u001b[A\nBatch      :  45%|               | 720/1605 [09:23<10:54,  1.35it/s]\u001b[A\nBatch      :  45%|              | 721/1605 [09:23<10:53,  1.35it/s]\u001b[A\nBatch      :  45%|              | 722/1605 [09:24<10:51,  1.36it/s]\u001b[A\nBatch      :  45%|              | 723/1605 [09:25<10:51,  1.35it/s]\u001b[A\nBatch      :  45%|              | 724/1605 [09:26<10:55,  1.34it/s]\u001b[A\nBatch      :  45%|              | 725/1605 [09:26<10:53,  1.35it/s]\u001b[A\nBatch      :  45%|              | 726/1605 [09:27<10:52,  1.35it/s]\u001b[A\nBatch      :  45%|              | 727/1605 [09:28<10:49,  1.35it/s]\u001b[A\nBatch      :  45%|              | 728/1605 [09:28<10:48,  1.35it/s]\u001b[A\nBatch      :  45%|              | 729/1605 [09:29<10:47,  1.35it/s]\u001b[A\nBatch      :  45%|              | 730/1605 [09:30<10:50,  1.35it/s]\u001b[A\nBatch      :  46%|              | 731/1605 [09:31<10:51,  1.34it/s]\u001b[A\nBatch      :  46%|              | 732/1605 [09:31<10:47,  1.35it/s]\u001b[A\nBatch      :  46%|              | 733/1605 [09:32<10:47,  1.35it/s]\u001b[A\nBatch      :  46%|              | 734/1605 [09:33<10:44,  1.35it/s]\u001b[A\nBatch      :  46%|              | 735/1605 [09:34<10:42,  1.35it/s]\u001b[A\nBatch      :  46%|              | 736/1605 [09:34<10:41,  1.36it/s]\u001b[A\nBatch      :  46%|              | 737/1605 [09:35<10:43,  1.35it/s]\u001b[A\nBatch      :  46%|              | 738/1605 [09:36<10:41,  1.35it/s]\u001b[A\nBatch      :  46%|              | 739/1605 [09:37<10:40,  1.35it/s]\u001b[A\nBatch      :  46%|              | 740/1605 [09:37<10:39,  1.35it/s]\u001b[A\nBatch      :  46%|              | 741/1605 [09:38<10:38,  1.35it/s]\u001b[A\nBatch      :  46%|              | 742/1605 [09:39<10:37,  1.35it/s]\u001b[A\nBatch      :  46%|              | 743/1605 [09:40<10:36,  1.35it/s]\u001b[A\nBatch      :  46%|              | 744/1605 [09:40<10:35,  1.35it/s]\u001b[A\nBatch      :  46%|              | 745/1605 [09:41<10:35,  1.35it/s]\u001b[A\nBatch      :  46%|              | 746/1605 [09:42<10:34,  1.35it/s]\u001b[A\nBatch      :  47%|              | 747/1605 [09:43<10:34,  1.35it/s]\u001b[A\nBatch      :  47%|              | 748/1605 [09:43<10:33,  1.35it/s]\u001b[A\nBatch      :  47%|              | 749/1605 [09:44<10:32,  1.35it/s]\u001b[A\nBatch      :  47%|              | 750/1605 [09:45<10:31,  1.35it/s]\u001b[A\nBatch      :  47%|              | 751/1605 [09:46<10:34,  1.35it/s]\u001b[A\nBatch      :  47%|              | 752/1605 [09:46<10:32,  1.35it/s]\u001b[A\nBatch      :  47%|              | 753/1605 [09:47<10:33,  1.35it/s]\u001b[A\nBatch      :  47%|              | 754/1605 [09:48<10:31,  1.35it/s]\u001b[A\nBatch      :  47%|              | 755/1605 [09:48<10:29,  1.35it/s]\u001b[A\nBatch      :  47%|              | 756/1605 [09:49<10:30,  1.35it/s]\u001b[A\nBatch      :  47%|              | 757/1605 [09:50<10:31,  1.34it/s]\u001b[A\nBatch      :  47%|              | 758/1605 [09:51<10:31,  1.34it/s]\u001b[A\nBatch      :  47%|              | 759/1605 [09:51<10:29,  1.34it/s]\u001b[A\nBatch      :  47%|              | 760/1605 [09:52<10:28,  1.34it/s]\u001b[A\nBatch      :  47%|              | 761/1605 [09:53<10:28,  1.34it/s]\u001b[A\nBatch      :  47%|              | 762/1605 [09:54<10:26,  1.34it/s]\u001b[A\nBatch      :  48%|              | 763/1605 [09:54<10:24,  1.35it/s]\u001b[A\nBatch      :  48%|              | 764/1605 [09:55<10:25,  1.34it/s]\u001b[A\nBatch      :  48%|              | 765/1605 [09:56<10:25,  1.34it/s]\u001b[A\nBatch      :  48%|              | 766/1605 [09:57<10:24,  1.34it/s]\u001b[A\nBatch      :  48%|              | 767/1605 [09:57<10:24,  1.34it/s]\u001b[A\nBatch      :  48%|              | 768/1605 [09:58<10:23,  1.34it/s]\u001b[A\nBatch      :  48%|              | 769/1605 [09:59<10:22,  1.34it/s]\u001b[A\nBatch      :  48%|              | 770/1605 [10:00<10:19,  1.35it/s]\u001b[A\nBatch      :  48%|              | 771/1605 [10:00<10:17,  1.35it/s]\u001b[A\nBatch      :  48%|              | 772/1605 [10:01<10:22,  1.34it/s]\u001b[A\nBatch      :  48%|              | 773/1605 [10:02<10:22,  1.34it/s]\u001b[A\nBatch      :  48%|              | 774/1605 [10:03<10:19,  1.34it/s]\u001b[A\nBatch      :  48%|              | 775/1605 [10:03<10:18,  1.34it/s]\u001b[A\nBatch      :  48%|              | 776/1605 [10:04<10:15,  1.35it/s]\u001b[A\nBatch      :  48%|              | 777/1605 [10:05<10:15,  1.34it/s]\u001b[A\nBatch      :  48%|              | 778/1605 [10:06<10:13,  1.35it/s]\u001b[A\nBatch      :  49%|              | 779/1605 [10:06<10:13,  1.35it/s]\u001b[A\nBatch      :  49%|              | 780/1605 [10:07<10:10,  1.35it/s]\u001b[A\nBatch      :  49%|             | 781/1605 [10:08<10:08,  1.35it/s]\u001b[A\nBatch      :  49%|             | 782/1605 [10:09<10:09,  1.35it/s]\u001b[A\nBatch      :  49%|             | 783/1605 [10:09<10:08,  1.35it/s]\u001b[A\nBatch      :  49%|             | 784/1605 [10:10<10:07,  1.35it/s]\u001b[A\nBatch      :  49%|             | 785/1605 [10:11<10:09,  1.34it/s]\u001b[A\nBatch      :  49%|             | 786/1605 [10:12<10:08,  1.35it/s]\u001b[A\nBatch      :  49%|             | 787/1605 [10:12<10:06,  1.35it/s]\u001b[A\nBatch      :  49%|             | 788/1605 [10:13<10:05,  1.35it/s]\u001b[A\nBatch      :  49%|             | 789/1605 [10:14<10:05,  1.35it/s]\u001b[A\nBatch      :  49%|             | 790/1605 [10:14<10:04,  1.35it/s]\u001b[A\nBatch      :  49%|             | 791/1605 [10:15<10:03,  1.35it/s]\u001b[A\nBatch      :  49%|             | 792/1605 [10:16<10:01,  1.35it/s]\u001b[A\nBatch      :  49%|             | 793/1605 [10:17<09:59,  1.36it/s]\u001b[A\nBatch      :  49%|             | 794/1605 [10:17<09:58,  1.35it/s]\u001b[A\nBatch      :  50%|             | 795/1605 [10:18<09:57,  1.36it/s]\u001b[A\nBatch      :  50%|             | 796/1605 [10:19<09:57,  1.35it/s]\u001b[A\nBatch      :  50%|             | 797/1605 [10:20<09:58,  1.35it/s]\u001b[A\nBatch      :  50%|             | 798/1605 [10:20<10:02,  1.34it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A(epoch: 1, iters: 800, time: 0.733) G_gan_w: 0.423 G_recon_w: 30.038 G_gan_d: 1.117 G_recon_d: 29.656 D_fake_w: 0.662 D_real_w: 0.291 D_fake_d: 0.270 D_real_d: 0.291 G_SSIM: 4.596 G_feature: 2.692 G_style: 6.659 G_tv: 1.073 G_CD: 0.093 \nEpoch      :   0%|                                       | 0/20 [10:22<?, ?it/s]\nBatch      :  50%|             | 799/1605 [10:22<09:58,  1.35it/s]\u001b[A\nBatch      :  50%|             | 800/1605 [10:22<09:57,  1.35it/s]\u001b[A\nBatch      :  50%|             | 801/1605 [10:23<09:56,  1.35it/s]\u001b[A\nBatch      :  50%|             | 802/1605 [10:23<09:54,  1.35it/s]\u001b[A\nBatch      :  50%|             | 803/1605 [10:24<09:52,  1.35it/s]\u001b[A\nBatch      :  50%|             | 804/1605 [10:25<09:53,  1.35it/s]\u001b[A\nBatch      :  50%|             | 805/1605 [10:26<09:51,  1.35it/s]\u001b[A\nBatch      :  50%|             | 806/1605 [10:26<09:51,  1.35it/s]\u001b[A\nBatch      :  50%|             | 807/1605 [10:27<09:50,  1.35it/s]\u001b[A\nBatch      :  50%|             | 808/1605 [10:28<09:48,  1.35it/s]\u001b[A\nBatch      :  50%|             | 809/1605 [10:29<09:47,  1.35it/s]\u001b[A\nBatch      :  50%|             | 810/1605 [10:29<09:47,  1.35it/s]\u001b[A\nBatch      :  51%|             | 811/1605 [10:30<09:46,  1.35it/s]\u001b[A\nBatch      :  51%|             | 812/1605 [10:31<09:46,  1.35it/s]\u001b[A\nBatch      :  51%|             | 813/1605 [10:31<09:45,  1.35it/s]\u001b[A\nBatch      :  51%|             | 814/1605 [10:32<09:50,  1.34it/s]\u001b[A\nBatch      :  51%|             | 815/1605 [10:33<09:50,  1.34it/s]\u001b[A\nBatch      :  51%|             | 816/1605 [10:34<09:48,  1.34it/s]\u001b[A\nBatch      :  51%|             | 817/1605 [10:34<09:45,  1.35it/s]\u001b[A\nBatch      :  51%|             | 818/1605 [10:35<09:43,  1.35it/s]\u001b[A\nBatch      :  51%|             | 819/1605 [10:36<09:41,  1.35it/s]\u001b[A\nBatch      :  51%|             | 820/1605 [10:37<09:41,  1.35it/s]\u001b[A\nBatch      :  51%|             | 821/1605 [10:37<09:40,  1.35it/s]\u001b[A\nBatch      :  51%|             | 822/1605 [10:38<09:39,  1.35it/s]\u001b[A\nBatch      :  51%|             | 823/1605 [10:39<09:39,  1.35it/s]\u001b[A\nBatch      :  51%|             | 824/1605 [10:40<09:38,  1.35it/s]\u001b[A\nBatch      :  51%|             | 825/1605 [10:40<09:38,  1.35it/s]\u001b[A\nBatch      :  51%|             | 826/1605 [10:41<09:37,  1.35it/s]\u001b[A\nBatch      :  52%|             | 827/1605 [10:42<09:35,  1.35it/s]\u001b[A\nBatch      :  52%|             | 828/1605 [10:43<09:33,  1.35it/s]\u001b[A\nBatch      :  52%|             | 829/1605 [10:43<09:32,  1.36it/s]\u001b[A\nBatch      :  52%|             | 830/1605 [10:44<09:30,  1.36it/s]\u001b[A\nBatch      :  52%|             | 831/1605 [10:45<09:33,  1.35it/s]\u001b[A\nBatch      :  52%|             | 832/1605 [10:46<09:33,  1.35it/s]\u001b[A\nBatch      :  52%|             | 833/1605 [10:46<09:31,  1.35it/s]\u001b[A\nBatch      :  52%|             | 834/1605 [10:47<09:31,  1.35it/s]\u001b[A\nBatch      :  52%|             | 835/1605 [10:48<09:29,  1.35it/s]\u001b[A\nBatch      :  52%|             | 836/1605 [10:49<09:28,  1.35it/s]\u001b[A\nBatch      :  52%|             | 837/1605 [10:49<09:27,  1.35it/s]\u001b[A\nBatch      :  52%|             | 838/1605 [10:50<09:25,  1.36it/s]\u001b[A\nBatch      :  52%|             | 839/1605 [10:51<09:25,  1.35it/s]\u001b[A\nBatch      :  52%|            | 840/1605 [10:51<09:25,  1.35it/s]\u001b[A\nBatch      :  52%|            | 841/1605 [10:52<09:26,  1.35it/s]\u001b[A\nBatch      :  52%|            | 842/1605 [10:53<09:24,  1.35it/s]\u001b[A\nBatch      :  53%|            | 843/1605 [10:54<09:23,  1.35it/s]\u001b[A\nBatch      :  53%|            | 844/1605 [10:54<09:22,  1.35it/s]\u001b[A\nBatch      :  53%|            | 845/1605 [10:55<09:23,  1.35it/s]\u001b[A\nBatch      :  53%|            | 846/1605 [10:56<09:21,  1.35it/s]\u001b[A\nBatch      :  53%|            | 847/1605 [10:57<09:20,  1.35it/s]\u001b[A\nBatch      :  53%|            | 848/1605 [10:57<09:18,  1.35it/s]\u001b[A\nBatch      :  53%|            | 849/1605 [10:58<09:18,  1.35it/s]\u001b[A\nBatch      :  53%|            | 850/1605 [10:59<09:18,  1.35it/s]\u001b[A\nBatch      :  53%|            | 851/1605 [11:00<09:17,  1.35it/s]\u001b[A\nBatch      :  53%|            | 852/1605 [11:00<09:17,  1.35it/s]\u001b[A\nBatch      :  53%|            | 853/1605 [11:01<09:17,  1.35it/s]\u001b[A\nBatch      :  53%|            | 854/1605 [11:02<09:17,  1.35it/s]\u001b[A\nBatch      :  53%|            | 855/1605 [11:03<09:16,  1.35it/s]\u001b[A\nBatch      :  53%|            | 856/1605 [11:03<09:19,  1.34it/s]\u001b[A\nBatch      :  53%|            | 857/1605 [11:04<09:18,  1.34it/s]\u001b[A\nBatch      :  53%|            | 858/1605 [11:05<09:18,  1.34it/s]\u001b[A\nBatch      :  54%|            | 859/1605 [11:06<09:16,  1.34it/s]\u001b[A\nBatch      :  54%|            | 860/1605 [11:06<09:15,  1.34it/s]\u001b[A\nBatch      :  54%|            | 861/1605 [11:07<09:13,  1.34it/s]\u001b[A\nBatch      :  54%|            | 862/1605 [11:08<09:11,  1.35it/s]\u001b[A\nBatch      :  54%|            | 863/1605 [11:09<09:12,  1.34it/s]\u001b[A\nBatch      :  54%|            | 864/1605 [11:09<09:11,  1.34it/s]\u001b[A\nBatch      :  54%|            | 865/1605 [11:10<09:09,  1.35it/s]\u001b[A\nBatch      :  54%|            | 866/1605 [11:11<09:08,  1.35it/s]\u001b[A\nBatch      :  54%|            | 867/1605 [11:12<09:09,  1.34it/s]\u001b[A\nBatch      :  54%|            | 868/1605 [11:12<09:08,  1.34it/s]\u001b[A\nBatch      :  54%|            | 869/1605 [11:13<09:05,  1.35it/s]\u001b[A\nBatch      :  54%|            | 870/1605 [11:14<09:03,  1.35it/s]\u001b[A\nBatch      :  54%|            | 871/1605 [11:14<09:02,  1.35it/s]\u001b[A\nBatch      :  54%|            | 872/1605 [11:15<09:03,  1.35it/s]\u001b[A\nBatch      :  54%|            | 873/1605 [11:16<09:01,  1.35it/s]\u001b[A\nBatch      :  54%|            | 874/1605 [11:17<09:00,  1.35it/s]\u001b[A\nBatch      :  55%|            | 875/1605 [11:17<08:59,  1.35it/s]\u001b[A\nBatch      :  55%|            | 876/1605 [11:18<08:58,  1.35it/s]\u001b[A\nBatch      :  55%|            | 877/1605 [11:19<08:56,  1.36it/s]\u001b[A\nBatch      :  55%|            | 878/1605 [11:20<08:56,  1.35it/s]\u001b[A\nBatch      :  55%|            | 879/1605 [11:20<08:55,  1.35it/s]\u001b[A\nBatch      :  55%|            | 880/1605 [11:21<08:55,  1.36it/s]\u001b[A\nBatch      :  55%|            | 881/1605 [11:22<08:56,  1.35it/s]\u001b[A\nBatch      :  55%|            | 882/1605 [11:23<08:56,  1.35it/s]\u001b[A\nBatch      :  55%|            | 883/1605 [11:23<08:54,  1.35it/s]\u001b[A\nBatch      :  55%|            | 884/1605 [11:24<08:52,  1.35it/s]\u001b[A\nBatch      :  55%|            | 885/1605 [11:25<08:52,  1.35it/s]\u001b[A\nBatch      :  55%|            | 886/1605 [11:26<08:51,  1.35it/s]\u001b[A\nBatch      :  55%|            | 887/1605 [11:26<08:49,  1.36it/s]\u001b[A\nBatch      :  55%|            | 888/1605 [11:27<08:48,  1.36it/s]\u001b[A\nBatch      :  55%|            | 889/1605 [11:28<08:48,  1.36it/s]\u001b[A\nBatch      :  55%|            | 890/1605 [11:29<08:48,  1.35it/s]\u001b[A\nBatch      :  56%|            | 891/1605 [11:29<08:46,  1.36it/s]\u001b[A\nBatch      :  56%|            | 892/1605 [11:30<08:47,  1.35it/s]\u001b[A\nBatch      :  56%|            | 893/1605 [11:31<08:47,  1.35it/s]\u001b[A\nBatch      :  56%|            | 894/1605 [11:32<08:49,  1.34it/s]\u001b[A\nBatch      :  56%|            | 895/1605 [11:32<08:47,  1.35it/s]\u001b[A\nBatch      :  56%|            | 896/1605 [11:33<08:44,  1.35it/s]\u001b[A\nBatch      :  56%|            | 897/1605 [11:34<08:43,  1.35it/s]\u001b[A\nBatch      :  56%|            | 898/1605 [11:34<08:46,  1.34it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A(epoch: 1, iters: 900, time: 0.734) G_gan_w: 1.151 G_recon_w: 27.532 G_gan_d: 1.890 G_recon_d: 25.475 D_fake_w: 0.166 D_real_w: 1.311 D_fake_d: 0.004 D_real_d: 1.311 G_SSIM: 4.845 G_feature: 2.622 G_style: 5.826 G_tv: 0.948 G_CD: 0.142 \nEpoch      :   0%|                                       | 0/20 [11:36<?, ?it/s]\nBatch      :  56%|            | 899/1605 [11:36<08:48,  1.34it/s]\u001b[A\nBatch      :  56%|           | 900/1605 [11:36<08:45,  1.34it/s]\u001b[A\nBatch      :  56%|           | 901/1605 [11:37<08:45,  1.34it/s]\u001b[A\nBatch      :  56%|           | 902/1605 [11:37<08:43,  1.34it/s]\u001b[A\nBatch      :  56%|           | 903/1605 [11:38<08:40,  1.35it/s]\u001b[A\nBatch      :  56%|           | 904/1605 [11:39<08:39,  1.35it/s]\u001b[A\nBatch      :  56%|           | 905/1605 [11:40<08:38,  1.35it/s]\u001b[A\nBatch      :  56%|           | 906/1605 [11:40<08:36,  1.35it/s]\u001b[A\nBatch      :  57%|           | 907/1605 [11:41<08:36,  1.35it/s]\u001b[A\nBatch      :  57%|           | 908/1605 [11:42<08:34,  1.35it/s]\u001b[A\nBatch      :  57%|           | 909/1605 [11:43<08:34,  1.35it/s]\u001b[A\nBatch      :  57%|           | 910/1605 [11:43<08:34,  1.35it/s]\u001b[A\nBatch      :  57%|           | 911/1605 [11:44<08:33,  1.35it/s]\u001b[A\nBatch      :  57%|           | 912/1605 [11:45<08:33,  1.35it/s]\u001b[A\nBatch      :  57%|           | 913/1605 [11:46<08:33,  1.35it/s]\u001b[A\nBatch      :  57%|           | 914/1605 [11:46<08:31,  1.35it/s]\u001b[A\nBatch      :  57%|           | 915/1605 [11:47<08:30,  1.35it/s]\u001b[A\nBatch      :  57%|           | 916/1605 [11:48<08:29,  1.35it/s]\u001b[A\nBatch      :  57%|           | 917/1605 [11:49<08:30,  1.35it/s]\u001b[A\nBatch      :  57%|           | 918/1605 [11:49<08:30,  1.35it/s]\u001b[A\nBatch      :  57%|           | 919/1605 [11:50<08:28,  1.35it/s]\u001b[A\nBatch      :  57%|           | 920/1605 [11:51<08:26,  1.35it/s]\u001b[A\nBatch      :  57%|           | 921/1605 [11:52<08:24,  1.36it/s]\u001b[A\nBatch      :  57%|           | 922/1605 [11:52<08:23,  1.36it/s]\u001b[A\nBatch      :  58%|           | 923/1605 [11:53<08:23,  1.36it/s]\u001b[A\nBatch      :  58%|           | 924/1605 [11:54<08:22,  1.36it/s]\u001b[A\nBatch      :  58%|           | 925/1605 [11:54<08:21,  1.35it/s]\u001b[A\nBatch      :  58%|           | 926/1605 [11:55<08:22,  1.35it/s]\u001b[A\nBatch      :  58%|           | 927/1605 [11:56<08:21,  1.35it/s]\u001b[A\nBatch      :  58%|           | 928/1605 [11:57<08:20,  1.35it/s]\u001b[A\nBatch      :  58%|           | 929/1605 [11:57<08:20,  1.35it/s]\u001b[A\nBatch      :  58%|           | 930/1605 [11:58<08:19,  1.35it/s]\u001b[A\nBatch      :  58%|           | 931/1605 [11:59<08:17,  1.36it/s]\u001b[A\nBatch      :  58%|           | 932/1605 [12:00<08:16,  1.36it/s]\u001b[A\nBatch      :  58%|           | 933/1605 [12:00<08:15,  1.36it/s]\u001b[A\nBatch      :  58%|           | 934/1605 [12:01<08:15,  1.36it/s]\u001b[A\nBatch      :  58%|           | 935/1605 [12:02<08:14,  1.35it/s]\u001b[A\nBatch      :  58%|           | 936/1605 [12:03<08:13,  1.36it/s]\u001b[A\nBatch      :  58%|           | 937/1605 [12:03<08:12,  1.36it/s]\u001b[A\nBatch      :  58%|           | 938/1605 [12:04<08:11,  1.36it/s]\u001b[A\nBatch      :  59%|           | 939/1605 [12:05<08:10,  1.36it/s]\u001b[A\nBatch      :  59%|           | 940/1605 [12:06<08:15,  1.34it/s]\u001b[A\nBatch      :  59%|           | 941/1605 [12:06<08:15,  1.34it/s]\u001b[A\nBatch      :  59%|           | 942/1605 [12:07<08:12,  1.35it/s]\u001b[A\nBatch      :  59%|           | 943/1605 [12:08<08:09,  1.35it/s]\u001b[A\nBatch      :  59%|           | 944/1605 [12:09<08:09,  1.35it/s]\u001b[A\nBatch      :  59%|           | 945/1605 [12:09<08:08,  1.35it/s]\u001b[A\nBatch      :  59%|           | 946/1605 [12:10<08:06,  1.35it/s]\u001b[A\nBatch      :  59%|           | 947/1605 [12:11<08:06,  1.35it/s]\u001b[A\nBatch      :  59%|           | 948/1605 [12:11<08:06,  1.35it/s]\u001b[A\nBatch      :  59%|           | 949/1605 [12:12<08:05,  1.35it/s]\u001b[A\nBatch      :  59%|           | 950/1605 [12:13<08:03,  1.35it/s]\u001b[A\nBatch      :  59%|           | 951/1605 [12:14<08:01,  1.36it/s]\u001b[A\nBatch      :  59%|           | 952/1605 [12:14<08:01,  1.35it/s]\u001b[A\nBatch      :  59%|           | 953/1605 [12:15<08:02,  1.35it/s]\u001b[A\nBatch      :  59%|           | 954/1605 [12:16<08:00,  1.35it/s]\u001b[A\nBatch      :  60%|           | 955/1605 [12:17<07:59,  1.35it/s]\u001b[A\nBatch      :  60%|           | 956/1605 [12:17<07:58,  1.36it/s]\u001b[A\nBatch      :  60%|           | 957/1605 [12:18<07:57,  1.36it/s]\u001b[A\nBatch      :  60%|           | 958/1605 [12:19<07:59,  1.35it/s]\u001b[A\nBatch      :  60%|          | 959/1605 [12:20<07:59,  1.35it/s]\u001b[A\nBatch      :  60%|          | 960/1605 [12:20<07:58,  1.35it/s]\u001b[A\nBatch      :  60%|          | 961/1605 [12:21<07:58,  1.35it/s]\u001b[A\nBatch      :  60%|          | 962/1605 [12:22<07:56,  1.35it/s]\u001b[A\nBatch      :  60%|          | 963/1605 [12:23<07:55,  1.35it/s]\u001b[A\nBatch      :  60%|          | 964/1605 [12:23<07:53,  1.35it/s]\u001b[A\nBatch      :  60%|          | 965/1605 [12:24<07:53,  1.35it/s]\u001b[A\nBatch      :  60%|          | 966/1605 [12:25<07:52,  1.35it/s]\u001b[A\nBatch      :  60%|          | 967/1605 [12:26<07:53,  1.35it/s]\u001b[A\nBatch      :  60%|          | 968/1605 [12:26<07:52,  1.35it/s]\u001b[A\nBatch      :  60%|          | 969/1605 [12:27<07:51,  1.35it/s]\u001b[A\nBatch      :  60%|          | 970/1605 [12:28<07:51,  1.35it/s]\u001b[A\nBatch      :  60%|          | 971/1605 [12:28<07:49,  1.35it/s]\u001b[A\nBatch      :  61%|          | 972/1605 [12:29<07:48,  1.35it/s]\u001b[A\nBatch      :  61%|          | 973/1605 [12:30<07:47,  1.35it/s]\u001b[A\nBatch      :  61%|          | 974/1605 [12:31<07:47,  1.35it/s]\u001b[A\nBatch      :  61%|          | 975/1605 [12:31<07:47,  1.35it/s]\u001b[A\nBatch      :  61%|          | 976/1605 [12:32<07:46,  1.35it/s]\u001b[A\nBatch      :  61%|          | 977/1605 [12:33<07:45,  1.35it/s]\u001b[A\nBatch      :  61%|          | 978/1605 [12:34<07:45,  1.35it/s]\u001b[A\nBatch      :  61%|          | 979/1605 [12:34<07:44,  1.35it/s]\u001b[A\nBatch      :  61%|          | 980/1605 [12:35<07:44,  1.35it/s]\u001b[A\nBatch      :  61%|          | 981/1605 [12:36<07:42,  1.35it/s]\u001b[A\nBatch      :  61%|          | 982/1605 [12:37<07:44,  1.34it/s]\u001b[A\nBatch      :  61%|          | 983/1605 [12:37<07:44,  1.34it/s]\u001b[A\nBatch      :  61%|          | 984/1605 [12:38<07:42,  1.34it/s]\u001b[A\nBatch      :  61%|          | 985/1605 [12:39<07:41,  1.34it/s]\u001b[A\nBatch      :  61%|          | 986/1605 [12:40<07:38,  1.35it/s]\u001b[A\nBatch      :  61%|          | 987/1605 [12:40<07:36,  1.35it/s]\u001b[A\nBatch      :  62%|          | 988/1605 [12:41<07:34,  1.36it/s]\u001b[A\nBatch      :  62%|          | 989/1605 [12:42<07:33,  1.36it/s]\u001b[A\nBatch      :  62%|          | 990/1605 [12:43<07:33,  1.36it/s]\u001b[A\nBatch      :  62%|          | 991/1605 [12:43<07:34,  1.35it/s]\u001b[A\nBatch      :  62%|          | 992/1605 [12:44<07:33,  1.35it/s]\u001b[A\nBatch      :  62%|          | 993/1605 [12:45<07:34,  1.35it/s]\u001b[A\nBatch      :  62%|          | 994/1605 [12:46<07:34,  1.35it/s]\u001b[A\nBatch      :  62%|          | 995/1605 [12:46<07:34,  1.34it/s]\u001b[A\nBatch      :  62%|          | 996/1605 [12:47<07:32,  1.35it/s]\u001b[A\nBatch      :  62%|          | 997/1605 [12:48<07:30,  1.35it/s]\u001b[A\nBatch      :  62%|          | 998/1605 [12:49<07:28,  1.35it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A(epoch: 1, iters: 1000, time: 0.734) G_gan_w: -1.023 G_recon_w: 37.317 G_gan_d: -1.237 G_recon_d: 31.388 D_fake_w: 2.023 D_real_w: -0.000 D_fake_d: 2.237 D_real_d: -0.000 G_SSIM: 5.229 G_feature: 2.768 G_style: 6.622 G_tv: 1.087 G_CD: 0.604 \nEpoch      :   0%|                                       | 0/20 [12:50<?, ?it/s]\nBatch      :  62%|          | 999/1605 [12:50<07:27,  1.35it/s]\u001b[A\nBatch      :  62%|         | 1000/1605 [12:50<07:26,  1.35it/s]\u001b[A\nBatch      :  62%|         | 1001/1605 [12:51<07:25,  1.35it/s]\u001b[A\nBatch      :  62%|         | 1002/1605 [12:51<07:24,  1.36it/s]\u001b[A\nBatch      :  62%|         | 1003/1605 [12:52<07:23,  1.36it/s]\u001b[A\nBatch      :  63%|         | 1004/1605 [12:53<07:23,  1.36it/s]\u001b[A\nBatch      :  63%|         | 1005/1605 [12:54<07:22,  1.36it/s]\u001b[A\nBatch      :  63%|         | 1006/1605 [12:54<07:21,  1.36it/s]\u001b[A\nBatch      :  63%|         | 1007/1605 [12:55<07:23,  1.35it/s]\u001b[A\nBatch      :  63%|         | 1008/1605 [12:56<07:22,  1.35it/s]\u001b[A\nBatch      :  63%|         | 1009/1605 [12:57<07:21,  1.35it/s]\u001b[A\nBatch      :  63%|         | 1010/1605 [12:57<07:19,  1.35it/s]\u001b[A\nBatch      :  63%|         | 1011/1605 [12:58<07:18,  1.35it/s]\u001b[A\nBatch      :  63%|         | 1012/1605 [12:59<07:17,  1.35it/s]\u001b[A\nBatch      :  63%|         | 1013/1605 [13:00<07:16,  1.36it/s]\u001b[A\nBatch      :  63%|         | 1014/1605 [13:00<07:15,  1.36it/s]\u001b[A\nBatch      :  63%|         | 1015/1605 [13:01<07:14,  1.36it/s]\u001b[A\nBatch      :  63%|         | 1016/1605 [13:02<07:13,  1.36it/s]\u001b[A\nBatch      :  63%|         | 1017/1605 [13:03<07:12,  1.36it/s]\u001b[A\nBatch      :  63%|         | 1018/1605 [13:03<07:12,  1.36it/s]\u001b[A\nBatch      :  63%|         | 1019/1605 [13:04<07:12,  1.35it/s]\u001b[A\nBatch      :  64%|         | 1020/1605 [13:05<07:13,  1.35it/s]\u001b[A\nBatch      :  64%|         | 1021/1605 [13:06<07:15,  1.34it/s]\u001b[A\nBatch      :  64%|         | 1022/1605 [13:06<07:13,  1.35it/s]\u001b[A\nBatch      :  64%|         | 1023/1605 [13:07<07:13,  1.34it/s]\u001b[A\nBatch      :  64%|         | 1024/1605 [13:08<07:14,  1.34it/s]\u001b[A\nBatch      :  64%|         | 1025/1605 [13:08<07:13,  1.34it/s]\u001b[A\nBatch      :  64%|         | 1026/1605 [13:09<07:10,  1.35it/s]\u001b[A\nBatch      :  64%|         | 1027/1605 [13:10<07:08,  1.35it/s]\u001b[A\nBatch      :  64%|         | 1028/1605 [13:11<07:06,  1.35it/s]\u001b[A\nBatch      :  64%|         | 1029/1605 [13:11<07:05,  1.36it/s]\u001b[A\nBatch      :  64%|         | 1030/1605 [13:12<07:03,  1.36it/s]\u001b[A\nBatch      :  64%|         | 1031/1605 [13:13<07:02,  1.36it/s]\u001b[A\nBatch      :  64%|         | 1032/1605 [13:14<07:01,  1.36it/s]\u001b[A\nBatch      :  64%|         | 1033/1605 [13:14<07:00,  1.36it/s]\u001b[A\nBatch      :  64%|         | 1034/1605 [13:15<07:02,  1.35it/s]\u001b[A\nBatch      :  64%|         | 1035/1605 [13:16<07:01,  1.35it/s]\u001b[A\nBatch      :  65%|         | 1036/1605 [13:17<07:01,  1.35it/s]\u001b[A\nBatch      :  65%|         | 1037/1605 [13:17<07:00,  1.35it/s]\u001b[A\nBatch      :  65%|         | 1038/1605 [13:18<06:59,  1.35it/s]\u001b[A\nBatch      :  65%|         | 1039/1605 [13:19<06:58,  1.35it/s]\u001b[A\nBatch      :  65%|         | 1040/1605 [13:20<06:57,  1.35it/s]\u001b[A\nBatch      :  65%|         | 1041/1605 [13:20<06:55,  1.36it/s]\u001b[A\nBatch      :  65%|         | 1042/1605 [13:21<06:55,  1.36it/s]\u001b[A\nBatch      :  65%|         | 1043/1605 [13:22<06:54,  1.36it/s]\u001b[A\nBatch      :  65%|         | 1044/1605 [13:23<06:53,  1.36it/s]\u001b[A\nBatch      :  65%|         | 1045/1605 [13:23<06:52,  1.36it/s]\u001b[A\nBatch      :  65%|         | 1046/1605 [13:24<06:52,  1.36it/s]\u001b[A\nBatch      :  65%|         | 1047/1605 [13:25<06:51,  1.35it/s]\u001b[A\nBatch      :  65%|         | 1048/1605 [13:25<06:54,  1.34it/s]\u001b[A\nBatch      :  65%|         | 1049/1605 [13:26<06:55,  1.34it/s]\u001b[A\nBatch      :  65%|         | 1050/1605 [13:27<06:53,  1.34it/s]\u001b[A\nBatch      :  65%|         | 1051/1605 [13:28<06:52,  1.34it/s]\u001b[A\nBatch      :  66%|         | 1052/1605 [13:28<06:50,  1.35it/s]\u001b[A\nBatch      :  66%|         | 1053/1605 [13:29<06:48,  1.35it/s]\u001b[A\nBatch      :  66%|         | 1054/1605 [13:30<06:47,  1.35it/s]\u001b[A\nBatch      :  66%|         | 1055/1605 [13:31<06:45,  1.36it/s]\u001b[A\nBatch      :  66%|         | 1056/1605 [13:31<06:44,  1.36it/s]\u001b[A\nBatch      :  66%|         | 1057/1605 [13:32<06:44,  1.36it/s]\u001b[A\nBatch      :  66%|        | 1058/1605 [13:33<06:44,  1.35it/s]\u001b[A\nBatch      :  66%|        | 1059/1605 [13:34<06:42,  1.36it/s]\u001b[A\nBatch      :  66%|        | 1060/1605 [13:34<06:42,  1.36it/s]\u001b[A\nBatch      :  66%|        | 1061/1605 [13:35<06:42,  1.35it/s]\u001b[A\nBatch      :  66%|        | 1062/1605 [13:36<06:41,  1.35it/s]\u001b[A\nBatch      :  66%|        | 1063/1605 [13:37<06:40,  1.35it/s]\u001b[A\nBatch      :  66%|        | 1064/1605 [13:37<06:39,  1.36it/s]\u001b[A\nBatch      :  66%|        | 1065/1605 [13:38<06:38,  1.36it/s]\u001b[A\nBatch      :  66%|        | 1066/1605 [13:39<06:38,  1.35it/s]\u001b[A\nBatch      :  66%|        | 1067/1605 [13:40<06:40,  1.34it/s]\u001b[A\nBatch      :  67%|        | 1068/1605 [13:40<06:39,  1.35it/s]\u001b[A\nBatch      :  67%|        | 1069/1605 [13:41<06:37,  1.35it/s]\u001b[A\nBatch      :  67%|        | 1070/1605 [13:42<06:35,  1.35it/s]\u001b[A\nBatch      :  67%|        | 1071/1605 [13:43<06:35,  1.35it/s]\u001b[A\nBatch      :  67%|        | 1072/1605 [13:43<06:35,  1.35it/s]\u001b[A\nBatch      :  67%|        | 1073/1605 [13:44<06:34,  1.35it/s]\u001b[A\nBatch      :  67%|        | 1074/1605 [13:45<06:33,  1.35it/s]\u001b[A\nBatch      :  67%|        | 1075/1605 [13:45<06:34,  1.35it/s]\u001b[A\nBatch      :  67%|        | 1076/1605 [13:46<06:32,  1.35it/s]\u001b[A\nBatch      :  67%|        | 1077/1605 [13:47<06:31,  1.35it/s]\u001b[A\nBatch      :  67%|        | 1078/1605 [13:48<06:29,  1.35it/s]\u001b[A\nBatch      :  67%|        | 1079/1605 [13:48<06:28,  1.35it/s]\u001b[A\nBatch      :  67%|        | 1080/1605 [13:49<06:27,  1.35it/s]\u001b[A\nBatch      :  67%|        | 1081/1605 [13:50<06:26,  1.35it/s]\u001b[A\nBatch      :  67%|        | 1082/1605 [13:51<06:25,  1.36it/s]\u001b[A\nBatch      :  67%|        | 1083/1605 [13:51<06:25,  1.36it/s]\u001b[A\nBatch      :  68%|        | 1084/1605 [13:52<06:24,  1.36it/s]\u001b[A\nBatch      :  68%|        | 1085/1605 [13:53<06:23,  1.36it/s]\u001b[A\nBatch      :  68%|        | 1086/1605 [13:54<06:21,  1.36it/s]\u001b[A\nBatch      :  68%|        | 1087/1605 [13:54<06:20,  1.36it/s]\u001b[A\nBatch      :  68%|        | 1088/1605 [13:55<06:21,  1.35it/s]\u001b[A\nBatch      :  68%|        | 1089/1605 [13:56<06:22,  1.35it/s]\u001b[A\nBatch      :  68%|        | 1090/1605 [13:57<06:22,  1.35it/s]\u001b[A\nBatch      :  68%|        | 1091/1605 [13:57<06:21,  1.35it/s]\u001b[A\nBatch      :  68%|        | 1092/1605 [13:58<06:20,  1.35it/s]\u001b[A\nBatch      :  68%|        | 1093/1605 [13:59<06:19,  1.35it/s]\u001b[A\nBatch      :  68%|        | 1094/1605 [14:00<06:19,  1.35it/s]\u001b[A\nBatch      :  68%|        | 1095/1605 [14:00<06:17,  1.35it/s]\u001b[A\nBatch      :  68%|        | 1096/1605 [14:01<06:16,  1.35it/s]\u001b[A\nBatch      :  68%|        | 1097/1605 [14:02<06:15,  1.35it/s]\u001b[A\nBatch      :  68%|        | 1098/1605 [14:02<06:13,  1.36it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A(epoch: 1, iters: 1100, time: 0.737) G_gan_w: 0.595 G_recon_w: 38.869 G_gan_d: 1.870 G_recon_d: 37.508 D_fake_w: 0.474 D_real_w: 0.003 D_fake_d: 0.010 D_real_d: 0.003 G_SSIM: 4.555 G_feature: 2.620 G_style: 5.625 G_tv: 1.100 G_CD: 0.126 \nEpoch      :   0%|                                       | 0/20 [14:04<?, ?it/s]\nBatch      :  68%|        | 1099/1605 [14:04<06:12,  1.36it/s]\u001b[A\nBatch      :  69%|        | 1100/1605 [14:04<06:12,  1.35it/s]\u001b[A\nBatch      :  69%|        | 1101/1605 [14:05<06:13,  1.35it/s]\u001b[A\nBatch      :  69%|        | 1102/1605 [14:05<06:14,  1.34it/s]\u001b[A\nBatch      :  69%|        | 1103/1605 [14:06<06:12,  1.35it/s]\u001b[A\nBatch      :  69%|        | 1104/1605 [14:07<06:11,  1.35it/s]\u001b[A\nBatch      :  69%|        | 1105/1605 [14:08<06:09,  1.35it/s]\u001b[A\nBatch      :  69%|        | 1106/1605 [14:08<06:07,  1.36it/s]\u001b[A\nBatch      :  69%|        | 1107/1605 [14:09<06:06,  1.36it/s]\u001b[A\nBatch      :  69%|        | 1108/1605 [14:10<06:07,  1.35it/s]\u001b[A\nBatch      :  69%|        | 1109/1605 [14:11<06:09,  1.34it/s]\u001b[A\nBatch      :  69%|        | 1110/1605 [14:11<06:09,  1.34it/s]\u001b[A\nBatch      :  69%|        | 1111/1605 [14:12<06:06,  1.35it/s]\u001b[A\nBatch      :  69%|        | 1112/1605 [14:13<06:04,  1.35it/s]\u001b[A\nBatch      :  69%|        | 1113/1605 [14:14<06:03,  1.35it/s]\u001b[A\nBatch      :  69%|        | 1114/1605 [14:14<06:02,  1.35it/s]\u001b[A\nBatch      :  69%|        | 1115/1605 [14:15<06:02,  1.35it/s]\u001b[A\nBatch      :  70%|        | 1116/1605 [14:16<06:00,  1.36it/s]\u001b[A\nBatch      :  70%|        | 1117/1605 [14:17<06:00,  1.35it/s]\u001b[A\nBatch      :  70%|        | 1118/1605 [14:17<06:00,  1.35it/s]\u001b[A\nBatch      :  70%|       | 1119/1605 [14:18<05:59,  1.35it/s]\u001b[A\nBatch      :  70%|       | 1120/1605 [14:19<05:58,  1.35it/s]\u001b[A\nBatch      :  70%|       | 1121/1605 [14:19<05:57,  1.35it/s]\u001b[A\nBatch      :  70%|       | 1122/1605 [14:20<05:56,  1.36it/s]\u001b[A\nBatch      :  70%|       | 1123/1605 [14:21<05:55,  1.36it/s]\u001b[A\nBatch      :  70%|       | 1124/1605 [14:22<05:54,  1.36it/s]\u001b[A\nBatch      :  70%|       | 1125/1605 [14:22<05:52,  1.36it/s]\u001b[A\nBatch      :  70%|       | 1126/1605 [14:23<05:51,  1.36it/s]\u001b[A\nBatch      :  70%|       | 1127/1605 [14:24<05:52,  1.36it/s]\u001b[A\nBatch      :  70%|       | 1128/1605 [14:25<05:51,  1.36it/s]\u001b[A\nBatch      :  70%|       | 1129/1605 [14:25<05:51,  1.36it/s]\u001b[A\nBatch      :  70%|       | 1130/1605 [14:26<05:51,  1.35it/s]\u001b[A\nBatch      :  70%|       | 1131/1605 [14:27<05:52,  1.35it/s]\u001b[A\nBatch      :  71%|       | 1132/1605 [14:28<05:52,  1.34it/s]\u001b[A\nBatch      :  71%|       | 1133/1605 [14:28<05:50,  1.34it/s]\u001b[A\nBatch      :  71%|       | 1134/1605 [14:29<05:49,  1.35it/s]\u001b[A\nBatch      :  71%|       | 1135/1605 [14:30<05:47,  1.35it/s]\u001b[A\nBatch      :  71%|       | 1136/1605 [14:31<05:46,  1.35it/s]\u001b[A\nBatch      :  71%|       | 1137/1605 [14:31<05:45,  1.36it/s]\u001b[A\nBatch      :  71%|       | 1138/1605 [14:32<05:44,  1.36it/s]\u001b[A\nBatch      :  71%|       | 1139/1605 [14:33<05:43,  1.35it/s]\u001b[A\nBatch      :  71%|       | 1140/1605 [14:34<05:42,  1.36it/s]\u001b[A\nBatch      :  71%|       | 1141/1605 [14:34<05:42,  1.36it/s]\u001b[A\nBatch      :  71%|       | 1142/1605 [14:35<05:42,  1.35it/s]\u001b[A\nBatch      :  71%|       | 1143/1605 [14:36<05:41,  1.35it/s]\u001b[A\nBatch      :  71%|       | 1144/1605 [14:36<05:40,  1.36it/s]\u001b[A\nBatch      :  71%|       | 1145/1605 [14:37<05:39,  1.36it/s]\u001b[A\nBatch      :  71%|       | 1146/1605 [14:38<05:37,  1.36it/s]\u001b[A\nBatch      :  71%|       | 1147/1605 [14:39<05:37,  1.36it/s]\u001b[A\nBatch      :  72%|       | 1148/1605 [14:39<05:37,  1.36it/s]\u001b[A\nBatch      :  72%|       | 1149/1605 [14:40<05:37,  1.35it/s]\u001b[A\nBatch      :  72%|       | 1150/1605 [14:41<05:36,  1.35it/s]\u001b[A\nBatch      :  72%|       | 1151/1605 [14:42<05:39,  1.34it/s]\u001b[A\nBatch      :  72%|       | 1152/1605 [14:42<05:38,  1.34it/s]\u001b[A\nBatch      :  72%|       | 1153/1605 [14:43<05:36,  1.35it/s]\u001b[A\nBatch      :  72%|       | 1154/1605 [14:44<05:34,  1.35it/s]\u001b[A\nBatch      :  72%|       | 1155/1605 [14:45<05:32,  1.35it/s]\u001b[A\nBatch      :  72%|       | 1156/1605 [14:45<05:33,  1.35it/s]\u001b[A\nBatch      :  72%|       | 1157/1605 [14:46<05:31,  1.35it/s]\u001b[A\nBatch      :  72%|       | 1158/1605 [14:47<05:30,  1.35it/s]\u001b[A\nBatch      :  72%|       | 1159/1605 [14:48<05:30,  1.35it/s]\u001b[A\nBatch      :  72%|       | 1160/1605 [14:48<05:30,  1.35it/s]\u001b[A\nBatch      :  72%|       | 1161/1605 [14:49<05:28,  1.35it/s]\u001b[A\nBatch      :  72%|       | 1162/1605 [14:50<05:27,  1.35it/s]\u001b[A\nBatch      :  72%|       | 1163/1605 [14:51<05:25,  1.36it/s]\u001b[A\nBatch      :  73%|       | 1164/1605 [14:51<05:25,  1.36it/s]\u001b[A\nBatch      :  73%|       | 1165/1605 [14:52<05:24,  1.36it/s]\u001b[A\nBatch      :  73%|       | 1166/1605 [14:53<05:23,  1.36it/s]\u001b[A\nBatch      :  73%|       | 1167/1605 [14:53<05:22,  1.36it/s]\u001b[A\nBatch      :  73%|       | 1168/1605 [14:54<05:23,  1.35it/s]\u001b[A\nBatch      :  73%|       | 1169/1605 [14:55<05:24,  1.35it/s]\u001b[A\nBatch      :  73%|       | 1170/1605 [14:56<05:23,  1.34it/s]\u001b[A\nBatch      :  73%|       | 1171/1605 [14:56<05:22,  1.35it/s]\u001b[A\nBatch      :  73%|       | 1172/1605 [14:57<05:20,  1.35it/s]\u001b[A\nBatch      :  73%|       | 1173/1605 [14:58<05:19,  1.35it/s]\u001b[A\nBatch      :  73%|       | 1174/1605 [14:59<05:18,  1.35it/s]\u001b[A\nBatch      :  73%|       | 1175/1605 [14:59<05:17,  1.36it/s]\u001b[A\nBatch      :  73%|       | 1176/1605 [15:00<05:15,  1.36it/s]\u001b[A\nBatch      :  73%|       | 1177/1605 [15:01<05:14,  1.36it/s]\u001b[A\nBatch      :  73%|       | 1178/1605 [15:02<05:14,  1.36it/s]\u001b[A\nBatch      :  73%|       | 1179/1605 [15:02<05:14,  1.36it/s]\u001b[A\nBatch      :  74%|       | 1180/1605 [15:03<05:13,  1.36it/s]\u001b[A\nBatch      :  74%|      | 1181/1605 [15:04<05:12,  1.36it/s]\u001b[A\nBatch      :  74%|      | 1182/1605 [15:05<05:11,  1.36it/s]\u001b[A\nBatch      :  74%|      | 1183/1605 [15:05<05:11,  1.35it/s]\u001b[A\nBatch      :  74%|      | 1184/1605 [15:06<05:10,  1.36it/s]\u001b[A\nBatch      :  74%|      | 1185/1605 [15:07<05:09,  1.36it/s]\u001b[A\nBatch      :  74%|      | 1186/1605 [15:08<05:08,  1.36it/s]\u001b[A\nBatch      :  74%|      | 1187/1605 [15:08<05:08,  1.36it/s]\u001b[A\nBatch      :  74%|      | 1188/1605 [15:09<05:08,  1.35it/s]\u001b[A\nBatch      :  74%|      | 1189/1605 [15:10<05:06,  1.36it/s]\u001b[A\nBatch      :  74%|      | 1190/1605 [15:10<05:06,  1.35it/s]\u001b[A\nBatch      :  74%|      | 1191/1605 [15:11<05:06,  1.35it/s]\u001b[A\nBatch      :  74%|      | 1192/1605 [15:12<05:04,  1.36it/s]\u001b[A\nBatch      :  74%|      | 1193/1605 [15:13<05:05,  1.35it/s]\u001b[A\nBatch      :  74%|      | 1194/1605 [15:13<05:04,  1.35it/s]\u001b[A\nBatch      :  74%|      | 1195/1605 [15:14<05:03,  1.35it/s]\u001b[A\nBatch      :  75%|      | 1196/1605 [15:15<05:03,  1.35it/s]\u001b[A\nBatch      :  75%|      | 1197/1605 [15:16<05:02,  1.35it/s]\u001b[A\nBatch      :  75%|      | 1198/1605 [15:16<05:02,  1.35it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A(epoch: 1, iters: 1200, time: 0.734) G_gan_w: 0.896 G_recon_w: 29.169 G_gan_d: 0.782 G_recon_d: 29.382 D_fake_w: 0.348 D_real_w: 1.579 D_fake_d: 0.385 D_real_d: 1.579 G_SSIM: 5.272 G_feature: 2.888 G_style: 6.441 G_tv: 1.195 G_CD: 0.095 \nEpoch      :   0%|                                       | 0/20 [15:18<?, ?it/s]\nBatch      :  75%|      | 1199/1605 [15:18<05:00,  1.35it/s]\u001b[A\nBatch      :  75%|      | 1200/1605 [15:18<04:59,  1.35it/s]\u001b[A\nBatch      :  75%|      | 1201/1605 [15:19<04:59,  1.35it/s]\u001b[A\nBatch      :  75%|      | 1202/1605 [15:19<04:57,  1.35it/s]\u001b[A\nBatch      :  75%|      | 1203/1605 [15:20<04:57,  1.35it/s]\u001b[A\nBatch      :  75%|      | 1204/1605 [15:21<04:56,  1.35it/s]\u001b[A\nBatch      :  75%|      | 1205/1605 [15:22<04:56,  1.35it/s]\u001b[A\nBatch      :  75%|      | 1206/1605 [15:22<04:55,  1.35it/s]\u001b[A\nBatch      :  75%|      | 1207/1605 [15:23<04:54,  1.35it/s]\u001b[A\nBatch      :  75%|      | 1208/1605 [15:24<04:53,  1.35it/s]\u001b[A\nBatch      :  75%|      | 1209/1605 [15:25<04:52,  1.35it/s]\u001b[A\nBatch      :  75%|      | 1210/1605 [15:25<04:51,  1.35it/s]\u001b[A\nBatch      :  75%|      | 1211/1605 [15:26<04:50,  1.36it/s]\u001b[A\nBatch      :  76%|      | 1212/1605 [15:27<04:49,  1.36it/s]\u001b[A\nBatch      :  76%|      | 1213/1605 [15:27<04:49,  1.36it/s]\u001b[A\nBatch      :  76%|      | 1214/1605 [15:28<04:48,  1.35it/s]\u001b[A\nBatch      :  76%|      | 1215/1605 [15:29<04:48,  1.35it/s]\u001b[A\nBatch      :  76%|      | 1216/1605 [15:30<04:47,  1.35it/s]\u001b[A\nBatch      :  76%|      | 1217/1605 [15:30<04:46,  1.35it/s]\u001b[A\nBatch      :  76%|      | 1218/1605 [15:31<04:46,  1.35it/s]\u001b[A\nBatch      :  76%|      | 1219/1605 [15:32<04:45,  1.35it/s]\u001b[A\nBatch      :  76%|      | 1220/1605 [15:33<04:44,  1.36it/s]\u001b[A\nBatch      :  76%|      | 1221/1605 [15:33<04:43,  1.36it/s]\u001b[A\nBatch      :  76%|      | 1222/1605 [15:34<04:42,  1.36it/s]\u001b[A\nBatch      :  76%|      | 1223/1605 [15:35<04:43,  1.35it/s]\u001b[A\nBatch      :  76%|      | 1224/1605 [15:36<04:42,  1.35it/s]\u001b[A\nBatch      :  76%|      | 1225/1605 [15:36<04:41,  1.35it/s]\u001b[A\nBatch      :  76%|      | 1226/1605 [15:37<04:41,  1.35it/s]\u001b[A\nBatch      :  76%|      | 1227/1605 [15:38<04:39,  1.35it/s]\u001b[A\nBatch      :  77%|      | 1228/1605 [15:39<04:37,  1.36it/s]\u001b[A\nBatch      :  77%|      | 1229/1605 [15:39<04:37,  1.36it/s]\u001b[A\nBatch      :  77%|      | 1230/1605 [15:40<04:36,  1.36it/s]\u001b[A\nBatch      :  77%|      | 1231/1605 [15:41<04:35,  1.36it/s]\u001b[A\nBatch      :  77%|      | 1232/1605 [15:42<04:34,  1.36it/s]\u001b[A\nBatch      :  77%|      | 1233/1605 [15:42<04:33,  1.36it/s]\u001b[A\nBatch      :  77%|      | 1234/1605 [15:43<04:33,  1.36it/s]\u001b[A\nBatch      :  77%|      | 1235/1605 [15:44<04:35,  1.34it/s]\u001b[A\nBatch      :  77%|      | 1236/1605 [15:44<04:34,  1.34it/s]\u001b[A\nBatch      :  77%|      | 1237/1605 [15:45<04:33,  1.35it/s]\u001b[A\nBatch      :  77%|      | 1238/1605 [15:46<04:32,  1.35it/s]\u001b[A\nBatch      :  77%|      | 1239/1605 [15:47<04:31,  1.35it/s]\u001b[A\nBatch      :  77%|      | 1240/1605 [15:47<04:30,  1.35it/s]\u001b[A\nBatch      :  77%|      | 1241/1605 [15:48<04:29,  1.35it/s]\u001b[A\nBatch      :  77%|      | 1242/1605 [15:49<04:28,  1.35it/s]\u001b[A\nBatch      :  77%|     | 1243/1605 [15:50<04:27,  1.35it/s]\u001b[A\nBatch      :  78%|     | 1244/1605 [15:50<04:26,  1.35it/s]\u001b[A\nBatch      :  78%|     | 1245/1605 [15:51<04:26,  1.35it/s]\u001b[A\nBatch      :  78%|     | 1246/1605 [15:52<04:25,  1.35it/s]\u001b[A\nBatch      :  78%|     | 1247/1605 [15:53<04:24,  1.35it/s]\u001b[A\nBatch      :  78%|     | 1248/1605 [15:53<04:23,  1.35it/s]\u001b[A\nBatch      :  78%|     | 1249/1605 [15:54<04:22,  1.36it/s]\u001b[A\nBatch      :  78%|     | 1250/1605 [15:55<04:23,  1.35it/s]\u001b[A\nBatch      :  78%|     | 1251/1605 [15:56<04:22,  1.35it/s]\u001b[A\nBatch      :  78%|     | 1252/1605 [15:56<04:21,  1.35it/s]\u001b[A\nBatch      :  78%|     | 1253/1605 [15:57<04:20,  1.35it/s]\u001b[A\nBatch      :  78%|     | 1254/1605 [15:58<04:20,  1.35it/s]\u001b[A\nBatch      :  78%|     | 1255/1605 [15:59<04:19,  1.35it/s]\u001b[A\nBatch      :  78%|     | 1256/1605 [15:59<04:18,  1.35it/s]\u001b[A\nBatch      :  78%|     | 1257/1605 [16:00<04:17,  1.35it/s]\u001b[A\nBatch      :  78%|     | 1258/1605 [16:01<04:16,  1.35it/s]\u001b[A\nBatch      :  78%|     | 1259/1605 [16:02<04:15,  1.35it/s]\u001b[A\nBatch      :  79%|     | 1260/1605 [16:02<04:15,  1.35it/s]\u001b[A\nBatch      :  79%|     | 1261/1605 [16:03<04:14,  1.35it/s]\u001b[A\nBatch      :  79%|     | 1262/1605 [16:04<04:13,  1.35it/s]\u001b[A\nBatch      :  79%|     | 1263/1605 [16:04<04:12,  1.35it/s]\u001b[A\nBatch      :  79%|     | 1264/1605 [16:05<04:12,  1.35it/s]\u001b[A\nBatch      :  79%|     | 1265/1605 [16:06<04:11,  1.35it/s]\u001b[A\nBatch      :  79%|     | 1266/1605 [16:07<04:10,  1.35it/s]\u001b[A\nBatch      :  79%|     | 1267/1605 [16:07<04:10,  1.35it/s]\u001b[A\nBatch      :  79%|     | 1268/1605 [16:08<04:10,  1.35it/s]\u001b[A\nBatch      :  79%|     | 1269/1605 [16:09<04:09,  1.35it/s]\u001b[A\nBatch      :  79%|     | 1270/1605 [16:10<04:09,  1.34it/s]\u001b[A\nBatch      :  79%|     | 1271/1605 [16:10<04:08,  1.35it/s]\u001b[A\nBatch      :  79%|     | 1272/1605 [16:11<04:07,  1.35it/s]\u001b[A\nBatch      :  79%|     | 1273/1605 [16:12<04:06,  1.35it/s]\u001b[A\nBatch      :  79%|     | 1274/1605 [16:13<04:05,  1.35it/s]\u001b[A\nBatch      :  79%|     | 1275/1605 [16:13<04:03,  1.35it/s]\u001b[A\nBatch      :  80%|     | 1276/1605 [16:14<04:02,  1.36it/s]\u001b[A\nBatch      :  80%|     | 1277/1605 [16:15<04:03,  1.35it/s]\u001b[A\nBatch      :  80%|     | 1278/1605 [16:16<04:02,  1.35it/s]\u001b[A\nBatch      :  80%|     | 1279/1605 [16:16<04:01,  1.35it/s]\u001b[A\nBatch      :  80%|     | 1280/1605 [16:17<04:00,  1.35it/s]\u001b[A\nBatch      :  80%|     | 1281/1605 [16:18<04:00,  1.35it/s]\u001b[A\nBatch      :  80%|     | 1282/1605 [16:19<04:00,  1.35it/s]\u001b[A\nBatch      :  80%|     | 1283/1605 [16:19<03:59,  1.35it/s]\u001b[A\nBatch      :  80%|     | 1284/1605 [16:20<03:57,  1.35it/s]\u001b[A\nBatch      :  80%|     | 1285/1605 [16:21<03:57,  1.35it/s]\u001b[A\nBatch      :  80%|     | 1286/1605 [16:22<03:57,  1.35it/s]\u001b[A\nBatch      :  80%|     | 1287/1605 [16:22<03:57,  1.34it/s]\u001b[A\nBatch      :  80%|     | 1288/1605 [16:23<03:55,  1.35it/s]\u001b[A\nBatch      :  80%|     | 1289/1605 [16:24<03:54,  1.35it/s]\u001b[A\nBatch      :  80%|     | 1290/1605 [16:25<03:53,  1.35it/s]\u001b[A\nBatch      :  80%|     | 1291/1605 [16:25<03:53,  1.35it/s]\u001b[A\nBatch      :  80%|     | 1292/1605 [16:26<03:52,  1.35it/s]\u001b[A\nBatch      :  81%|     | 1293/1605 [16:27<03:51,  1.35it/s]\u001b[A\nBatch      :  81%|     | 1294/1605 [16:27<03:50,  1.35it/s]\u001b[A\nBatch      :  81%|     | 1295/1605 [16:28<03:49,  1.35it/s]\u001b[A\nBatch      :  81%|     | 1296/1605 [16:29<03:48,  1.35it/s]\u001b[A\nBatch      :  81%|     | 1297/1605 [16:30<03:47,  1.35it/s]\u001b[A\nBatch      :  81%|     | 1298/1605 [16:30<03:46,  1.35it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A(epoch: 1, iters: 1300, time: 0.743) G_gan_w: -0.177 G_recon_w: 27.380 G_gan_d: -0.078 G_recon_d: 26.154 D_fake_w: 1.178 D_real_w: 0.046 D_fake_d: 1.080 D_real_d: 0.046 G_SSIM: 5.356 G_feature: 2.612 G_style: 7.012 G_tv: 0.952 G_CD: 0.315 \nEpoch      :   0%|                                       | 0/20 [16:32<?, ?it/s]\nBatch      :  81%|     | 1299/1605 [16:32<03:46,  1.35it/s]\u001b[A\nBatch      :  81%|     | 1300/1605 [16:32<03:46,  1.35it/s]\u001b[A\nBatch      :  81%|     | 1301/1605 [16:33<03:45,  1.35it/s]\u001b[A\nBatch      :  81%|     | 1302/1605 [16:33<03:44,  1.35it/s]\u001b[A\nBatch      :  81%|     | 1303/1605 [16:34<03:43,  1.35it/s]\u001b[A\nBatch      :  81%|     | 1304/1605 [16:35<03:43,  1.35it/s]\u001b[A\nBatch      :  81%|    | 1305/1605 [16:36<03:41,  1.35it/s]\u001b[A\nBatch      :  81%|    | 1306/1605 [16:36<03:41,  1.35it/s]\u001b[A\nBatch      :  81%|    | 1307/1605 [16:37<03:40,  1.35it/s]\u001b[A\nBatch      :  81%|    | 1308/1605 [16:38<03:39,  1.35it/s]\u001b[A\nBatch      :  82%|    | 1309/1605 [16:39<03:38,  1.35it/s]\u001b[A\nBatch      :  82%|    | 1310/1605 [16:39<03:38,  1.35it/s]\u001b[A\nBatch      :  82%|    | 1311/1605 [16:40<03:38,  1.35it/s]\u001b[A\nBatch      :  82%|    | 1312/1605 [16:41<03:37,  1.35it/s]\u001b[A\nBatch      :  82%|    | 1313/1605 [16:42<03:37,  1.34it/s]\u001b[A\nBatch      :  82%|    | 1314/1605 [16:42<03:36,  1.34it/s]\u001b[A\nBatch      :  82%|    | 1315/1605 [16:43<03:35,  1.35it/s]\u001b[A\nBatch      :  82%|    | 1316/1605 [16:44<03:34,  1.35it/s]\u001b[A\nBatch      :  82%|    | 1317/1605 [16:45<03:33,  1.35it/s]\u001b[A\nBatch      :  82%|    | 1318/1605 [16:45<03:32,  1.35it/s]\u001b[A\nBatch      :  82%|    | 1319/1605 [16:46<03:34,  1.33it/s]\u001b[A\nBatch      :  82%|    | 1320/1605 [16:47<03:33,  1.33it/s]\u001b[A\nBatch      :  82%|    | 1321/1605 [16:48<03:31,  1.34it/s]\u001b[A\nBatch      :  82%|    | 1322/1605 [16:48<03:30,  1.35it/s]\u001b[A\nBatch      :  82%|    | 1323/1605 [16:49<03:29,  1.35it/s]\u001b[A\nBatch      :  82%|    | 1324/1605 [16:50<03:28,  1.35it/s]\u001b[A\nBatch      :  83%|    | 1325/1605 [16:50<03:27,  1.35it/s]\u001b[A\nBatch      :  83%|    | 1326/1605 [16:51<03:26,  1.35it/s]\u001b[A\nBatch      :  83%|    | 1327/1605 [16:52<03:24,  1.36it/s]\u001b[A\nBatch      :  83%|    | 1328/1605 [16:53<03:24,  1.36it/s]\u001b[A\nBatch      :  83%|    | 1329/1605 [16:53<03:23,  1.36it/s]\u001b[A\nBatch      :  83%|    | 1330/1605 [16:54<03:22,  1.35it/s]\u001b[A\nBatch      :  83%|    | 1331/1605 [16:55<03:23,  1.35it/s]\u001b[A\nBatch      :  83%|    | 1332/1605 [16:56<03:22,  1.35it/s]\u001b[A\nBatch      :  83%|    | 1333/1605 [16:56<03:21,  1.35it/s]\u001b[A\nBatch      :  83%|    | 1334/1605 [16:57<03:20,  1.35it/s]\u001b[A\nBatch      :  83%|    | 1335/1605 [16:58<03:19,  1.35it/s]\u001b[A\nBatch      :  83%|    | 1336/1605 [16:59<03:18,  1.35it/s]\u001b[A\nBatch      :  83%|    | 1337/1605 [16:59<03:17,  1.36it/s]\u001b[A\nBatch      :  83%|    | 1338/1605 [17:00<03:16,  1.36it/s]\u001b[A\nBatch      :  83%|    | 1339/1605 [17:01<03:16,  1.35it/s]\u001b[A\nBatch      :  83%|    | 1340/1605 [17:02<03:16,  1.35it/s]\u001b[A\nBatch      :  84%|    | 1341/1605 [17:02<03:15,  1.35it/s]\u001b[A\nBatch      :  84%|    | 1342/1605 [17:03<03:14,  1.35it/s]\u001b[A\nBatch      :  84%|    | 1343/1605 [17:04<03:13,  1.35it/s]\u001b[A\nBatch      :  84%|    | 1344/1605 [17:05<03:12,  1.35it/s]\u001b[A\nBatch      :  84%|    | 1345/1605 [17:05<03:12,  1.35it/s]\u001b[A\nBatch      :  84%|    | 1346/1605 [17:06<03:12,  1.35it/s]\u001b[A\nBatch      :  84%|    | 1347/1605 [17:07<03:11,  1.35it/s]\u001b[A\nBatch      :  84%|    | 1348/1605 [17:07<03:11,  1.35it/s]\u001b[A\nBatch      :  84%|    | 1349/1605 [17:08<03:09,  1.35it/s]\u001b[A\nBatch      :  84%|    | 1350/1605 [17:09<03:08,  1.35it/s]\u001b[A\nBatch      :  84%|    | 1351/1605 [17:10<03:08,  1.35it/s]\u001b[A\nBatch      :  84%|    | 1352/1605 [17:10<03:06,  1.35it/s]\u001b[A\nBatch      :  84%|    | 1353/1605 [17:11<03:06,  1.35it/s]\u001b[A\nBatch      :  84%|    | 1354/1605 [17:12<03:05,  1.35it/s]\u001b[A\nBatch      :  84%|    | 1355/1605 [17:13<03:04,  1.35it/s]\u001b[A\nBatch      :  84%|    | 1356/1605 [17:13<03:03,  1.35it/s]\u001b[A\nBatch      :  85%|    | 1357/1605 [17:14<03:03,  1.35it/s]\u001b[A\nBatch      :  85%|    | 1358/1605 [17:15<03:02,  1.35it/s]\u001b[A\nBatch      :  85%|    | 1359/1605 [17:16<03:01,  1.35it/s]\u001b[A\nBatch      :  85%|    | 1360/1605 [17:16<03:00,  1.35it/s]\u001b[A\nBatch      :  85%|    | 1361/1605 [17:17<03:01,  1.34it/s]\u001b[A\nBatch      :  85%|    | 1362/1605 [17:18<03:00,  1.35it/s]\u001b[A\nBatch      :  85%|    | 1363/1605 [17:19<02:59,  1.35it/s]\u001b[A\nBatch      :  85%|    | 1364/1605 [17:19<02:57,  1.36it/s]\u001b[A\nBatch      :  85%|    | 1365/1605 [17:20<02:57,  1.36it/s]\u001b[A\nBatch      :  85%|   | 1366/1605 [17:21<02:56,  1.36it/s]\u001b[A\nBatch      :  85%|   | 1367/1605 [17:22<02:55,  1.36it/s]\u001b[A\nBatch      :  85%|   | 1368/1605 [17:22<02:55,  1.35it/s]\u001b[A\nBatch      :  85%|   | 1369/1605 [17:23<02:53,  1.36it/s]\u001b[A\nBatch      :  85%|   | 1370/1605 [17:24<02:53,  1.36it/s]\u001b[A\nBatch      :  85%|   | 1371/1605 [17:24<02:52,  1.36it/s]\u001b[A\nBatch      :  85%|   | 1372/1605 [17:25<02:51,  1.36it/s]\u001b[A\nBatch      :  86%|   | 1373/1605 [17:26<02:51,  1.36it/s]\u001b[A\nBatch      :  86%|   | 1374/1605 [17:27<02:50,  1.36it/s]\u001b[A\nBatch      :  86%|   | 1375/1605 [17:27<02:49,  1.36it/s]\u001b[A\nBatch      :  86%|   | 1376/1605 [17:28<02:48,  1.36it/s]\u001b[A\nBatch      :  86%|   | 1377/1605 [17:29<02:48,  1.36it/s]\u001b[A\nBatch      :  86%|   | 1378/1605 [17:30<02:47,  1.36it/s]\u001b[A\nBatch      :  86%|   | 1379/1605 [17:30<02:46,  1.36it/s]\u001b[A\nBatch      :  86%|   | 1380/1605 [17:31<02:45,  1.36it/s]\u001b[A\nBatch      :  86%|   | 1381/1605 [17:32<02:45,  1.36it/s]\u001b[A\nBatch      :  86%|   | 1382/1605 [17:33<02:44,  1.35it/s]\u001b[A\nBatch      :  86%|   | 1383/1605 [17:33<02:43,  1.35it/s]\u001b[A\nBatch      :  86%|   | 1384/1605 [17:34<02:43,  1.35it/s]\u001b[A\nBatch      :  86%|   | 1385/1605 [17:35<02:42,  1.35it/s]\u001b[A\nBatch      :  86%|   | 1386/1605 [17:36<02:42,  1.35it/s]\u001b[A\nBatch      :  86%|   | 1387/1605 [17:36<02:41,  1.35it/s]\u001b[A\nBatch      :  86%|   | 1388/1605 [17:37<02:40,  1.35it/s]\u001b[A\nBatch      :  87%|   | 1389/1605 [17:38<02:39,  1.35it/s]\u001b[A\nBatch      :  87%|   | 1390/1605 [17:39<02:39,  1.35it/s]\u001b[A\nBatch      :  87%|   | 1391/1605 [17:39<02:38,  1.35it/s]\u001b[A\nBatch      :  87%|   | 1392/1605 [17:40<02:37,  1.35it/s]\u001b[A\nBatch      :  87%|   | 1393/1605 [17:41<02:36,  1.35it/s]\u001b[A\nBatch      :  87%|   | 1394/1605 [17:41<02:36,  1.35it/s]\u001b[A\nBatch      :  87%|   | 1395/1605 [17:42<02:35,  1.35it/s]\u001b[A\nBatch      :  87%|   | 1396/1605 [17:43<02:34,  1.35it/s]\u001b[A\nBatch      :  87%|   | 1397/1605 [17:44<02:33,  1.35it/s]\u001b[A\nBatch      :  87%|   | 1398/1605 [17:44<02:32,  1.35it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A(epoch: 1, iters: 1400, time: 0.734) G_gan_w: -0.277 G_recon_w: 27.802 G_gan_d: -1.367 G_recon_d: 24.150 D_fake_w: 1.277 D_real_w: -0.000 D_fake_d: 2.367 D_real_d: -0.000 G_SSIM: 5.667 G_feature: 2.708 G_style: 7.816 G_tv: 1.010 G_CD: 0.377 \nEpoch      :   0%|                                       | 0/20 [17:46<?, ?it/s]\nBatch      :  87%|   | 1399/1605 [17:46<02:32,  1.35it/s]\u001b[A\nBatch      :  87%|   | 1400/1605 [17:46<02:31,  1.35it/s]\u001b[A\nBatch      :  87%|   | 1401/1605 [17:47<02:31,  1.35it/s]\u001b[A\nBatch      :  87%|   | 1402/1605 [17:47<02:30,  1.35it/s]\u001b[A\nBatch      :  87%|   | 1403/1605 [17:48<02:31,  1.34it/s]\u001b[A\nBatch      :  87%|   | 1404/1605 [17:49<02:30,  1.34it/s]\u001b[A\nBatch      :  88%|   | 1405/1605 [17:50<02:29,  1.34it/s]\u001b[A\nBatch      :  88%|   | 1406/1605 [17:50<02:27,  1.35it/s]\u001b[A\nBatch      :  88%|   | 1407/1605 [17:51<02:26,  1.35it/s]\u001b[A\nBatch      :  88%|   | 1408/1605 [17:52<02:25,  1.35it/s]\u001b[A\nBatch      :  88%|   | 1409/1605 [17:53<02:24,  1.36it/s]\u001b[A\nBatch      :  88%|   | 1410/1605 [17:53<02:23,  1.36it/s]\u001b[A\nBatch      :  88%|   | 1411/1605 [17:54<02:22,  1.36it/s]\u001b[A\nBatch      :  88%|   | 1412/1605 [17:55<02:22,  1.35it/s]\u001b[A\nBatch      :  88%|   | 1413/1605 [17:56<02:22,  1.35it/s]\u001b[A\nBatch      :  88%|   | 1414/1605 [17:56<02:21,  1.35it/s]\u001b[A\nBatch      :  88%|   | 1415/1605 [17:57<02:20,  1.35it/s]\u001b[A\nBatch      :  88%|   | 1416/1605 [17:58<02:20,  1.35it/s]\u001b[A\nBatch      :  88%|   | 1417/1605 [17:59<02:19,  1.35it/s]\u001b[A\nBatch      :  88%|   | 1418/1605 [17:59<02:18,  1.35it/s]\u001b[A\nBatch      :  88%|   | 1419/1605 [18:00<02:16,  1.36it/s]\u001b[A\nBatch      :  88%|   | 1420/1605 [18:01<02:16,  1.36it/s]\u001b[A\nBatch      :  89%|   | 1421/1605 [18:01<02:15,  1.36it/s]\u001b[A\nBatch      :  89%|   | 1422/1605 [18:02<02:14,  1.36it/s]\u001b[A\nBatch      :  89%|   | 1423/1605 [18:03<02:14,  1.35it/s]\u001b[A\nBatch      :  89%|   | 1424/1605 [18:04<02:13,  1.35it/s]\u001b[A\nBatch      :  89%|   | 1425/1605 [18:04<02:13,  1.35it/s]\u001b[A","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Hello\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}